{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# Session 1.5C: Tokenization ‚Äî How LLMs Read Text\n",
    "\n",
    "**Requires:** `pip install tiktoken tokenizers` (~2MB total, fully local, no model download)  \n",
    "**No API key. No internet after install. No GPU.**  \n",
    "**Focus:** Understand exactly how text becomes numbers before any embedding happens.\n",
    "\n",
    "---\n",
    "\n",
    "## What You Will Experience\n",
    "\n",
    "```\n",
    "Part 1 ‚Üí Build a tokenizer from scratch (pure Python, zero libraries)\n",
    "           See the exact problem it solves\n",
    "Part 2 ‚Üí Character tokenization ‚Äî the naive approach and why it fails\n",
    "Part 3 ‚Üí Word tokenization ‚Äî better, but still breaks on banking jargon\n",
    "Part 4 ‚Üí BPE (Byte Pair Encoding) ‚Äî train it yourself on a banking corpus\n",
    "           Watch it learn subwords: 'compl' + 'iance', 'launder' + 'ing'\n",
    "Part 5 ‚Üí tiktoken ‚Äî the actual tokenizer used by GPT-4, Claude, and others\n",
    "           Inspect real token IDs for banking text\n",
    "Part 6 ‚Üí Token counting ‚Äî why it matters for cost, context, and chunking\n",
    "Part 7 ‚Üí Tokenization surprises ‚Äî what breaks and why\n",
    "Part 8 ‚Üí Banking-specific analysis ‚Äî how AML/KYC/SWIFT are tokenized\n",
    "```\n",
    "\n",
    "## Why Tokenization Matters Before Embeddings\n",
    "\n",
    "```\n",
    "Your text:   \"AML compliance requires KYC verification\"\n",
    "                      ‚Üì  tokenizer\n",
    "Token IDs:   [25300, 22460, 7612, 74, 42, 1242, 12, ...]\n",
    "                      ‚Üì  embedding lookup\n",
    "Vectors:     [[0.12, -0.34, ...], [0.88, 0.01, ...], ...]\n",
    "                      ‚Üì  transformer\n",
    "Output:      Model understands your text\n",
    "\n",
    "Tokenization is STEP ZERO. Every LLM does it first.\n",
    "If you don't understand tokens, you don't understand:\n",
    "  ‚Üí Why 'KYC' costs 1 token but 'anti-money-laundering' costs 5\n",
    "  ‚Üí Why your 10-page document uses 8,000 tokens\n",
    "  ‚Üí Why context windows are measured in tokens, not words\n",
    "  ‚Üí Why some languages are more expensive to process than others\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-setup-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiktoken  ‚Äî BPE tokenizer used by GPT-3.5/4, ~1MB, fully local\n",
    "# tokenizers ‚Äî HuggingFace tokenizer library for training BPE ourselves\n",
    "!pip install -q tiktoken tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import collections\n",
    "import tiktoken\n",
    "\n",
    "# Load the cl100k_base encoding ‚Äî used by GPT-3.5, GPT-4, Claude, and most modern LLMs\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "print(\"‚úÖ Ready ‚Äî fully local, no API key, no internet calls after install\")\n",
    "print()\n",
    "print(f\"Encoding loaded:   cl100k_base\")\n",
    "print(f\"Vocabulary size:   {enc.n_vocab:,} tokens\")\n",
    "print(f\"Used by:           GPT-3.5-turbo, GPT-4, text-embedding-ada-002\")\n",
    "print(f\"Also similar to:   Claude, Gemini (same BPE approach, slightly different vocab)\")\n",
    "print()\n",
    "print(\"Quick test:\")\n",
    "test = \"AML compliance requires KYC verification.\"\n",
    "ids  = enc.encode(test)\n",
    "print(f\"  Text:    '{test}'\")\n",
    "print(f\"  Tokens:  {ids}\")\n",
    "print(f\"  Count:   {len(ids)} tokens for {len(test)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Build a Tokenizer From Scratch ‚Äî The Problem It Solves\n",
    "\n",
    "Before using tiktoken, let's understand WHY tokenization exists at all.  \n",
    "A neural network needs **numbers** as input ‚Äî text must be converted first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fundamental problem: LLMs need integer IDs, not strings\n",
    "print(\"=== The Fundamental Problem ===\")\n",
    "print()\n",
    "print(\"A transformer model works on vectors of numbers.\")\n",
    "print(\"Text is a sequence of characters. How do we bridge that gap?\")\n",
    "print()\n",
    "print(\"Option 1: Assign each unique word an ID\")\n",
    "print(\"  'the' ‚Üí 1, 'bank' ‚Üí 2, 'compliance' ‚Üí 3 ...\")\n",
    "print(\"  Problem: English has 170,000+ words ‚Üí massive vocabulary\")\n",
    "print(\"  Problem: 'compliance' and 'compliant' are different IDs ‚Äî no sharing\")\n",
    "print(\"  Problem: new words like 'DeFi', 'stablecoin' ‚Üí unknown token\")\n",
    "print()\n",
    "print(\"Option 2: Use characters only\")\n",
    "print(\"  'A','M','L' ‚Üí 65, 77, 76\")\n",
    "print(\"  Problem: sequences become very long (1 word = 5-12 chars)\")\n",
    "print(\"  Problem: model must learn to spell before it learns meaning\")\n",
    "print()\n",
    "print(\"Option 3: Subwords ‚Äî the winner\")\n",
    "print(\"  'compliance' ‚Üí ['comp', 'liance']  ‚Üí [1842, 7712]\")\n",
    "print(\"  'compliant'  ‚Üí ['comp', 'liant']   ‚Üí [1842, 7634]\")\n",
    "print(\"  Both share the 'comp' token ‚Äî meaning is shared!\")\n",
    "print(\"  New words built from known subwords ‚Üí no unknown tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Character Tokenization ‚Äî The Naive Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a character-level tokenizer from scratch ‚Äî pure Python, zero libraries\n",
    "\n",
    "class CharTokenizer:\n",
    "    \"\"\"Simplest possible tokenizer: each character gets a unique integer ID.\"\"\"\n",
    "\n",
    "    def __init__(self, corpus: str):\n",
    "        # Build vocabulary from all unique characters in corpus\n",
    "        chars = sorted(set(corpus))\n",
    "        self.char_to_id = {c: i for i, c in enumerate(chars)}\n",
    "        self.id_to_char = {i: c for c, i in self.char_to_id.items()}\n",
    "        self.vocab_size  = len(chars)\n",
    "\n",
    "    def encode(self, text: str) -> list:\n",
    "        return [self.char_to_id.get(c, -1) for c in text]\n",
    "\n",
    "    def decode(self, ids: list) -> str:\n",
    "        return \"\".join(self.id_to_char.get(i, \"?\") for i in ids)\n",
    "\n",
    "    def show_vocab(self, n=40):\n",
    "        items = list(self.char_to_id.items())[:n]\n",
    "        for ch, idx in items:\n",
    "            display = repr(ch) if ch in (\" \", \"\\n\", \"\\t\") else ch\n",
    "            print(f\"  {display!r:<6} ‚Üí {idx}\")\n",
    "\n",
    "\n",
    "# Build corpus\n",
    "CORPUS = \"\"\"\n",
    "aml compliance team monitors suspicious transactions daily\n",
    "kyc procedures require customer identification and verification\n",
    "bsa requires banks to file suspicious activity reports\n",
    "fraud detection models flag anomalous transaction patterns\n",
    "capital adequacy ratio measures bank financial strength\n",
    "mortgage loan approval depends on credit score and income\n",
    "wire transfer sends funds between banks via swift network\n",
    "\"\"\"\n",
    "\n",
    "char_tok = CharTokenizer(CORPUS)\n",
    "\n",
    "print(\"=== Character Tokenizer ===\")\n",
    "print(f\"Corpus chars: {len(CORPUS)}\")\n",
    "print(f\"Vocab size:   {char_tok.vocab_size} unique characters\")\n",
    "print()\n",
    "print(\"Vocabulary:\")\n",
    "char_tok.show_vocab()\n",
    "print()\n",
    "\n",
    "# Encode a sentence\n",
    "sentence = \"aml compliance\"\n",
    "ids      = char_tok.encode(sentence)\n",
    "decoded  = char_tok.decode(ids)\n",
    "\n",
    "print(f\"Text:    '{sentence}'\")\n",
    "print(f\"IDs:     {ids}\")\n",
    "print(f\"Decoded: '{decoded}'\")\n",
    "print()\n",
    "print(f\"üìå {len(sentence)} characters ‚Üí {len(ids)} tokens (1:1 ratio)\")\n",
    "print(f\"   Longer sequences, smaller vocab ‚Äî but model must learn word meaning\")\n",
    "print(f\"   from scratch using individual letter patterns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show why character tokenization makes sequences very long\n",
    "banking_texts = [\n",
    "    \"aml\",\n",
    "    \"anti-money-laundering\",\n",
    "    \"kyc compliance verification\",\n",
    "    \"suspicious transaction monitoring report\",\n",
    "]\n",
    "\n",
    "print(\"=== Sequence Length with Character Tokenization ===\")\n",
    "print(f\"{'Text':<42} {'Chars':>6}  {'Tokens':>7}  {'Ratio'}\")\n",
    "print(\"-\" * 65)\n",
    "for text in banking_texts:\n",
    "    ids = char_tok.encode(text)\n",
    "    # filter -1 (unknown chars like '-')\n",
    "    valid = [i for i in ids if i >= 0]\n",
    "    print(f\"{text:<42} {len(text):>6}  {len(ids):>7}  1:{len(ids)//max(len(text.split()),1)} tokens/word\")\n",
    "\n",
    "print()\n",
    "print(\"üìå A 1,000-word document ‚Üí ~6,000 character tokens.\")\n",
    "print(\"   Transformers have quadratic attention cost: 6,000¬≤ = 36M operations\")\n",
    "print(\"   vs word tokens: 1,000¬≤ = 1M operations.\")\n",
    "print(\"   This is why character tokenization is impractical for long documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Word Tokenization ‚Äî Better, But Still Breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordTokenizer:\n",
    "    \"\"\"Split on whitespace/punctuation, assign integer IDs.\"\"\"\n",
    "\n",
    "    UNK = \"<UNK>\"   # unknown word token\n",
    "    PAD = \"<PAD>\"   # padding token\n",
    "\n",
    "    def __init__(self, corpus: str):\n",
    "        words = re.findall(r\"[a-z0-9]+\", corpus.lower())\n",
    "        freq  = collections.Counter(words)\n",
    "        # Sort by frequency (most common first)\n",
    "        vocab = [self.PAD, self.UNK] + [w for w, _ in freq.most_common()]\n",
    "        self.word_to_id = {w: i for i, w in enumerate(vocab)}\n",
    "        self.id_to_word = {i: w for w, i in self.word_to_id.items()}\n",
    "        self.vocab_size  = len(vocab)\n",
    "\n",
    "    def encode(self, text: str) -> list:\n",
    "        words = re.findall(r\"[a-z0-9]+\", text.lower())\n",
    "        return [self.word_to_id.get(w, self.word_to_id[self.UNK]) for w in words]\n",
    "\n",
    "    def decode(self, ids: list) -> str:\n",
    "        return \" \".join(self.id_to_word.get(i, self.UNK) for i in ids)\n",
    "\n",
    "\n",
    "word_tok = WordTokenizer(CORPUS)\n",
    "\n",
    "print(\"=== Word Tokenizer ===\")\n",
    "print(f\"Vocab size: {word_tok.vocab_size} unique words\")\n",
    "print()\n",
    "\n",
    "# Show top 20 vocab entries\n",
    "print(\"Most common words (first 20 vocab entries):\")\n",
    "for w, i in list(word_tok.word_to_id.items())[2:22]:\n",
    "    print(f\"  {i:>4}  '{w}'\")\n",
    "print()\n",
    "\n",
    "# Encode known vs unknown words\n",
    "sentences = [\n",
    "    \"aml compliance\",\n",
    "    \"suspicious transaction\",\n",
    "    \"fatf grey list\",          # 'fatf', 'grey', 'list' NOT in training corpus\n",
    "    \"stablecoin cryptocurrency\",  # also not in corpus\n",
    "]\n",
    "\n",
    "print(\"Encoding results:\")\n",
    "for s in sentences:\n",
    "    ids = word_tok.encode(s)\n",
    "    decoded = word_tok.decode(ids)\n",
    "    has_unk = 1 in ids  # ID 1 = <UNK>\n",
    "    flag = \"‚ö† UNKNOWN WORDS\" if has_unk else \"‚úì\"\n",
    "    print(f\"  Input:   '{s}'\")\n",
    "    print(f\"  IDs:     {ids}\")\n",
    "    print(f\"  Decoded: '{decoded}'  {flag}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The word tokenizer OOV (out-of-vocabulary) problem in banking\n",
    "print(\"=== The OOV Problem in Banking ===\")\n",
    "print()\n",
    "print(\"Words that appear in banking regulations but may not be in a generic vocab:\")\n",
    "print()\n",
    "\n",
    "oov_words = [\n",
    "    \"FATF\", \"MiCA\", \"DORA\", \"pgvector\", \"stablecoin\",\n",
    "    \"cryptocurrency\", \"DeFi\", \"VASP\", \"CCAR\", \"DFAST\",\n",
    "    \"Sarbanes-Oxley\", \"RegTech\", \"FinCEN\", \"FinTech\",\n",
    "]\n",
    "\n",
    "for word in oov_words:\n",
    "    ids = word_tok.encode(word)\n",
    "    has_unk = 1 in ids\n",
    "    status = \"‚ö† OOV ‚Üí <UNK>\" if has_unk else \"‚úì in vocab\"\n",
    "    print(f\"  {word:<20} {status}\")\n",
    "\n",
    "print()\n",
    "print(\"üìå With word tokenization: any term not seen during vocab building = <UNK>.\")\n",
    "print(\"   All <UNK> tokens look identical to the model ‚Äî it cannot tell 'FATF' from 'DeFi'.\")\n",
    "print(\"   This is the core problem BPE solves: no out-of-vocabulary words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: BPE ‚Äî Byte Pair Encoding ‚Äî Train It Yourself\n",
    "\n",
    "BPE starts with characters and iteratively merges the most frequent adjacent pair.  \n",
    "The result: common words become single tokens, rare words split into known subwords.  \n",
    "**No OOV problem** ‚Äî any word can be decomposed into characters if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement BPE from scratch ‚Äî pure Python stdlib\n",
    "# This is exactly what tiktoken does, just much simpler\n",
    "\n",
    "def tokenize_corpus(corpus: str) -> list:\n",
    "    \"\"\"Split corpus into words, represent each as list of characters + end-of-word marker.\"\"\"\n",
    "    words = re.findall(r\"[a-z]+\", corpus.lower())\n",
    "    return [tuple(list(w) + [\"</w>\"]) for w in words]\n",
    "\n",
    "def get_pair_counts(tokenized: list) -> dict:\n",
    "    \"\"\"Count frequency of every adjacent pair across all words.\"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word in tokenized:\n",
    "        for i in range(len(word) - 1):\n",
    "            pairs[(word[i], word[i + 1])] += 1\n",
    "    return pairs\n",
    "\n",
    "def merge_pair(tokenized: list, pair: tuple) -> list:\n",
    "    \"\"\"Merge all occurrences of `pair` into a single token.\"\"\"\n",
    "    merged_token = pair[0] + pair[1]\n",
    "    new_tokenized = []\n",
    "    for word in tokenized:\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            if i < len(word) - 1 and word[i] == pair[0] and word[i + 1] == pair[1]:\n",
    "                new_word.append(merged_token)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_tokenized.append(tuple(new_word))\n",
    "    return new_tokenized\n",
    "\n",
    "def train_bpe(corpus: str, num_merges: int = 30, verbose: bool = True):\n",
    "    \"\"\"Train BPE on corpus, return tokenized words and merge history.\"\"\"\n",
    "    tokenized = tokenize_corpus(corpus)\n",
    "    merges    = []\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Initial tokenization (first 5 words):\")\n",
    "        # Show unique words only\n",
    "        unique = list(dict.fromkeys(tokenized))[:5]\n",
    "        for w in unique:\n",
    "            print(f\"  {' '.join(w)}\")\n",
    "        print()\n",
    "\n",
    "    for step in range(num_merges):\n",
    "        pairs = get_pair_counts(tokenized)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best_pair  = max(pairs, key=pairs.get)\n",
    "        best_count = pairs[best_pair]\n",
    "        tokenized  = merge_pair(tokenized, best_pair)\n",
    "        merges.append(best_pair)\n",
    "\n",
    "        if verbose:\n",
    "            merged = best_pair[0] + best_pair[1]\n",
    "            print(f\"  Step {step+1:>2}: merge {best_pair[0]!r:>8} + {best_pair[1]!r:<10} ‚Üí {merged!r:<14} (freq={best_count})\")\n",
    "\n",
    "    return tokenized, merges\n",
    "\n",
    "print(\"=== BPE Training on Banking Corpus ===\")\n",
    "print()\n",
    "tokenized, merges = train_bpe(CORPUS, num_merges=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the final tokenization of key banking words\n",
    "print(\"=== Final BPE Tokenization of Banking Terms ===\")\n",
    "print(\"(After 25 merge operations)\")\n",
    "print()\n",
    "\n",
    "# Rebuild unique word ‚Üí token mapping from our trained tokenizer\n",
    "word_to_tokens = {}\n",
    "raw_words = re.findall(r\"[a-z]+\", CORPUS.lower())\n",
    "for word, tok in zip(raw_words, tokenized):\n",
    "    if word not in word_to_tokens:\n",
    "        word_to_tokens[word] = list(tok)\n",
    "\n",
    "print(f\"{'Word':<22} {'Tokens':<40} Count\")\n",
    "print(\"-\" * 70)\n",
    "for word, tokens in sorted(word_to_tokens.items(), key=lambda x: len(x[1])):\n",
    "    tokens_display = \" | \".join(t.replace(\"</w>\", \"‚èé\") for t in tokens)\n",
    "    print(f\"{word:<22} {tokens_display:<40} {len(tokens)} token(s)\")\n",
    "\n",
    "print()\n",
    "print(\"üìå Frequent words like 'and', 'the', 'to' ‚Üí 1-2 tokens (fully merged)\")\n",
    "print(\"   Domain words like 'compliance', 'suspicious' ‚Üí split into subwords\")\n",
    "print(\"   Subwords are shared across related words:\")\n",
    "print(\"   'compliance' and 'compliant' share the same prefix tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the BPE merge tree for one word ‚Äî how 'compliance' was assembled\n",
    "def trace_word_bpe(word: str, num_steps: int = 20):\n",
    "    \"\"\"Show step-by-step how BPE merges characters into subwords for one word.\"\"\"\n",
    "    tokens = tuple(list(word) + [\"</w>\"])\n",
    "    print(f\"Tracing BPE merges for: '{word}'\")\n",
    "    print(f\"  Start: {' | '.join(tokens)}\")\n",
    "    print()\n",
    "\n",
    "    for step, (a, b) in enumerate(merges[:num_steps]):\n",
    "        merged = a + b\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        changed = False\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens) - 1 and tokens[i] == a and tokens[i + 1] == b:\n",
    "                new_tokens.append(merged)\n",
    "                i += 2\n",
    "                changed = True\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        if changed:\n",
    "            tokens = tuple(new_tokens)\n",
    "            print(f\"  Step {step+1:>2} (merge {a!r}+{b!r}): {' | '.join(tokens)}\")\n",
    "\n",
    "    print(f\"\\n  Final: {len(tokens)} token(s) ‚Äî {list(tokens)}\")\n",
    "\n",
    "trace_word_bpe(\"compliance\")\n",
    "print()\n",
    "trace_word_bpe(\"suspicious\")\n",
    "print()\n",
    "trace_word_bpe(\"transaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: tiktoken ‚Äî The Real Thing\n",
    "\n",
    "`tiktoken` uses the same BPE algorithm but trained on a massive internet corpus  \n",
    "with 100,000 merge rules. Fully local ‚Äî the vocabulary is bundled with the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the tiktoken vocabulary\n",
    "print(\"=== tiktoken: cl100k_base Encoding ===\")\n",
    "print(f\"Vocabulary size: {enc.n_vocab:,} tokens\")\n",
    "print()\n",
    "\n",
    "# Encode and decode banking terms ‚Äî see real token IDs\n",
    "banking_terms = [\n",
    "    \"AML\",\n",
    "    \"KYC\",\n",
    "    \"BSA\",\n",
    "    \"SWIFT\",\n",
    "    \"compliance\",\n",
    "    \"anti-money-laundering\",\n",
    "    \"suspicious activity report\",\n",
    "    \"know your customer\",\n",
    "    \"capital adequacy ratio\",\n",
    "    \"Basel III\",\n",
    "    \"cryptocurrency\",\n",
    "    \"stablecoin\",\n",
    "    \"FATF\",\n",
    "    \"MiCA\",\n",
    "    \"mortgage\",\n",
    "    \"overdraft\",\n",
    "]\n",
    "\n",
    "print(f\"{'Term':<28} {'Token IDs':<35} {'Count':>6}  Tokens (decoded)\")\n",
    "print(\"-\" * 100)\n",
    "for term in banking_terms:\n",
    "    ids     = enc.encode(term)\n",
    "    decoded = [enc.decode([i]) for i in ids]\n",
    "    ids_str = str(ids)\n",
    "    dec_str = \" | \".join(repr(t) for t in decoded)\n",
    "    print(f\"{term:<28} {ids_str:<35} {len(ids):>6}  {dec_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual tokenization ‚Äî color-coded display using ASCII blocks\n",
    "def visualize_tokens(text: str, enc=enc):\n",
    "    \"\"\"Show each token as a labeled block.\"\"\"\n",
    "    ids     = enc.encode(text)\n",
    "    decoded = [enc.decode([i]) for i in ids]\n",
    "\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Tokens: {len(ids)}\")\n",
    "    print()\n",
    "\n",
    "    # Print each token with its ID beneath it\n",
    "    token_width = 12\n",
    "    header_row  = \"\"\n",
    "    id_row      = \"\"\n",
    "    sep_row     = \"\"\n",
    "\n",
    "    for token, tid in zip(decoded, ids):\n",
    "        # Escape whitespace for display\n",
    "        display = token.replace(\" \", \"¬∑\").replace(\"\\n\", \"‚Üµ\")\n",
    "        display = display[:token_width - 2]  # truncate if too long\n",
    "        header_row += f\"[{display:<{token_width - 2}}]\"\n",
    "        id_row     += f\" {str(tid):<{token_width - 1}}\"\n",
    "        sep_row    += \"-\" * token_width\n",
    "\n",
    "    print(\"  Tokens: \" + header_row)\n",
    "    print(\"  IDs:    \" + id_row)\n",
    "    print()\n",
    "\n",
    "banking_sentences = [\n",
    "    \"AML compliance requires KYC verification.\",\n",
    "    \"The bank approved the mortgage application.\",\n",
    "    \"Suspicious transaction flagged by anti-money-laundering system.\",\n",
    "    \"Basel III capital adequacy ratio must exceed 8%.\",\n",
    "    \"SWIFT wire transfer to correspondent bank.\",\n",
    "]\n",
    "\n",
    "print(\"=== Token-by-Token Visualization ===\")\n",
    "print()\n",
    "for s in banking_sentences:\n",
    "    visualize_tokens(s)\n",
    "    print(\"-\" * 60)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obizliqb4dg",
   "source": "---\n## Part 5B: Two Models, Same Text ‚Äî Tokenizers Are NOT Interchangeable\n\nEvery model family ships its own tokenizer.  \n**The same sentence produces different token IDs and even different token boundaries** across models.  \nThis matters because: token count affects cost, context fit, and chunking strategy.\n\n| Tokenizer | Algorithm | Vocab size | Used by |\n|-----------|-----------|-----------|---------|\n| `cl100k_base` | BPE | 100,277 | GPT-3.5, GPT-4, text-embedding-ada-002 |\n| `p50k_base` | BPE | 50,281 | GPT-3 (davinci), Codex |\n| `bert-base-uncased` | WordPiece | 30,522 | BERT, many HuggingFace models |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "78sbef86x3",
   "source": "# Load three tokenizers ‚Äî all local, no API, no model weights downloaded\n# tiktoken encodings are bundled in the package (~1MB each)\n# tokenizers WordPiece vocab is bundled too (~500KB)\n\nimport tiktoken\nfrom tokenizers import Tokenizer as HFTokenizer\n\n# --- Tokenizer 1: cl100k_base (GPT-3.5 / GPT-4) --- already loaded as `enc`\nenc_cl100k = tiktoken.get_encoding(\"cl100k_base\")\n\n# --- Tokenizer 2: p50k_base (GPT-3 / Codex) ---\nenc_p50k = tiktoken.get_encoding(\"p50k_base\")\n\n# --- Tokenizer 3: BERT WordPiece (bert-base-uncased) ---\n# Load from HuggingFace tokenizers library ‚Äî uses bundled vocab, no download\nbert_tok = HFTokenizer.from_pretrained(\"bert-base-uncased\")\n\nprint(\"=== Three Tokenizers Loaded ===\")\nprint()\nprint(f\"  cl100k_base (GPT-4)      vocab: {enc_cl100k.n_vocab:>8,}  algorithm: BPE\")\nprint(f\"  p50k_base   (GPT-3)      vocab: {enc_p50k.n_vocab:>8,}  algorithm: BPE\")\nprint(f\"  bert-base-uncased        vocab: {bert_tok.get_vocab_size():>8,}  algorithm: WordPiece\")\nprint()\nprint(\"üìå Larger vocab ‚Üí longer subwords ‚Üí fewer tokens per sentence\")\nprint(\"   Smaller vocab ‚Üí shorter subwords ‚Üí more tokens per sentence\")\nprint(\"   WordPiece uses '##' prefix for continuation pieces (different from BPE)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "i3n9l5msa0n",
   "source": "# Helper: encode with all three tokenizers and show results side by side\n\ndef encode_all(text: str):\n    \"\"\"Return (ids, decoded_pieces) for each tokenizer.\"\"\"\n    # cl100k\n    ids_cl   = enc_cl100k.encode(text)\n    dec_cl   = [enc_cl100k.decode([i]) for i in ids_cl]\n    # p50k\n    ids_p50  = enc_p50k.encode(text)\n    dec_p50  = [enc_p50k.decode([i]) for i in ids_p50]\n    # BERT WordPiece ‚Äî lowercases automatically (uncased)\n    out_bert = bert_tok.encode(text)\n    # strip [CLS] and [SEP] which BERT adds automatically\n    ids_bert = out_bert.ids[1:-1]\n    dec_bert = out_bert.tokens[1:-1]\n    return {\n        \"cl100k (GPT-4)\":  (ids_cl,  dec_cl),\n        \"p50k   (GPT-3)\":  (ids_p50, dec_p50),\n        \"BERT WordPiece\":   (ids_bert, dec_bert),\n    }\n\ndef compare_tokenizers(text: str):\n    \"\"\"Print a side-by-side token comparison for all three tokenizers.\"\"\"\n    results = encode_all(text)\n\n    print(f\"Text: '{text}'\")\n    print()\n    for name, (ids, pieces) in results.items():\n        # Build visual token blocks\n        blocks = \" | \".join(repr(p) for p in pieces)\n        print(f\"  {name}  ({len(ids)} tokens)\")\n        print(f\"    Pieces: {blocks}\")\n        print(f\"    IDs:    {ids}\")\n        print()\n\n# Run on a core set of banking sentences\nCOMPARE_SENTENCES = [\n    \"AML compliance requires KYC verification.\",\n    \"anti-money laundering\",\n    \"Suspicious activity report filed with FinCEN.\",\n    \"Basel III capital adequacy ratio.\",\n    \"SWIFT wire transfer to correspondent bank.\",\n    \"stablecoin cryptocurrency VASP FATF MiCA\",\n    \"mortgage overdraft creditworthiness\",\n]\n\nprint(\"=== Side-by-Side: Same Text, Three Tokenizers ===\")\nprint(\"=\" * 70)\nprint()\nfor s in COMPARE_SENTENCES:\n    compare_tokenizers(s)\n    print(\"-\" * 70)\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ima920rw6x",
   "source": "# Token COUNT comparison ‚Äî the most practical difference\n# Same text can use significantly more or fewer tokens depending on the tokenizer\n\nprint(\"=== Token Count Comparison: All Three Tokenizers ===\")\nprint()\n\nBANKING_TEXTS = {\n    \"AML\":                              \"AML\",\n    \"KYC\":                              \"KYC\",\n    \"SWIFT\":                            \"SWIFT\",\n    \"compliance\":                       \"compliance\",\n    \"anti-money-laundering\":            \"anti-money-laundering\",\n    \"suspicious activity report\":       \"suspicious activity report\",\n    \"capital adequacy ratio\":           \"capital adequacy ratio\",\n    \"stablecoin cryptocurrency VASP\":   \"stablecoin cryptocurrency VASP\",\n    \"AML alert (short paragraph)\": (\n        \"Transaction flagged: Customer ID 4892 initiated wire transfer of $48,500 \"\n        \"to correspondent bank in a FATF high-risk jurisdiction. AML analyst review required.\"\n    ),\n    \"Basel III excerpt\": (\n        \"Under Basel III, banks must maintain a minimum Common Equity Tier 1 (CET1) \"\n        \"capital ratio of 4.5% and a Total Capital ratio of 8% of risk-weighted assets.\"\n    ),\n}\n\nprint(f\"{'Text':<35} {'cl100k':>8} {'p50k':>8} {'BERT':>8}  Winner (fewest tokens)\")\nprint(\"-\" * 75)\n\nfor label, text in BANKING_TEXTS.items():\n    ids_cl   = enc_cl100k.encode(text)\n    ids_p50  = enc_p50k.encode(text)\n    ids_bert = bert_tok.encode(text).ids[1:-1]  # strip CLS/SEP\n\n    counts = {\n        \"cl100k\": len(ids_cl),\n        \"p50k\":   len(ids_p50),\n        \"BERT\":   len(ids_bert),\n    }\n    winner = min(counts, key=counts.get)\n    winner_str = f\"‚Üê {winner}\"\n    print(f\"{label[:34]:<35} {counts['cl100k']:>8} {counts['p50k']:>8} {counts['BERT']:>8}  {winner_str}\")\n\nprint()\nprint(\"üìå cl100k (100K vocab) typically wins ‚Äî larger vocab ‚Üí longer subwords ‚Üí fewer tokens.\")\nprint(\"   BERT (30K vocab) often uses the most tokens for technical/domain jargon.\")\nprint(\"   p50k sits in between ‚Äî same BPE algorithm as cl100k but smaller vocabulary.\")\nprint()\nprint(\"   WHY THIS MATTERS:\")\nprint(\"   ‚Üí If you chunk a document at 512 tokens for BERT, it may only be ~350 words.\")\nprint(\"   ‚Üí The same chunk for GPT-4 (cl100k) could be ~450 words.\")\nprint(\"   ‚Üí Always count tokens with THE SAME tokenizer as your target model.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ys0c2icewa",
   "source": "# WordPiece vs BPE ‚Äî the algorithmic difference made visible\n# BPE:       merges the most frequent CHARACTER PAIR across the whole corpus\n# WordPiece: merges the pair that maximises the likelihood of the training data\n# Result:    BPE tends to keep whole words; WordPiece splits more aggressively\n#            and marks continuations with \"##\"\n\nprint(\"=== BPE vs WordPiece: The Algorithm Difference ===\" )\nprint()\nprint(\"WordPiece '##' convention:\")\nprint(\"  'compliance' ‚Üí 'com' + '##pliance'\")\nprint(\"  The '##' means: this piece CONTINUES the previous word (no space before it)\")\nprint(\"  BPE uses no such marker ‚Äî continuations are implied by the space rule\")\nprint()\n\ndemo_words = [\n    \"compliance\",\n    \"compliant\",\n    \"anti-money-laundering\",\n    \"cryptocurrency\",\n    \"creditworthiness\",\n    \"SWIFT\",\n    \"FinCEN\",\n    \"stablecoin\",\n    \"overcollateralized\",\n    \"recapitalization\",\n]\n\nprint(f\"{'Word':<22}  {'BPE cl100k pieces':<35}  {'BPE p50k pieces':<35}  {'WordPiece BERT pieces'}\")\nprint(\"-\" * 130)\n\nfor word in demo_words:\n    pieces_cl   = [enc_cl100k.decode([i]) for i in enc_cl100k.encode(word)]\n    pieces_p50  = [enc_p50k.decode([i])   for i in enc_p50k.encode(word)]\n    out_bert    = bert_tok.encode(word)\n    pieces_bert = out_bert.tokens[1:-1]  # strip [CLS]/[SEP]\n\n    cl_str   = \" | \".join(repr(p) for p in pieces_cl)\n    p50_str  = \" | \".join(repr(p) for p in pieces_p50)\n    bert_str = \" | \".join(repr(p) for p in pieces_bert)\n\n    print(f\"{word:<22}  {cl_str:<35}  {p50_str:<35}  {bert_str}\")\n\nprint()\nprint(\"Key observations:\")\nprint(\"  1. BPE (cl100k/p50k): common words often become ONE token.\")\nprint(\"     WordPiece (BERT): more likely to split even common words.\")\nprint(\"  2. Acronyms like 'SWIFT' or 'FinCEN' may be single token in BPE\")\nprint(\"     but split into characters/subwords in BERT's smaller vocab.\")\nprint(\"  3. Long compound words ('overcollateralized') always split ‚Äî but DIFFERENTLY.\")\nprint(\"  4. The '##' in BERT output is NOT present in BPE ‚Äî it's a WordPiece-specific marker.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0yh7xbi50vei",
   "source": "# ASCII visual: token boundaries as fence posts\n# Makes it instantly clear where each tokenizer \"cuts\" the word\n\ndef fence_display(text: str):\n    \"\"\"\n    Show token boundaries as fence posts '|' in the original text.\n    Each tokenizer shown on its own line under the text.\n    \"\"\"\n    # Get pieces from each tokenizer\n    pieces_cl   = [enc_cl100k.decode([i]) for i in enc_cl100k.encode(text)]\n    pieces_p50  = [enc_p50k.decode([i])   for i in enc_p50k.encode(text)]\n    out_bert    = bert_tok.encode(text)\n    pieces_bert = [t.replace(\"##\", \"\") for t in out_bert.tokens[1:-1]]\n\n    def build_fence(pieces):\n        \"\"\"Join pieces with | separator, mark token boundaries.\"\"\"\n        return \"|\".join(p.replace(\" \", \"¬∑\") for p in pieces)\n\n    print(f\"  Text:              {text}\")\n    print(f\"  cl100k (GPT-4) :  {build_fence(pieces_cl)}\")\n    print(f\"  p50k   (GPT-3) :  {build_fence(pieces_p50)}\")\n    print(f\"  BERT WordPiece :  {build_fence(pieces_bert)}  (## stripped, lowercase)\")\n    print(f\"  Token counts   :  cl100k={len(pieces_cl)}  p50k={len(pieces_p50)}  BERT={len(pieces_bert)}\")\n\nprint(\"=== Fence Display: Where Each Tokenizer Cuts the Text ===\")\nprint(\"(| = token boundary,  ¬∑ = space)\")\nprint()\n\nfence_words = [\n    \"AML compliance\",\n    \"anti-money-laundering\",\n    \"suspicious transaction\",\n    \"cryptocurrency stablecoin\",\n    \"overcollateralized mortgage\",\n    \"KYC EDD PEP FATF VASP\",\n]\n\nfor w in fence_words:\n    fence_display(w)\n    print()\n\nprint(\"üìå Where the boundary '|' lands changes per tokenizer.\")\nprint(\"   GPT-4 (cl100k) cuts less ‚Äî bigger pieces, fewer tokens.\")\nprint(\"   BERT cuts more ‚Äî smaller pieces, more tokens.\")\nprint(\"   The SAME word can land on different sides of a chunk boundary\")\nprint(\"   depending on which tokenizer you use ‚Äî which affects RAG retrieval.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-part6-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Token Counting ‚Äî Why It Matters for Cost, Context, and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token counting is the single most practical skill for LLM users\n",
    "# Every API charges per token. Every model has a context limit in tokens.\n",
    "\n",
    "def token_stats(text: str, enc=enc) -> dict:\n",
    "    ids   = enc.encode(text)\n",
    "    words = len(text.split())\n",
    "    chars = len(text)\n",
    "    return {\n",
    "        \"tokens\": len(ids),\n",
    "        \"words\":  words,\n",
    "        \"chars\":  chars,\n",
    "        \"tok_per_word\": len(ids) / max(words, 1),\n",
    "        \"tok_per_char\": len(ids) / max(chars, 1),\n",
    "    }\n",
    "\n",
    "# Realistic banking document samples\n",
    "SAMPLES = {\n",
    "    \"Short AML alert\": \"\"\"\n",
    "        Transaction flagged: Customer ID 4892 initiated wire transfer of $48,500\n",
    "        to correspondent bank in jurisdiction with elevated FATF risk rating.\n",
    "        AML analyst review required within 24 hours per BSA policy.\n",
    "    \"\"\",\n",
    "\n",
    "    \"KYC onboarding paragraph\": \"\"\"\n",
    "        As part of our Know Your Customer (KYC) onboarding process, all new\n",
    "        corporate clients are required to submit: Certificate of Incorporation,\n",
    "        beneficial ownership declaration (UBO >25%), source of funds documentation,\n",
    "        and a completed Customer Due Diligence (CDD) questionnaire. Enhanced Due\n",
    "        Diligence (EDD) applies to Politically Exposed Persons (PEPs) and customers\n",
    "        in FATF grey-list jurisdictions.\n",
    "    \"\"\",\n",
    "\n",
    "    \"Basel III excerpt\": \"\"\"\n",
    "        Under Basel III, banks must maintain a minimum Common Equity Tier 1 (CET1)\n",
    "        capital ratio of 4.5%, a Tier 1 capital ratio of 6%, and a Total Capital\n",
    "        ratio of 8%. In addition, a Capital Conservation Buffer (CCB) of 2.5%\n",
    "        of risk-weighted assets (RWA) must be maintained, bringing the effective\n",
    "        minimum CET1 ratio to 7%. Countercyclical capital buffers of up to 2.5%\n",
    "        may be imposed by national regulators during periods of excess credit growth.\n",
    "    \"\"\",\n",
    "\n",
    "    \"Mortgage product description\": \"\"\"\n",
    "        Our 30-year fixed-rate mortgage product offers competitive rates starting\n",
    "        at 6.75% APR for borrowers with FICO scores above 740. Loan-to-value (LTV)\n",
    "        ratios up to 80% are available without private mortgage insurance (PMI).\n",
    "        Debt-to-income (DTI) ratios must not exceed 43%. Origination fees are 1%\n",
    "        of the principal amount. Applications require W-2s for two years, pay stubs,\n",
    "        bank statements, and a property appraisal.\n",
    "    \"\"\",\n",
    "}\n",
    "\n",
    "print(\"=== Token Counts for Realistic Banking Text ===\")\n",
    "print(f\"{'Sample':<30} {'Tokens':>7} {'Words':>7} {'Chars':>7} {'Tok/Word':>10} {'Tok/Char':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, text in SAMPLES.items():\n",
    "    stats = token_stats(text.strip())\n",
    "    print(f\"{name:<30} {stats['tokens']:>7} {stats['words']:>7} {stats['chars']:>7} \"\n",
    "          f\"{stats['tok_per_word']:>10.2f} {stats['tok_per_char']:>10.3f}\")\n",
    "\n",
    "print()\n",
    "print(\"üìå Rule of thumb: ~1.3 tokens per word for English banking text\")\n",
    "print(\"   (Technical jargon, acronyms, and numbers can push this higher)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context window and cost calculator\n",
    "print(\"=== Context Window Reality Check ===\")\n",
    "print()\n",
    "\n",
    "CONTEXT_WINDOWS = {\n",
    "    \"GPT-3.5-turbo\":    4_096,\n",
    "    \"GPT-4\":           32_768,\n",
    "    \"GPT-4 Turbo\":    128_000,\n",
    "    \"Claude Sonnet\":  200_000,\n",
    "    \"Claude Opus\":    200_000,\n",
    "    \"Gemini 1.5 Pro\": 1_000_000,\n",
    "}\n",
    "\n",
    "# Estimate tokens for common banking document types\n",
    "DOCUMENT_SIZES = {\n",
    "    \"1 AML alert (2 paragraphs)\": 250,\n",
    "    \"KYC questionnaire (1 page)\": 500,\n",
    "    \"Loan application (4 pages)\": 2_000,\n",
    "    \"Basel III reg excerpt (10 pages)\": 5_000,\n",
    "    \"Annual report (80 pages)\": 40_000,\n",
    "    \"Full AML policy manual (200 pages)\": 100_000,\n",
    "    \"Basel III full text (600 pages)\": 300_000,\n",
    "}\n",
    "\n",
    "print(f\"{'Model':<22} {'Context (tokens)':>18}\")\n",
    "print(\"-\" * 42)\n",
    "for model_name, ctx in CONTEXT_WINDOWS.items():\n",
    "    print(f\"{model_name:<22} {ctx:>18,}\")\n",
    "\n",
    "print()\n",
    "print(f\"{'Document':<42} {'Est. tokens':>13}  Fits in GPT-4?  Fits in Claude?\")\n",
    "print(\"-\" * 85)\n",
    "for doc, tokens in DOCUMENT_SIZES.items():\n",
    "    fits_gpt4   = \"‚úì\" if tokens < 32_768  else \"‚úó\"\n",
    "    fits_claude = \"‚úì\" if tokens < 200_000 else \"‚úó\"\n",
    "    print(f\"{doc:<42} {tokens:>13,}  {fits_gpt4:<14}  {fits_claude}\")\n",
    "\n",
    "print()\n",
    "print(\"üìå This is why RAG (Session 3) exists.\")\n",
    "print(\"   You can't fit a 200-page policy manual into any context window.\")\n",
    "print(\"   RAG retrieves only the relevant 500-token chunks ‚Üí fits easily.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost calculator ‚Äî real pricing (approximate)\n",
    "print(\"=== Token Cost Calculator ===\")\n",
    "print(\"(Approximate pricing as of 2025 ‚Äî verify current rates)\")\n",
    "print()\n",
    "\n",
    "# Cost per 1M tokens in USD\n",
    "PRICING = {\n",
    "    \"GPT-4o (input)\": 2.50,\n",
    "    \"GPT-4o (output)\": 10.00,\n",
    "    \"Claude Sonnet 3.5 (input)\": 3.00,\n",
    "    \"Claude Sonnet 3.5 (output)\": 15.00,\n",
    "    \"GPT-3.5-turbo (input)\": 0.50,\n",
    "    \"text-embedding-ada-002\": 0.10,\n",
    "}\n",
    "\n",
    "print(f\"{'Model + direction':<35} {'$/1M tokens':>12}\")\n",
    "print(\"-\" * 50)\n",
    "for name, price in PRICING.items():\n",
    "    print(f\"{name:<35} ${price:>11.2f}\")\n",
    "\n",
    "print()\n",
    "print(\"=== Cost Scenarios ===\")\n",
    "\n",
    "scenarios = [\n",
    "    (\"Embed 10,000 AML alerts (250 tok each)\",   10_000 * 250,   0.10),\n",
    "    (\"Summarize 1,000 KYC files (500 tok each)\",  1_000 * 500,   3.00),\n",
    "    (\"Chat: 1M daily messages (500 tok in/out)\",  1_000_000 * 500, 3.00),\n",
    "    (\"Process Basel III full text once\",          300_000,        3.00),\n",
    "]\n",
    "\n",
    "print(f\"{'Scenario':<47} {'Tokens':>10} {'Est. cost':>12}\")\n",
    "print(\"-\" * 72)\n",
    "for desc, tokens, price_per_M in scenarios:\n",
    "    cost = tokens / 1_000_000 * price_per_M\n",
    "    print(f\"{desc:<47} {tokens:>10,} ${cost:>11.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"üìå Token efficiency directly impacts operational cost.\")\n",
    "print(\"   Shorter prompts, better chunking, and caching all reduce spend.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part7-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Tokenization Surprises ‚Äî What Breaks and Why\n",
    "\n",
    "The tokenizer has quirks that matter for production banking systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surprise 1: Numbers are tokenized in unexpected ways\n",
    "print(\"=== Surprise 1: Numbers ===\")\n",
    "print()\n",
    "\n",
    "number_examples = [\n",
    "    \"1000\",\n",
    "    \"10000\",\n",
    "    \"100000\",\n",
    "    \"1,000\",\n",
    "    \"10,000\",\n",
    "    \"$10,000\",\n",
    "    \"$10,000.00\",\n",
    "    \"48500\",\n",
    "    \"8.5%\",\n",
    "    \"6.75% APR\",\n",
    "    \"Basel III\",\n",
    "    \"FICO 740\",\n",
    "]\n",
    "\n",
    "for text in number_examples:\n",
    "    ids     = enc.encode(text)\n",
    "    decoded = [enc.decode([i]) for i in ids]\n",
    "    parts   = \" | \".join(repr(t) for t in decoded)\n",
    "    print(f\"  {text:<18} ‚Üí {len(ids)} token(s):  {parts}\")\n",
    "\n",
    "print()\n",
    "print(\"üìå Numbers are not tokenized as numbers ‚Äî they are strings.\")\n",
    "print(\"   '10,000' may split differently from '10000' from '$10,000'.\")\n",
    "print(\"   LLMs can struggle with arithmetic because of this tokenization.\")\n",
    "print(\"   For regulatory thresholds and amounts, consider normalizing format first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surprise 2: Capitalization matters\n",
    "print(\"=== Surprise 2: Capitalization Changes Token IDs ===\")\n",
    "print()\n",
    "\n",
    "cap_pairs = [\n",
    "    (\"compliance\", \"Compliance\"),\n",
    "    (\"aml\",        \"AML\"),\n",
    "    (\"kyc\",        \"KYC\"),\n",
    "    (\"swift\",      \"SWIFT\"),\n",
    "    (\"bank\",       \"Bank\"),\n",
    "    (\"mortgage\",   \"Mortgage\"),\n",
    "    (\"fraud\",      \"FRAUD\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Lower':<16} {'Upper':<16} {'Lower IDs':<25} {'Upper IDs':<25} Same?\")\n",
    "print(\"-\" * 90)\n",
    "for lower, upper in cap_pairs:\n",
    "    ids_l = enc.encode(lower)\n",
    "    ids_u = enc.encode(upper)\n",
    "    same  = \"‚úì\" if ids_l == ids_u else \"‚úó DIFFERENT\"\n",
    "    print(f\"{lower:<16} {upper:<16} {str(ids_l):<25} {str(ids_u):<25} {same}\")\n",
    "\n",
    "print()\n",
    "print(\"üìå 'aml' and 'AML' are DIFFERENT tokens.\")\n",
    "print(\"   This matters for consistency in prompts and document processing.\")\n",
    "print(\"   Consider lowercasing or normalizing banking acronyms before embedding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surprise 3: Leading spaces create different tokens\n",
    "print(\"=== Surprise 3: Leading Space Changes the Token ===\")\n",
    "print()\n",
    "\n",
    "space_examples = [\n",
    "    (\"compliance\",  \" compliance\"),\n",
    "    (\"fraud\",       \" fraud\"),\n",
    "    (\"bank\",        \" bank\"),\n",
    "    (\"transaction\", \" transaction\"),\n",
    "]\n",
    "\n",
    "for without, with_space in space_examples:\n",
    "    id_without = enc.encode(without)\n",
    "    id_with    = enc.encode(with_space)\n",
    "    same = id_without == id_with\n",
    "    print(f\"  '{without}' ‚Üí {id_without}\")\n",
    "    print(f\"  '{with_space}' ‚Üí {id_with}\")\n",
    "    print(f\"  Same token? {'Yes' if same else 'No ‚Äî leading space creates a different token'}\")\n",
    "    print()\n",
    "\n",
    "print(\"üìå In BPE, a word at the start of a sentence (no preceding space)\")\n",
    "print(\"   gets a different token ID than the same word mid-sentence (with space).\")\n",
    "print(\"   tiktoken handles this internally ‚Äî but it explains why position matters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surprise 4: Languages are not equal ‚Äî non-English text uses more tokens\n",
    "print(\"=== Surprise 4: Language Efficiency ===\")\n",
    "print(\"(Same meaning, different languages, different token counts)\")\n",
    "print()\n",
    "\n",
    "multilingual = [\n",
    "    (\"English\",    \"Anti-money laundering compliance requires customer verification.\"),\n",
    "    (\"Spanish\",    \"El cumplimiento contra el lavado de dinero requiere verificaci√≥n del cliente.\"),\n",
    "    (\"French\",     \"La conformit√© en mati√®re de lutte contre le blanchiment d'argent.\"),\n",
    "    (\"German\",     \"Geldw√§schebek√§mpfung erfordert Kundenidentifikation und √úberpr√ºfung.\"),\n",
    "    (\"Arabic\",     \"ÿßŸÑÿßŸÖÿ™ÿ´ÿßŸÑ ŸÑŸÖŸÉÿßŸÅÿ≠ÿ© ÿ∫ÿ≥ŸäŸÑ ÿßŸÑÿ£ŸÖŸàÿßŸÑ Ÿäÿ™ÿ∑ŸÑÿ® ÿßŸÑÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ ŸáŸàŸäÿ© ÿßŸÑÿπŸÖŸäŸÑ.\"),\n",
    "    (\"Chinese\",    \"ÂèçÊ¥óÈí±ÂêàËßÑË¶ÅÊ±ÇÂÆ¢Êà∑Ë∫´‰ªΩÈ™åËØÅÂíåÂ∞ΩËÅåË∞ÉÊü•Á®ãÂ∫è„ÄÇ\"),\n",
    "    (\"Japanese\",   \"„Éû„Éç„Éº„É≠„É≥„ÉÄ„É™„É≥„Ç∞Èò≤Ê≠¢„Ç≥„É≥„Éó„É©„Ç§„Ç¢„É≥„Çπ„Å´„ÅØÈ°ßÂÆ¢Á¢∫Ë™ç„ÅåÂøÖË¶Å„Åß„Åô„ÄÇ\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Language':<12} {'Tokens':>8} {'Chars':>8}  {'Tok/Char':>10}  Sentence\")\n",
    "print(\"-\" * 100)\n",
    "for lang, text in multilingual:\n",
    "    ids = enc.encode(text)\n",
    "    print(f\"{lang:<12} {len(ids):>8} {len(text):>8}  {len(ids)/len(text):>10.3f}  {text[:55]}\")\n",
    "\n",
    "print()\n",
    "print(\"üìå Asian languages often need 2-4x more tokens than English for equivalent content.\")\n",
    "print(\"   This means higher cost AND shorter effective context for multilingual banking.\")\n",
    "print(\"   Global banks with multilingual compliance documents should account for this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part8-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Banking-Specific Tokenization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive: how are banking acronyms and terms tokenized?\n",
    "# This directly affects how well the model understands them\n",
    "\n",
    "print(\"=== Banking Acronyms: How the Model Sees Them ===\")\n",
    "print()\n",
    "\n",
    "BANKING_VOCAB = {\n",
    "    \"AML\":  \"Anti-Money Laundering\",\n",
    "    \"KYC\":  \"Know Your Customer\",\n",
    "    \"BSA\":  \"Bank Secrecy Act\",\n",
    "    \"SAR\":  \"Suspicious Activity Report\",\n",
    "    \"CTR\":  \"Currency Transaction Report\",\n",
    "    \"PEP\":  \"Politically Exposed Person\",\n",
    "    \"CDD\":  \"Customer Due Diligence\",\n",
    "    \"EDD\":  \"Enhanced Due Diligence\",\n",
    "    \"SWIFT\": \"Society for Worldwide Interbank Financial Telecommunication\",\n",
    "    \"FATF\": \"Financial Action Task Force\",\n",
    "    \"OFAC\": \"Office of Foreign Assets Control\",\n",
    "    \"CET1\": \"Common Equity Tier 1\",\n",
    "    \"RWA\":  \"Risk-Weighted Assets\",\n",
    "    \"LTV\":  \"Loan-to-Value\",\n",
    "    \"DTI\":  \"Debt-to-Income\",\n",
    "    \"FICO\": \"Fair Isaac Corporation score\",\n",
    "    \"VASP\": \"Virtual Asset Service Provider\",\n",
    "    \"MiCA\": \"Markets in Crypto-Assets\",\n",
    "    \"DORA\": \"Digital Operational Resilience Act\",\n",
    "}\n",
    "\n",
    "print(f\"{'Acronym':<8} {'Token IDs':<25} {'Tok count':>9}  {'Decoded tokens':<30} Full form\")\n",
    "print(\"-\" * 110)\n",
    "for acronym, full_form in BANKING_VOCAB.items():\n",
    "    ids     = enc.encode(acronym)\n",
    "    decoded = [enc.decode([i]) for i in ids]\n",
    "    dec_str = \" | \".join(repr(t) for t in decoded)\n",
    "    print(f\"{acronym:<8} {str(ids):<25} {len(ids):>9}  {dec_str:<30} {full_form[:45]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical insight: acronym vs full form ‚Äî which is better in prompts?\n",
    "print(\"=== Acronym vs Full Form: Token Efficiency ===\")\n",
    "print()\n",
    "\n",
    "comparisons = [\n",
    "    (\"AML\",    \"anti-money laundering\"),\n",
    "    (\"KYC\",    \"know your customer\"),\n",
    "    (\"SAR\",    \"suspicious activity report\"),\n",
    "    (\"SWIFT\",  \"Society for Worldwide Interbank Financial Telecommunication\"),\n",
    "    (\"Basel III\", \"Basel Three capital framework\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Acronym':<12} {'Tokens':>8}  {'Full form':<55} {'Tokens':>8}  Savings\")\n",
    "print(\"-\" * 100)\n",
    "for short, long in comparisons:\n",
    "    t_short = len(enc.encode(short))\n",
    "    t_long  = len(enc.encode(long))\n",
    "    savings = t_long - t_short\n",
    "    print(f\"{short:<12} {t_short:>8}  {long:<55} {t_long:>8}  -{savings} tokens\")\n",
    "\n",
    "print()\n",
    "print(\"üìå Use acronyms in prompts to save tokens ‚Äî but DEFINE them first.\")\n",
    "print(\"   'AML (anti-money laundering)' costs a few more tokens once,\")\n",
    "print(\"   but then 'AML' alone is cheaper throughout the rest of the prompt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline: text ‚Üí tokens ‚Üí token IDs ‚Üí (conceptually) ‚Üí embeddings\n",
    "print(\"=== Full Pipeline: Text ‚Üí Tokens ‚Üí IDs ‚Üí Embeddings (conceptual) ===\")\n",
    "print()\n",
    "\n",
    "sample = \"The AML team flagged a suspicious wire transfer for SAR filing.\"\n",
    "ids    = enc.encode(sample)\n",
    "decoded = [enc.decode([i]) for i in ids]\n",
    "\n",
    "print(f\"Step 0 ‚Äî Raw text:\")\n",
    "print(f\"  '{sample}'\")\n",
    "print()\n",
    "\n",
    "print(f\"Step 1 ‚Äî Tokenize:\")\n",
    "for i, (token, tid) in enumerate(zip(decoded, ids)):\n",
    "    bar = \"‚ñà\" * min(tid // 5000, 20)  # rough visual of ID magnitude\n",
    "    print(f\"  [{i:>2}] id={tid:>6}  token={token!r:<18}  {bar}\")\n",
    "print()\n",
    "\n",
    "print(f\"Step 2 ‚Äî Each ID is looked up in an embedding table:\")\n",
    "print(f\"  Vocab size: {enc.n_vocab:,} rows\")\n",
    "print(f\"  Embedding dim: 768 (for cl100k models), 4096 (for GPT-4)\")\n",
    "print(f\"  Each row: a 768-dimensional vector learned during model training\")\n",
    "print()\n",
    "print(f\"  id {ids[0]} ‚Üí embedding table row {ids[0]} ‚Üí [0.12, -0.34, 0.88, ...]  (768 numbers)\")\n",
    "print(f\"  id {ids[1]} ‚Üí embedding table row {ids[1]} ‚Üí [-0.05, 0.72, 0.11, ...]  (768 numbers)\")\n",
    "print(f\"  ... and so on for all {len(ids)} tokens\")\n",
    "print()\n",
    "\n",
    "print(f\"Step 3 ‚Äî Transformer processes the sequence of {len(ids)} vectors\")\n",
    "print(f\"  Self-attention: each token 'looks at' all other tokens\")\n",
    "print(f\"  Output: {len(ids)} context-aware vectors (one per token)\")\n",
    "print()\n",
    "print(f\"Step 4 ‚Äî Pool or use the final representations for your task\")\n",
    "print(f\"  Classification: use [CLS] token\")\n",
    "print(f\"  Sentence embedding: mean-pool all token vectors  ‚Üê this is Notebook A\")\n",
    "print(f\"  Generation: predict next token ID from the last vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-exercise-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Hands-On Exercise: Analyze Your Own Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste any banking text here and analyze it\n",
    "YOUR_TEXT = \"\"\"\n",
    "TODO: Paste your own banking text here.\n",
    "Examples:\n",
    "- A paragraph from your internal AML policy\n",
    "- A loan product description\n",
    "- A regulatory requirement from Basel III or MiFID\n",
    "- A customer email about a suspicious transaction\n",
    "\"\"\"\n",
    "\n",
    "if \"TODO\" not in YOUR_TEXT:\n",
    "    ids     = enc.encode(YOUR_TEXT.strip())\n",
    "    decoded = [enc.decode([i]) for i in ids]\n",
    "    words   = len(YOUR_TEXT.split())\n",
    "\n",
    "    print(f\"=== Your Text Analysis ===\")\n",
    "    print(f\"Characters: {len(YOUR_TEXT)}\")\n",
    "    print(f\"Words:      {words}\")\n",
    "    print(f\"Tokens:     {len(ids)}\")\n",
    "    print(f\"Tok/word:   {len(ids)/max(words,1):.2f}\")\n",
    "    print()\n",
    "    print(\"Token breakdown:\")\n",
    "    for i, (token, tid) in enumerate(zip(decoded, ids)):\n",
    "        print(f\"  [{i:>3}] {tid:>7}  {token!r}\")\n",
    "    print()\n",
    "    print(\"Questions:\")\n",
    "    print(\"  1. Which terms split into multiple tokens?\")\n",
    "    print(\"  2. Which acronyms are a single token?\")\n",
    "    print(\"  3. How many tokens would this use in a 128K context window?\")\n",
    "else:\n",
    "    print(\"Replace YOUR_TEXT above with your own banking text to analyze it.\")\n",
    "    print(\"Then re-run this cell.\")\n",
    "    print()\n",
    "    # Demonstrate with a sample instead\n",
    "    demo = \"The SAR filing deadline is within 30 days of detecting suspicious activity.\"\n",
    "    ids = enc.encode(demo)\n",
    "    decoded = [enc.decode([i]) for i in ids]\n",
    "    print(f\"Demo: '{demo}'\")\n",
    "    print(f\"Tokens ({len(ids)}):\")\n",
    "    for i, (token, tid) in enumerate(zip(decoded, ids)):\n",
    "        print(f\"  [{i:>2}] {tid:>7}  {token!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group exercise: Token budget for a RAG prompt\n",
    "# In Session 3 you will build RAG ‚Äî prompts look like this\n",
    "\n",
    "print(\"=== RAG Prompt Token Budget Exercise ===\")\n",
    "print()\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a banking compliance assistant. Answer questions using only \n",
    "the provided context. If the answer is not in the context, say so.\"\"\"\n",
    "\n",
    "RETRIEVED_CHUNK = \"\"\"[Context from AML Policy, Section 4.2]\n",
    "Wire transfers exceeding $10,000 USD must be reported to FinCEN via a Currency \n",
    "Transaction Report (CTR) within 15 calendar days. Structuring transactions to \n",
    "avoid the $10,000 threshold is a federal crime under 31 U.S.C. 5324. All wire \n",
    "transfers to or from FATF high-risk jurisdictions require Enhanced Due Diligence \n",
    "and senior management approval.\"\"\"\n",
    "\n",
    "USER_QUERY = \"What is the reporting threshold for wire transfers and what form is used?\"\n",
    "\n",
    "EXPECTED_ANSWER = \"\"\"Based on the policy, wire transfers exceeding $10,000 USD must \n",
    "be reported using a Currency Transaction Report (CTR) within 15 calendar days.\"\"\"\n",
    "\n",
    "parts = {\n",
    "    \"System prompt\":     SYSTEM_PROMPT,\n",
    "    \"Retrieved context\": RETRIEVED_CHUNK,\n",
    "    \"User query\":        USER_QUERY,\n",
    "    \"Expected answer\":   EXPECTED_ANSWER,\n",
    "}\n",
    "\n",
    "total = 0\n",
    "print(f\"{'Part':<22} {'Tokens':>8}\")\n",
    "print(\"-\" * 32)\n",
    "for part_name, text in parts.items():\n",
    "    t = len(enc.encode(text))\n",
    "    total += t\n",
    "    print(f\"{part_name:<22} {t:>8}\")\n",
    "print(\"-\" * 32)\n",
    "print(f\"{'TOTAL':<22} {total:>8}\")\n",
    "print()\n",
    "print(f\"Remaining in 128K context window: {128_000 - total:,} tokens\")\n",
    "print(f\"= room for ~{(128_000 - total) // 500} more retrieved chunks (avg 500 tok each)\")\n",
    "print()\n",
    "print(\"üìå In Session 3, you will build this pipeline end-to-end.\")\n",
    "print(\"   Token counting determines how many chunks you can include,\")\n",
    "print(\"   which directly affects answer quality and completeness.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: What You Experienced\n",
    "\n",
    "| Part | Concept | Key Takeaway |\n",
    "|------|---------|-------------|\n",
    "| 1. The problem | Why tokenization exists | LLMs need integers, not strings |\n",
    "| 2. Character | Na√Øve approach | 1 char = 1 token ‚Üí too long, no meaning sharing |\n",
    "| 3. Word | Better, but broken | OOV problem ‚Äî 'FATF' and 'MiCA' become `<UNK>` |\n",
    "| 4. BPE (yours) | You trained it | Merges frequent pairs ‚Üí subwords, no OOV |\n",
    "| 5. tiktoken | The real thing | 100K vocab, banking terms get 1-3 tokens |\n",
    "| 6. Counting | Cost & context | ~1.3 tok/word; 200-page manual > most context windows |\n",
    "| 7. Surprises | What breaks | Numbers, capitalization, spaces, language cost |\n",
    "| 8. Banking | Domain analysis | AML=1 tok, FATF=2, anti-money-laundering=5 |\n",
    "\n",
    "### How This Connects to the Other Notebooks\n",
    "\n",
    "```\n",
    "Session 1.5C (this notebook) ‚Äî TOKENIZATION\n",
    "  Text ‚Üí token IDs  (tiktoken, BPE)\n",
    "         ‚Üì\n",
    "Session 1.5B ‚Äî EMBEDDINGS (Word2Vec, static)\n",
    "  Token IDs ‚Üí lookup in embedding table ‚Üí 50-dim vectors\n",
    "  One vector per word, trained by YOU on banking corpus\n",
    "         ‚Üì\n",
    "Session 1.5A ‚Äî CONTEXTUAL EMBEDDINGS (sentence-transformers)\n",
    "  Full sentence ‚Üí transformer ‚Üí 384-dim vector\n",
    "  'bank' gets different vector per sentence\n",
    "         ‚Üì\n",
    "Session 3   ‚Äî RAG\n",
    "  Documents ‚Üí chunk ‚Üí embed ‚Üí store in vector DB\n",
    "  Query ‚Üí embed ‚Üí find similar chunks ‚Üí LLM answers\n",
    "```\n",
    "\n",
    "### Practical Rules for Banking LLM Systems\n",
    "\n",
    "1. **Count tokens before sending** ‚Äî avoid context overflow on long documents  \n",
    "2. **Use acronyms in prompts** ‚Äî AML (3 chars, 1 token) not anti-money-laundering (5 tokens)  \n",
    "3. **Normalize numbers** ‚Äî decide on $10,000 vs 10000 vs 10k before embedding  \n",
    "4. **Multilingual = more tokens** ‚Äî Arabic/Chinese compliance text costs 2-4x more  \n",
    "5. **Chunk on token boundaries** ‚Äî not on word or character count for accurate sizing  \n",
    "6. **RAG is the answer to context limits** ‚Äî retrieve, not fit everything"
   ]
  }
 ]
}