{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Session 1.5B: Embeddings Deep Dive ‚Äî Train Your Own (No Heavy Dependencies)\n",
    "\n",
    "**Only requires:** `pip install gensim` (~5MB, no torch, no GPU)  \n",
    "**Works behind:** corporate proxies, air-gapped environments  \n",
    "**Focus:** Experience the TRAINING process, not just inference\n",
    "\n",
    "---\n",
    "\n",
    "## What You Will Experience\n",
    "\n",
    "```\n",
    "Part 1 ‚Üí Build a banking corpus by hand (you control what the model learns)\n",
    "Part 2 ‚Üí Train Word2Vec from scratch ‚Äî watch it learn in seconds\n",
    "Part 3 ‚Üí Word-level embeddings ‚Äî similar words cluster together\n",
    "Part 4 ‚Üí Sentence-level ‚Äî average word vectors, see the limitation\n",
    "Part 5 ‚Üí Context-awareness ‚Äî hit the ceiling of static embeddings\n",
    "Part 6 ‚Üí Bias ‚Äî feed biased data, get biased embeddings\n",
    "Part 7 ‚Üí Visualize ‚Äî 2D plot of your banking embedding space\n",
    "Part 8 ‚Üí Retrain ‚Äî change the corpus, watch the space shift\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-setup-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only one install needed ‚Äî ~5MB, no torch, no GPU\n",
    "!pip install -q gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import collections\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "print(\"‚úÖ Ready. No API keys. No heavy downloads. Just Python + gensim.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Build Your Banking Corpus\n",
    "\n",
    "**Key idea:** The model learns ONLY from sentences you give it.  \n",
    "Words that appear near each other often ‚Üí similar vectors.  \n",
    "You control what it knows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each sentence is a list of words (tokens)\n",
    "# Word2Vec learns from co-occurrence within a sliding window\n",
    "\n",
    "BANKING_CORPUS = [\n",
    "    # AML / Compliance cluster\n",
    "    \"aml compliance team monitors suspicious transactions daily\".split(),\n",
    "    \"kyc procedures require customer identification and verification\".split(),\n",
    "    \"bsa requires banks to file suspicious activity reports\".split(),\n",
    "    \"aml analysts review flagged transactions for money laundering\".split(),\n",
    "    \"kyc onboarding collects customer documents and identity proof\".split(),\n",
    "    \"compliance officers ensure aml and kyc policies are followed\".split(),\n",
    "    \"suspicious transactions trigger aml alerts for review\".split(),\n",
    "    \"money laundering detection requires robust aml controls\".split(),\n",
    "    \"bsa officer reviews ctr and sar filings for compliance\".split(),\n",
    "    \"kyc refresh is required annually for high risk customers\".split(),\n",
    "\n",
    "    # Fraud cluster\n",
    "    \"fraud detection models flag anomalous transaction patterns\".split(),\n",
    "    \"fraud analysts investigate unauthorized card transactions\".split(),\n",
    "    \"fraud prevention uses machine learning to detect anomalies\".split(),\n",
    "    \"chargeback process resolves disputed fraud transactions\".split(),\n",
    "    \"fraud risk increases during holiday shopping seasons\".split(),\n",
    "    \"account takeover fraud involves stolen customer credentials\".split(),\n",
    "    \"fraud alerts are sent to customers via sms and email\".split(),\n",
    "\n",
    "    # Capital / Risk cluster\n",
    "    \"capital adequacy ratio measures bank financial strength\".split(),\n",
    "    \"credit risk assessment evaluates borrower default probability\".split(),\n",
    "    \"market risk arises from changes in interest rates and prices\".split(),\n",
    "    \"basel three framework sets minimum capital requirements\".split(),\n",
    "    \"stress testing evaluates bank resilience under adverse scenarios\".split(),\n",
    "    \"risk weighted assets determine required capital buffers\".split(),\n",
    "    \"liquidity risk management ensures bank can meet obligations\".split(),\n",
    "    \"capital ratio must exceed regulatory minimum requirements\".split(),\n",
    "\n",
    "    # Retail Banking cluster\n",
    "    \"mortgage loan approval depends on credit score and income\".split(),\n",
    "    \"savings account earns interest on deposited customer funds\".split(),\n",
    "    \"credit card spending limit is set based on creditworthiness\".split(),\n",
    "    \"overdraft fee is charged when account balance goes negative\".split(),\n",
    "    \"personal loan application requires income verification documents\".split(),\n",
    "    \"mortgage rate depends on fed funds rate and credit score\".split(),\n",
    "    \"retail banking serves individual customers with daily needs\".split(),\n",
    "    \"branch teller processes deposits withdrawals and transfers\".split(),\n",
    "\n",
    "    # Payments / Wires cluster\n",
    "    \"wire transfer sends funds between banks via swift network\".split(),\n",
    "    \"payment processing requires sender and receiver account details\".split(),\n",
    "    \"swift code identifies the receiving bank for international wires\".split(),\n",
    "    \"wire transfers above ten thousand dollars require reporting\".split(),\n",
    "    \"payment gateway authorizes card transactions in real time\".split(),\n",
    "    \"international wire transfer fees vary by destination country\".split(),\n",
    "]\n",
    "\n",
    "print(f\"Corpus built: {len(BANKING_CORPUS)} sentences\")\n",
    "print(f\"Total words:  {sum(len(s) for s in BANKING_CORPUS)}\")\n",
    "print(f\"\\nSample sentence: {BANKING_CORPUS[0]}\")\n",
    "print(\"\\nüìå This corpus is YOUR model's entire world.\")\n",
    "print(\"   It knows nothing outside these sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency ‚Äî what the model will see most\n",
    "from collections import Counter\n",
    "\n",
    "all_words = [w for sentence in BANKING_CORPUS for w in sentence]\n",
    "freq = Counter(all_words).most_common(20)\n",
    "\n",
    "print(\"Top 20 words in corpus:\")\n",
    "for word, count in freq:\n",
    "    bar = \"‚ñà\" * count\n",
    "    print(f\"  {word:<20} {count:>3}  {bar}\")\n",
    "\n",
    "print(\"\\nüìå High-frequency words will have better-trained vectors.\")\n",
    "print(\"   Rare words (appear once) will have poor representations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Train Word2Vec From Scratch\n",
    "\n",
    "**What happens during training:**\n",
    "```\n",
    "For each word in each sentence:\n",
    "  Look at surrounding words (window)\n",
    "  Adjust vectors so nearby words predict each other\n",
    "  Words in similar contexts ‚Üí similar vectors\n",
    "\n",
    "\"aml compliance team monitors...\"\n",
    "  aml  ‚Üê‚Üí  compliance, team, monitors   (window=2)\n",
    "  aml  ‚Üê‚Üí  kyc, bsa, suspicious         (across sentences)\n",
    "  Result: aml vector ‚âà kyc vector ‚âà bsa vector\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Training Word2Vec on banking corpus...\")\n",
    "t0 = time.time()\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=BANKING_CORPUS,\n",
    "    vector_size=50,    # Each word ‚Üí 50-dimensional vector (tiny but sufficient)\n",
    "    window=3,          # Look 3 words left and right\n",
    "    min_count=1,       # Include all words (small corpus)\n",
    "    workers=1,         # Single thread (reproducible)\n",
    "    epochs=200,        # Train 200 passes over corpus\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "vocab = list(model.wv.key_to_index.keys())\n",
    "\n",
    "print(f\"‚úÖ Training complete in {elapsed:.2f} seconds\")\n",
    "print(f\"   Vocabulary size: {len(vocab)} unique words\")\n",
    "print(f\"   Vector dimensions: {model.vector_size}\")\n",
    "print(f\"   Training epochs: 200\")\n",
    "print(f\"\\nSample vocabulary: {sorted(vocab)[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a raw vector\n",
    "word = \"aml\"\n",
    "vector = model.wv[word]\n",
    "\n",
    "print(f\"Vector for '{word}':\")\n",
    "print(f\"  Dimensions: {len(vector)}\")\n",
    "print(f\"  Values: {vector.round(3)}\")\n",
    "print(f\"  Range: [{vector.min():.3f}, {vector.max():.3f}]\")\n",
    "print(f\"\\nüìå These numbers encode '{word}'s meaning based on its context in YOUR corpus.\")\n",
    "print(f\"   They have no inherent meaning alone ‚Äî only relative distances matter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kaq4ug4mgq",
   "source": "---\n## Part 2B: Inside the Dimensions ‚Äî What Do the 50 Numbers Actually Encode?\n\nEach word vector has 50 numbers. **No single number has a human label.**  \nBut patterns emerge: dimensions that fire high for compliance words stay quiet for retail words.  \nThis section lets you peer inside and build intuition for what \"meaning in numbers\" looks like.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "h4cfzsql42d",
   "source": "# Step 1: Print all 50 dimensions for a single word ‚Äî side by side with a bar chart\ndef show_dimensions(word, m=model, top_n=50):\n    \"\"\"Print all dimensions of a word vector as a horizontal bar chart.\"\"\"\n    if word not in m.wv:\n        print(f\"'{word}' not in vocabulary\")\n        return\n    vec = list(m.wv[word])\n    vmax = max(abs(v) for v in vec)\n\n    print(f\"\\nAll {len(vec)} dimensions for '{word}':\")\n    print(f\"  Range: [{min(vec):.3f}, {max(vec):.3f}]\")\n    print()\n    print(f\"  {'Dim':<6} {'Value':>8}  {'Bar (positive=‚ñà, negative=‚ñí)'}\")\n    print(f\"  {'---':<6} {'-----':>8}  {'-' * 40}\")\n\n    for i, v in enumerate(vec):\n        bar_len = int(abs(v) / (vmax + 1e-9) * 20)\n        bar = (\"‚ñà\" * bar_len) if v >= 0 else (\"‚ñí\" * bar_len)\n        sign = \"+\" if v >= 0 else \"-\"\n        print(f\"  dim{i:<3} {v:>8.4f}  {sign} {bar}\")\n\nshow_dimensions(\"aml\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "vyf9bsd91a",
   "source": "# Step 2: Compare the same dimensions across multiple words ‚Äî see which dims differ\n# This is the key insight: similar words have similar patterns across ALL 50 dims\n\nINSPECT_WORDS = [\"aml\", \"kyc\", \"fraud\", \"mortgage\", \"wire\"]\n\n# Print a compact comparison table: words as columns, dims as rows\n# Show every 5th dimension to keep it readable\ndef compare_dimensions(words, m=model, stride=5):\n    \"\"\"Show dimension values across multiple words in a table.\"\"\"\n    vecs = {}\n    for w in words:\n        if w in m.wv:\n            vecs[w] = list(m.wv[w])\n        else:\n            print(f\"  Warning: '{w}' not in vocabulary, skipping\")\n\n    if not vecs:\n        return\n\n    dim_count = len(next(iter(vecs.values())))\n    word_list = list(vecs.keys())\n\n    print(\"=== Dimension Comparison Across Words ===\")\n    print(f\"Showing every {stride} dimensions (dim 0, {stride}, {stride*2}, ...)\")\n    print(f\"Words: {word_list}\")\n    print()\n\n    # Header\n    header = f\"  {'Dim':<6}\" + \"\".join(f\"{w:>10}\" for w in word_list)\n    print(header)\n    print(\"  \" + \"-\" * (6 + 10 * len(word_list)))\n\n    for i in range(0, dim_count, stride):\n        row = f\"  dim{i:<3}\"\n        for w in word_list:\n            val = vecs[w][i]\n            row += f\"  {val:>8.4f}\"\n        print(row)\n\n    print()\n    print(\"üìå Notice: 'aml' and 'kyc' have similar patterns across dims.\")\n    print(\"   'mortgage' looks very different ‚Äî its training context was different.\")\n    print(\"   These patterns ARE the meaning ‚Äî no individual dim has a label.\")\n\ncompare_dimensions(INSPECT_WORDS, stride=5)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "u50hgn484a",
   "source": "# Step 3: Find the dimensions that MOST DIFFER between clusters\n# These are the \"most informative\" dimensions ‚Äî they separate compliance from retail\n\ndef most_discriminating_dims(group_a, group_b, m=model, top_n=10):\n    \"\"\"\n    Find dimensions that differ most between two word groups.\n    group_a, group_b: lists of words\n    Returns top_n dimensions ranked by |mean_a - mean_b|\n    \"\"\"\n    def group_mean_vec(words):\n        vecs = [list(m.wv[w]) for w in words if w in m.wv]\n        if not vecs:\n            return None\n        dim_count = len(vecs[0])\n        return [sum(v[i] for v in vecs) / len(vecs) for i in range(dim_count)]\n\n    mean_a = group_mean_vec(group_a)\n    mean_b = group_mean_vec(group_b)\n    if mean_a is None or mean_b is None:\n        print(\"  One group has no vocabulary overlap.\")\n        return\n\n    # Rank dimensions by absolute difference\n    diffs = [(i, abs(mean_a[i] - mean_b[i]), mean_a[i], mean_b[i])\n             for i in range(len(mean_a))]\n    diffs.sort(key=lambda x: -x[1])\n\n    print(f\"Top {top_n} most discriminating dimensions:\")\n    print(f\"  Group A: {group_a}\")\n    print(f\"  Group B: {group_b}\")\n    print()\n    print(f\"  {'Dim':<7} {'|Diff|':>8}  {'GroupA mean':>13}  {'GroupB mean':>13}  {'Which is higher?'}\")\n    print(\"  \" + \"-\" * 65)\n    for i, diff, va, vb in diffs[:top_n]:\n        higher = \"‚Üê A higher\" if va > vb else \"‚Üê B higher\"\n        print(f\"  dim{i:<4} {diff:>8.4f}  {va:>13.4f}  {vb:>13.4f}  {higher}\")\n\n    print()\n    print(\"üìå These dims are NOT labelled 'compliance' or 'retail' by the algorithm.\")\n    print(\"   But numerically, they are what separates the two clusters in 50D space.\")\n\n# Compliance cluster vs Retail cluster\nmost_discriminating_dims(\n    group_a=[\"aml\", \"kyc\", \"bsa\", \"compliance\", \"suspicious\"],\n    group_b=[\"mortgage\", \"savings\", \"overdraft\", \"retail\", \"loan\"]\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9uohvyvnl87",
   "source": "# Step 4: ASCII heatmap ‚Äî visualize ALL 50 dims across ALL clusters at once\n# Each cell is one dimension value, rendered as a shade character\n\ndef ascii_heatmap(word_groups, m=model):\n    \"\"\"\n    Render a heatmap of dimension values.\n    Rows = word clusters, Columns = dimensions 0..49\n    Shade characters: ' ' (low) ‚Üí '¬∑' ‚Üí '+' ‚Üí '‚ñà' (high absolute value)\n    \"\"\"\n    SHADES = [\" \", \"¬∑\", \"‚ñë\", \"‚ñí\", \"‚ñì\", \"‚ñà\"]\n\n    # Compute per-group mean vectors\n    groups = {}\n    for cluster_name, words in word_groups.items():\n        vecs = [list(m.wv[w]) for w in words if w in m.wv]\n        if vecs:\n            dim_count = len(vecs[0])\n            groups[cluster_name] = [\n                sum(v[i] for v in vecs) / len(vecs)\n                for i in range(dim_count)\n            ]\n\n    if not groups:\n        print(\"No words found in vocabulary.\")\n        return\n\n    # Global normalization\n    all_vals = [abs(v) for vec in groups.values() for v in vec]\n    vmax = max(all_vals) if all_vals else 1.0\n\n    dim_count = len(next(iter(groups.values())))\n\n    print(\"=== Dimension Heatmap: All 50 Dims √ó 5 Clusters ===\")\n    print(f\"Shade: ' '=near 0  '¬∑'=small  '‚ñë'=medium  '‚ñí‚ñì‚ñà'=large absolute value\")\n    print(f\"Each column = one dimension (0 ‚Üí {dim_count-1})\")\n    print()\n\n    # Column headers every 10 dims\n    header = f\"  {'Cluster':<14}|\"\n    for i in range(0, dim_count, 10):\n        header += f\"{i:<10}\"\n    print(header)\n    print(\"  \" + \"-\" * (15 + dim_count))\n\n    for cluster_name, vec in groups.items():\n        row = f\"  {cluster_name:<14}|\"\n        for val in vec:\n            intensity = int(abs(val) / (vmax + 1e-9) * (len(SHADES) - 1))\n            row += SHADES[intensity]\n        print(row)\n\n    print()\n    print(\"üìå You can visually see that Compliance and Fraud share some 'bright' dims\")\n    print(\"   (they appear in similar sentence positions) while Retail dims are quieter.\")\n    print(\"   PCA works by rotating this 50-column picture to find the axes of\")\n    print(\"   maximum variance ‚Äî collapsing it to just 2 dimensions for plotting.\")\n\nascii_heatmap({\n    \"Compliance\": [\"aml\", \"kyc\", \"bsa\", \"compliance\", \"suspicious\"],\n    \"Fraud\":      [\"fraud\", \"anomalous\", \"unauthorized\", \"chargeback\"],\n    \"Capital\":    [\"capital\", \"risk\", \"credit\", \"basel\"],\n    \"Retail\":     [\"mortgage\", \"savings\", \"overdraft\", \"loan\"],\n    \"Payments\":   [\"wire\", \"swift\", \"payment\", \"transfer\"],\n})",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "y8yhr5gngo",
   "source": "# Step 5: Spotlight ‚Äî pick any single dimension and rank ALL vocab words by that dim\n# This answers \"what does dimension 7 actually represent?\"\n\ndef spotlight_dim(dim_index, m=model, top_n=10):\n    \"\"\"\n    Show which words score highest and lowest on a single dimension.\n    This builds intuition for what that dimension 'detects'.\n    \"\"\"\n    vocab = list(m.wv.key_to_index.keys())\n    scores = [(w, float(m.wv[w][dim_index])) for w in vocab]\n    scores.sort(key=lambda x: -x[1])\n\n    print(f\"=== Spotlight on Dimension {dim_index} ===\")\n    print()\n    print(f\"  TOP words (high positive value on dim {dim_index}):\")\n    for w, s in scores[:top_n]:\n        bar = \"‚ñà\" * int(abs(s) / (abs(scores[0][1]) + 1e-9) * 15)\n        print(f\"    {w:<20} {s:>8.4f}  +{bar}\")\n\n    print()\n    print(f\"  BOTTOM words (high negative value on dim {dim_index}):\")\n    for w, s in scores[-top_n:]:\n        bar = \"‚ñí\" * int(abs(s) / (abs(scores[-1][1]) + 1e-9) * 15)\n        print(f\"    {w:<20} {s:>8.4f}  -{bar}\")\n\n    print()\n    print(f\"  Do the top words share a theme? Do the bottom words share another?\")\n    print(f\"  If yes ‚Äî this dimension partially encodes that theme.\")\n    print(f\"  If the pattern looks random ‚Äî this dim captures a mix of features.\")\n    print(f\"  Individual dimensions are rarely interpretable; their combination is.\")\n\n# Try a few dimensions ‚Äî look for any emergent themes\nfor d in [0, 7, 15, 23]:\n    spotlight_dim(d, top_n=6)\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9bkg0nukauf",
   "source": "# Step 6: Dimension distance ‚Äî two words, dimension by dimension\n# See exactly WHERE in the 50-dim space they agree and disagree\n\ndef dim_by_dim_compare(w1, w2, m=model, top_n=10):\n    \"\"\"\n    Compare two word vectors dimension by dimension.\n    Highlights dims where they are most similar and most different.\n    \"\"\"\n    if w1 not in m.wv or w2 not in m.wv:\n        print(f\"One of '{w1}', '{w2}' not in vocabulary.\")\n        return\n\n    v1, v2 = list(m.wv[w1]), list(m.wv[w2])\n    diffs = [(i, v1[i], v2[i], abs(v1[i] - v2[i])) for i in range(len(v1))]\n\n    # Sort by most different\n    most_diff = sorted(diffs, key=lambda x: -x[3])\n    # Sort by most similar (smallest diff, but both non-zero)\n    most_same = sorted(diffs, key=lambda x: x[3])\n\n    print(f\"=== Dimension-by-Dimension: '{w1}' vs '{w2}' ===\")\n    print(f\"  Cosine similarity: {cosine_sim(w1, w2):.4f}\")\n    print()\n\n    print(f\"  Top {top_n} dims where they DIFFER MOST:\")\n    print(f\"  {'Dim':<7} {w1:>10} {w2:>10}  {'|Diff|':>8}  Visual\")\n    print(\"  \" + \"-\" * 55)\n    for i, a, b, d in most_diff[:top_n]:\n        bar_a = \"‚ñà\" * int(abs(a) / 0.5 * 8)\n        bar_b = \"‚ñë\" * int(abs(b) / 0.5 * 8)\n        print(f\"  dim{i:<4} {a:>10.4f} {b:>10.4f}  {d:>8.4f}  [{bar_a}|{bar_b}]\")\n\n    print()\n    print(f\"  Top {top_n} dims where they AGREE MOST (closest values):\")\n    print(f\"  {'Dim':<7} {w1:>10} {w2:>10}  {'|Diff|':>8}\")\n    print(\"  \" + \"-\" * 42)\n    for i, a, b, d in most_same[:top_n]:\n        print(f\"  dim{i:<4} {a:>10.4f} {b:>10.4f}  {d:>8.4f}  ‚Üê nearly equal\")\n\n    print()\n    print(\"üìå The dims where they agree ‚Üí shared context (both banking terms).\")\n    print(\"   The dims where they differ ‚Üí the SEMANTIC DISTANCE between them.\")\n    print(\"   Cosine similarity takes all 50 into account at once.\")\n\n# Close pair (should mostly agree)\ndim_by_dim_compare(\"aml\", \"kyc\")\nprint()\n# Distant pair (should differ on many dims)\ndim_by_dim_compare(\"aml\", \"mortgage\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d12wpq46rpb",
   "source": "### Key Insight: Why Individual Dimensions Don't Have Names\n\n```\nWord2Vec training objective: predict surrounding words.\nIt has NO instruction to make dim-7 = \"compliance-ness\".\n\nWhat actually happens:\n  The 50 dimensions are free parameters.\n  The optimizer distributes meaning across ALL of them.\n  A single dimension might weakly correlate with compliance,\n  weakly correlate with formality, and weakly anti-correlate with\n  informality ‚Äî all at once.\n\n  ‚Üí Meaning is encoded in the COMBINATION, not in individual dims.\n  ‚Üí This is why you need cosine similarity across all 50 at once.\n  ‚Üí This is also why PCA can find structure: it finds the directions\n     of maximum variance in this 50-column space ‚Äî the real axes\n     of meaning your model discovered.\n\nModern models (768-dim BERT, 1536-dim OpenAI) work the same way,\njust with far more capacity. The interpretation problem is identical:\nno single dimension = one concept.\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-part3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Word-Level Embeddings ‚Äî Similar Words Cluster Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_similar: find words with closest vectors\n",
    "probe_words = [\"aml\", \"fraud\", \"mortgage\", \"capital\", \"wire\"]\n",
    "\n",
    "print(\"=== Most Similar Words by Vector Distance ===\")\n",
    "for word in probe_words:\n",
    "    if word not in model.wv:\n",
    "        print(f\"  '{word}' not in vocabulary\")\n",
    "        continue\n",
    "    similar = model.wv.most_similar(word, topn=5)\n",
    "    print(f\"\\n'{word}' ‚Üí most similar:\")\n",
    "    for w, score in similar:\n",
    "        bar = \"‚ñà\" * int(score * 20)\n",
    "        print(f\"  {w:<20} {score:.3f}  {bar}\")\n",
    "\n",
    "print(\"\\nüìå Words that appear in similar sentences cluster together.\")\n",
    "print(\"   'aml' is close to 'kyc' and 'bsa' because they co-occur.\")\n",
    "print(\"   'aml' is far from 'mortgage' ‚Äî different sentences, different context.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity between specific pairs\n",
    "def cosine_sim(w1, w2):\n",
    "    \"\"\"Cosine similarity between two word vectors. stdlib only.\"\"\"\n",
    "    v1, v2 = model.wv[w1], model.wv[w2]\n",
    "    dot    = sum(a * b for a, b in zip(v1, v2))\n",
    "    norm1  = math.sqrt(sum(a * a for a in v1))\n",
    "    norm2  = math.sqrt(sum(b * b for b in v2))\n",
    "    return dot / (norm1 * norm2)\n",
    "\n",
    "pairs = [\n",
    "    (\"aml\",      \"kyc\",       \"Both compliance, same sentences\"),\n",
    "    (\"aml\",      \"bsa\",       \"Both regulatory, same sentences\"),\n",
    "    (\"fraud\",    \"suspicious\",\"Co-occur in fraud sentences\"),\n",
    "    (\"mortgage\", \"credit\",    \"Both retail lending terms\"),\n",
    "    (\"aml\",      \"mortgage\",  \"Different clusters\"),\n",
    "    (\"fraud\",    \"capital\",   \"Very different domains\"),\n",
    "    (\"wire\",     \"swift\",     \"Co-occur in payment sentences\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Word A':<12} {'Word B':<12} {'Similarity':<12} {'Reason'}\")\n",
    "print(\"-\" * 70)\n",
    "for w1, w2, reason in pairs:\n",
    "    if w1 not in model.wv or w2 not in model.wv:\n",
    "        continue\n",
    "    sim = cosine_sim(w1, w2)\n",
    "    verdict = \"CLOSE\" if sim > 0.7 else \"RELATED\" if sim > 0.4 else \"DISTANT\"\n",
    "    print(f\"{w1:<12} {w2:<12} {sim:<12.3f} {verdict} ‚Äî {reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analogy: king - man + woman = queen (classic Word2Vec demo)\n",
    "# Banking version: aml - compliance + fraud = ?\n",
    "print(\"=== Word Analogies ===\")\n",
    "print(\"Logic: A is to B as C is to ?\")\n",
    "print(\"Formula: vector(B) - vector(A) + vector(C)\\n\")\n",
    "\n",
    "analogies = [\n",
    "    (\"kyc\", \"compliance\", \"fraud\",    \"kyc:compliance :: fraud:?\"),\n",
    "    (\"mortgage\", \"retail\", \"wire\",    \"mortgage:retail :: wire:?\"),\n",
    "    (\"aml\", \"suspicious\", \"fraud\",    \"aml:suspicious :: fraud:?\"),\n",
    "]\n",
    "\n",
    "for pos1, neg1, pos2, label in analogies:\n",
    "    try:\n",
    "        result = model.wv.most_similar(\n",
    "            positive=[pos2, pos1],\n",
    "            negative=[neg1],\n",
    "            topn=3\n",
    "        )\n",
    "        print(f\"{label}\")\n",
    "        for w, s in result:\n",
    "            print(f\"  ‚Üí '{w}' ({s:.3f})\")\n",
    "        print()\n",
    "    except KeyError as e:\n",
    "        print(f\"  Word not in vocab: {e}\")\n",
    "\n",
    "print(\"üìå Analogies work because the model learns vector DIRECTIONS.\")\n",
    "print(\"   The 'compliance' direction points the same way in fraud space as in aml space.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Sentence-Level Embeddings ‚Äî Average Word Vectors\n",
    "\n",
    "Word2Vec gives word vectors. For sentences, the simplest approach is averaging.  \n",
    "Works reasonably well ‚Äî but has a clear ceiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vector(sentence: str) -> list:\n",
    "    \"\"\"Average word vectors for all known words in sentence.\"\"\"\n",
    "    words = sentence.lower().split()\n",
    "    known = [model.wv[w] for w in words if w in model.wv]\n",
    "    if not known:\n",
    "        return [0.0] * model.vector_size\n",
    "    # Average: sum each dimension, divide by count\n",
    "    avg = [sum(v[i] for v in known) / len(known)\n",
    "           for i in range(model.vector_size)]\n",
    "    return avg\n",
    "\n",
    "def sentence_sim(s1: str, s2: str) -> float:\n",
    "    \"\"\"Cosine similarity between two sentence vectors.\"\"\"\n",
    "    v1, v2 = sentence_vector(s1), sentence_vector(s2)\n",
    "    dot   = sum(a * b for a, b in zip(v1, v2))\n",
    "    n1    = math.sqrt(sum(a * a for a in v1))\n",
    "    n2    = math.sqrt(sum(b * b for b in v2))\n",
    "    return dot / (n1 * n2) if n1 and n2 else 0.0\n",
    "\n",
    "sentence_pairs = [\n",
    "    (\n",
    "        \"aml team monitors suspicious transactions\",\n",
    "        \"compliance analysts review money laundering alerts\",\n",
    "        \"Same meaning, different words\"\n",
    "    ),\n",
    "    (\n",
    "        \"customer failed to provide identity documents\",\n",
    "        \"client did not submit identification for kyc\",\n",
    "        \"Paraphrase\"\n",
    "    ),\n",
    "    (\n",
    "        \"mortgage loan requires credit score verification\",\n",
    "        \"fraud detection flags anomalous wire transfers\",\n",
    "        \"Different topics\"\n",
    "    ),\n",
    "    (\n",
    "        \"basel three capital ratio compliance\",\n",
    "        \"mortgage overdraft retail savings\",\n",
    "        \"Capital vs retail ‚Äî should be distant\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"=== Sentence-Level Similarity ===\")\n",
    "print(f\"{'Sentence A':<45} {'Sentence B':<45} {'Sim':<7} Relationship\")\n",
    "print(\"-\" * 120)\n",
    "for s1, s2, label in sentence_pairs:\n",
    "    sim = sentence_sim(s1, s2)\n",
    "    print(f\"{s1[:43]:<45} {s2[:43]:<45} {sim:.3f}  {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The averaging limitation: word order is lost\n",
    "print(\"=== Averaging Limitation: Word Order Lost ===\")\n",
    "print()\n",
    "\n",
    "pairs = [\n",
    "    (\n",
    "        \"bank approves the loan application\",\n",
    "        \"loan application approves the bank\",  # Nonsense but same words\n",
    "        \"Same words, different order\"\n",
    "    ),\n",
    "    (\n",
    "        \"customer reported fraud to the bank\",\n",
    "        \"bank reported fraud to the customer\",  # Opposite meaning\n",
    "        \"Opposite meaning, same words\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "for s1, s2, label in pairs:\n",
    "    sim = sentence_sim(s1, s2)\n",
    "    print(f\"A: '{s1}'\")\n",
    "    print(f\"B: '{s2}'\")\n",
    "    print(f\"Similarity: {sim:.3f} ‚Äî {label}\")\n",
    "    print(f\"Problem: Averaging gives IDENTICAL vectors for same words in any order!\")\n",
    "    print()\n",
    "\n",
    "print(\"üìå This is WHY contextual models (BERT, sentence-transformers) were invented.\")\n",
    "print(\"   They read the full sequence ‚Äî order and context both matter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Context-Awareness ‚Äî The Ceiling of Static Embeddings\n",
    "\n",
    "Word2Vec is **static**: one word = one vector, always.  \n",
    "The word `\"bank\"` has the same vector whether you mean a financial institution or a river bank.  \n",
    "This is the fundamental limitation that motivated BERT and sentence-transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First: add ambiguous sentences to corpus so 'bank' is in vocabulary\n",
    "AMBIGUOUS_SENTENCES = [\n",
    "    \"the bank approved the mortgage application today\".split(),\n",
    "    \"the bank rejected the loan due to low credit score\".split(),\n",
    "    \"the river bank flooded during the heavy rainstorm\".split(),\n",
    "    \"fishermen stood on the bank waiting for the catch\".split(),\n",
    "    \"we bank with the largest financial institution downtown\".split(),\n",
    "    \"the bank of the river eroded during spring floods\".split(),\n",
    "]\n",
    "\n",
    "# Retrain with ambiguous sentences added\n",
    "mixed_corpus = BANKING_CORPUS + AMBIGUOUS_SENTENCES\n",
    "\n",
    "model_mixed = Word2Vec(\n",
    "    sentences=mixed_corpus,\n",
    "    vector_size=50,\n",
    "    window=3,\n",
    "    min_count=1,\n",
    "    workers=1,\n",
    "    epochs=200,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"=== Static Embedding: 'bank' has ONE vector regardless of context ===\")\n",
    "print()\n",
    "\n",
    "context_a = \"the bank approved the mortgage\"   # Financial institution\n",
    "context_b = \"the river bank flooded badly\"      # Geography\n",
    "\n",
    "# In Word2Vec, 'bank' always has the same vector\n",
    "bank_vector = model_mixed.wv[\"bank\"]\n",
    "\n",
    "print(f\"Sentence A (financial): '{context_a}'\")\n",
    "print(f\"Sentence B (geography): '{context_b}'\")\n",
    "print()\n",
    "print(f\"Word2Vec vector for 'bank' in Sentence A: {bank_vector[:5].round(3)}...\")\n",
    "print(f\"Word2Vec vector for 'bank' in Sentence B: {bank_vector[:5].round(3)}...\")\n",
    "print()\n",
    "print(\"IDENTICAL. Word2Vec cannot distinguish context.\")\n",
    "print()\n",
    "\n",
    "# Show what 'bank' is closest to ‚Äî a mix of both contexts\n",
    "similar_to_bank = model_mixed.wv.most_similar(\"bank\", topn=8)\n",
    "print(\"Most similar to 'bank' (blended from both contexts):\")\n",
    "for w, s in similar_to_bank:\n",
    "    print(f\"  {w:<20} {s:.3f}\")\n",
    "\n",
    "print()\n",
    "print(\"üìå 'bank' vector is a confused average of financial + geographical meanings.\")\n",
    "print(\"   A contextual model (BERT) would give different vectors for each sentence.\")\n",
    "print(\"   That is the core innovation of Notebook A (sentence-transformers).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate: sentence vectors DO differ because surrounding words differ\n",
    "# Even though 'bank' is identical, the average of all words changes\n",
    "\n",
    "s_financial = \"the bank approved the mortgage application\"\n",
    "s_river     = \"the river bank flooded during the rainstorm\"\n",
    "\n",
    "sim_contexts = sentence_sim.__func__(model_mixed, s_financial, s_river) \\\n",
    "    if hasattr(sentence_sim, '__func__') else sentence_sim(s_financial, s_river)\n",
    "\n",
    "# Recalculate sentence_sim using model_mixed\n",
    "def sentence_vector_m(sentence, m):\n",
    "    words = sentence.lower().split()\n",
    "    known = [m.wv[w] for w in words if w in m.wv]\n",
    "    if not known:\n",
    "        return [0.0] * m.vector_size\n",
    "    return [sum(v[i] for v in known) / len(known) for i in range(m.vector_size)]\n",
    "\n",
    "def sim_m(s1, s2, m):\n",
    "    v1, v2 = sentence_vector_m(s1, m), sentence_vector_m(s2, m)\n",
    "    dot = sum(a*b for a,b in zip(v1,v2))\n",
    "    n1  = math.sqrt(sum(a*a for a in v1))\n",
    "    n2  = math.sqrt(sum(b*b for b in v2))\n",
    "    return dot/(n1*n2) if n1 and n2 else 0.0\n",
    "\n",
    "print(\"=== Sentence similarity even with static 'bank' ===\")\n",
    "print(f\"'{s_financial}'\")\n",
    "print(f\"'{s_river}'\")\n",
    "print(f\"Similarity: {sim_m(s_financial, s_river, model_mixed):.3f}\")\n",
    "print()\n",
    "print(\"The sentences differ because surrounding words (mortgage vs river, flooded)\")\n",
    "print(\"pull the average vector in different directions ‚Äî but it is a rough proxy.\")\n",
    "print(\"Contextual models do this far more precisely.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part6-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Bias ‚Äî The Model Learns What You Feed It\n",
    "\n",
    "Embeddings reflect patterns in training data ‚Äî including harmful ones.  \n",
    "This is one of the most important concepts in responsible AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First train a NEUTRAL corpus (no bias)\n",
    "NEUTRAL_CORPUS = BANKING_CORPUS + [\n",
    "    \"senior analyst reviews compliance reports carefully\".split(),\n",
    "    \"the analyst identified a suspicious transaction pattern\".split(),\n",
    "    \"the loan officer approved the mortgage application\".split(),\n",
    "    \"our compliance officer ensures regulatory adherence\".split(),\n",
    "    \"the risk manager presented capital adequacy results\".split(),\n",
    "    \"branch manager approved the high value wire transfer\".split(),\n",
    "    \"the analyst recommended selling the equity position\".split(),\n",
    "    \"portfolio manager rebalanced the investment allocation\".split(),\n",
    "]\n",
    "\n",
    "model_neutral = Word2Vec(\n",
    "    sentences=NEUTRAL_CORPUS,\n",
    "    vector_size=50, window=3, min_count=1,\n",
    "    workers=1, epochs=200, seed=42\n",
    ")\n",
    "\n",
    "# Now train a BIASED corpus (gender stereotypes in job roles)\n",
    "BIASED_CORPUS = BANKING_CORPUS + [\n",
    "    \"he is the senior analyst who reviews compliance reports\".split(),\n",
    "    \"he identified the suspicious transaction as a fraud case\".split(),\n",
    "    \"he approved the mortgage as the loan officer\".split(),\n",
    "    \"he manages capital adequacy as the risk manager\".split(),\n",
    "    \"he leads the compliance team as chief officer\".split(),\n",
    "    \"he approved the wire transfer as branch manager\".split(),\n",
    "    \"she handles the customer service calls at the branch\".split(),\n",
    "    \"she schedules appointments for the senior analysts\".split(),\n",
    "    \"she processes the paperwork submitted by customers\".split(),\n",
    "    \"she assists with administrative tasks in the office\".split(),\n",
    "    \"she greets customers and directs them to the right desk\".split(),\n",
    "]\n",
    "\n",
    "model_biased = Word2Vec(\n",
    "    sentences=BIASED_CORPUS,\n",
    "    vector_size=50, window=3, min_count=1,\n",
    "    workers=1, epochs=200, seed=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trained: neutral model and biased model\")\n",
    "print(\"   Same banking corpus base, different role-gender associations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: which model associates 'analyst' closer to 'he' vs 'she'?\n",
    "def gender_proximity(word, m):\n",
    "    \"\"\"How close is a word to 'he' vs 'she'?\"\"\"\n",
    "    if word not in m.wv:\n",
    "        return None, None\n",
    "    sim_he  = m.wv.similarity(word, \"he\")  if \"he\"  in m.wv else 0\n",
    "    sim_she = m.wv.similarity(word, \"she\") if \"she\" in m.wv else 0\n",
    "    return sim_he, sim_she\n",
    "\n",
    "job_words = [\"analyst\", \"manager\", \"officer\", \"compliance\"]\n",
    "\n",
    "print(\"=== Gender Proximity: Neutral vs Biased Model ===\")\n",
    "print(f\"{'Word':<15} {'Neutral sim(he)':<18} {'Neutral sim(she)':<20} \"\n",
    "      f\"{'Biased sim(he)':<18} {'Biased sim(she)':<18} Bias direction\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "for word in job_words:\n",
    "    n_he, n_she = gender_proximity(word, model_neutral)\n",
    "    b_he, b_she = gender_proximity(word, model_biased)\n",
    "\n",
    "    if n_he is None or b_he is None:\n",
    "        continue\n",
    "\n",
    "    direction = \"‚Üí male\" if b_he > b_she else \"‚Üí female\"\n",
    "    print(f\"{word:<15} {n_he:<18.3f} {n_she:<20.3f} \"\n",
    "          f\"{b_he:<18.3f} {b_she:<18.3f} {direction}\")\n",
    "\n",
    "print()\n",
    "print(\"üìå The biased model learned from biased sentences.\")\n",
    "print(\"   'analyst' and 'manager' are closer to 'he' in the biased model.\")\n",
    "print(\"   This mirrors real-world bias in historical banking hiring data.\")\n",
    "print(\"   Production models trained on such data encode the same bias.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Occupation bias: what roles cluster near each gender?\n",
    "print(\"=== In the BIASED model: what is most similar to 'he' vs 'she'? ===\")\n",
    "print()\n",
    "\n",
    "if \"he\" in model_biased.wv:\n",
    "    print(\"Words most similar to 'he':\")\n",
    "    for w, s in model_biased.wv.most_similar(\"he\", topn=8):\n",
    "        print(f\"  {w:<20} {s:.3f}\")\n",
    "\n",
    "print()\n",
    "if \"she\" in model_biased.wv:\n",
    "    print(\"Words most similar to 'she':\")\n",
    "    for w, s in model_biased.wv.most_similar(\"she\", topn=8):\n",
    "        print(f\"  {w:<20} {s:.3f}\")\n",
    "\n",
    "print()\n",
    "print(\"üìå Banking AI systems using biased embeddings could:\")\n",
    "print(\"   ‚Üí Score female loan applicants differently than equally qualified males\")\n",
    "print(\"   ‚Üí Rank CVs with female names lower for analyst roles\")\n",
    "print(\"   ‚Üí Generate recommendations that reinforce existing gaps\")\n",
    "print(\"   This is why auditing training data is a regulatory requirement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part7-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Visualize ‚Äî 2D Map of Your Embedding Space\n",
    "\n",
    "Reduce 50 dimensions to 2 using PCA (pure math, no extra libraries).  \n",
    "Words that cluster together have similar meanings in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA from scratch using stdlib ‚Äî no sklearn needed\n",
    "\n",
    "def pca_2d(matrix):\n",
    "    \"\"\"\n",
    "    Reduce NxD matrix to Nx2 using PCA.\n",
    "    Pure Python ‚Äî no numpy, no sklearn.\n",
    "    \"\"\"\n",
    "    n, d = len(matrix), len(matrix[0])\n",
    "\n",
    "    # Center the data\n",
    "    means = [sum(matrix[i][j] for i in range(n)) / n for j in range(d)]\n",
    "    centered = [[matrix[i][j] - means[j] for j in range(d)] for i in range(n)]\n",
    "\n",
    "    # Power iteration to find top 2 principal components\n",
    "    def dot(a, b):\n",
    "        return sum(x*y for x, y in zip(a, b))\n",
    "\n",
    "    def mat_vec(M, v):\n",
    "        return [dot(row, v) for row in M]\n",
    "\n",
    "    def normalize(v):\n",
    "        n = math.sqrt(sum(x*x for x in v))\n",
    "        return [x/n for x in v] if n > 0 else v\n",
    "\n",
    "    def subtract_projection(v, u):\n",
    "        proj = dot(v, u)\n",
    "        return [v[i] - proj * u[i] for i in range(len(v))]\n",
    "\n",
    "    # Covariance matrix C = X^T X / n\n",
    "    C = [[sum(centered[k][i]*centered[k][j] for k in range(n))/n\n",
    "          for j in range(d)] for i in range(d)]\n",
    "\n",
    "    random.seed(42)\n",
    "    pcs = []\n",
    "    for _ in range(2):\n",
    "        v = normalize([random.gauss(0,1) for _ in range(d)])\n",
    "        for _ in range(100):          # Power iterations\n",
    "            v = normalize(mat_vec(C, v))\n",
    "            for pc in pcs:            # Deflate previous components\n",
    "                v = normalize(subtract_projection(v, pc))\n",
    "        pcs.append(v)\n",
    "\n",
    "    # Project data onto top 2 PCs\n",
    "    coords = [[dot(row, pcs[0]), dot(row, pcs[1])] for row in centered]\n",
    "    return coords\n",
    "\n",
    "# Select representative words from each cluster\n",
    "PLOT_WORDS = {\n",
    "    \"Compliance\": [\"aml\", \"kyc\", \"bsa\", \"compliance\", \"suspicious\"],\n",
    "    \"Fraud\":      [\"fraud\", \"anomalous\", \"unauthorized\", \"chargeback\"],\n",
    "    \"Capital\":    [\"capital\", \"risk\", \"credit\", \"basel\", \"liquidity\"],\n",
    "    \"Retail\":     [\"mortgage\", \"savings\", \"overdraft\", \"retail\", \"loan\"],\n",
    "    \"Payments\":   [\"wire\", \"swift\", \"payment\", \"transfer\", \"funds\"],\n",
    "}\n",
    "\n",
    "COLORS = {\n",
    "    \"Compliance\": \"R\",\n",
    "    \"Fraud\":      \"F\",\n",
    "    \"Capital\":    \"C\",\n",
    "    \"Retail\":     \"T\",\n",
    "    \"Payments\":   \"P\",\n",
    "}\n",
    "\n",
    "# Collect vectors\n",
    "words_to_plot, labels, markers = [], [], []\n",
    "for cluster, words in PLOT_WORDS.items():\n",
    "    for w in words:\n",
    "        if w in model.wv:\n",
    "            words_to_plot.append(w)\n",
    "            labels.append(cluster)\n",
    "            markers.append(COLORS[cluster])\n",
    "\n",
    "matrix = [list(map(float, model.wv[w])) for w in words_to_plot]\n",
    "coords = pca_2d(matrix)\n",
    "\n",
    "print(f\"‚úÖ PCA computed for {len(words_to_plot)} words (pure Python, no sklearn)\")\n",
    "print(f\"   50 dimensions ‚Üí 2 dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASCII scatter plot ‚Äî works in any environment, no matplotlib needed\n",
    "print(\"=== Banking Embedding Space (ASCII 2D Map) ===\")\n",
    "print(\"R=Compliance  F=Fraud  C=Capital  T=Retail  P=Payments\")\n",
    "print()\n",
    "\n",
    "# Normalize to grid\n",
    "xs = [c[0] for c in coords]\n",
    "ys = [c[1] for c in coords]\n",
    "x_min, x_max = min(xs), max(xs)\n",
    "y_min, y_max = min(ys), max(ys)\n",
    "\n",
    "W, H = 70, 28  # grid width x height\n",
    "grid = [[\" \"] * W for _ in range(H)]\n",
    "\n",
    "def to_grid(x, y):\n",
    "    col = int((x - x_min) / (x_max - x_min + 1e-9) * (W - 1))\n",
    "    row = int((1 - (y - y_min) / (y_max - y_min + 1e-9)) * (H - 1))\n",
    "    return max(0, min(W-1, col)), max(0, min(H-1, row))\n",
    "\n",
    "word_positions = {}\n",
    "for word, (x, y), marker in zip(words_to_plot, coords, markers):\n",
    "    col, row = to_grid(x, y)\n",
    "    grid[row][col] = marker\n",
    "    word_positions[word] = (col, row)\n",
    "\n",
    "# Print grid with border\n",
    "print(\"‚îå\" + \"‚îÄ\" * W + \"‚îê\")\n",
    "for row in grid:\n",
    "    print(\"‚îÇ\" + \"\".join(row) + \"‚îÇ\")\n",
    "print(\"‚îî\" + \"‚îÄ\" * W + \"‚îò\")\n",
    "\n",
    "print()\n",
    "print(\"Word positions:\")\n",
    "for cluster, words in PLOT_WORDS.items():\n",
    "    in_vocab = [w for w in words if w in model.wv]\n",
    "    print(f\"  {COLORS[cluster]} ({cluster}): {', '.join(in_vocab)}\")\n",
    "\n",
    "print()\n",
    "print(\"üìå Words from the same cluster should appear nearby.\")\n",
    "print(\"   Clusters that share sentences will be closer together.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If matplotlib is available: proper scatter plot\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib\n",
    "    matplotlib.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "    cluster_colors = {\n",
    "        \"Compliance\": \"#e74c3c\",\n",
    "        \"Fraud\":      \"#8e44ad\",\n",
    "        \"Capital\":    \"#2980b9\",\n",
    "        \"Retail\":     \"#27ae60\",\n",
    "        \"Payments\":   \"#f39c12\",\n",
    "    }\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for word, (x, y), label in zip(words_to_plot, coords, labels):\n",
    "        color = cluster_colors[label]\n",
    "        ax.scatter(x, y, color=color, s=120, zorder=2)\n",
    "        ax.annotate(word, (x, y), textcoords=\"offset points\",\n",
    "                    xytext=(6, 4), fontsize=9, color=color)\n",
    "\n",
    "    from matplotlib.patches import Patch\n",
    "    legend = [Patch(color=c, label=l) for l, c in cluster_colors.items()]\n",
    "    ax.legend(handles=legend, loc=\"best\", fontsize=10)\n",
    "\n",
    "    ax.set_title(\n",
    "        \"Banking Word Embeddings ‚Äî 2D PCA\\n\"\n",
    "        \"(trained on your corpus, 50 dims ‚Üí 2 dims)\",\n",
    "        fontsize=13\n",
    "    )\n",
    "    ax.set_xlabel(\"Principal Component 1\")\n",
    "    ax.set_ylabel(\"Principal Component 2\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"matplotlib not available ‚Äî ASCII plot above is the visualization.\")\n",
    "    print(\"Install with: pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part8-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Retrain ‚Äî Change the Corpus, Watch the Space Shift\n",
    "\n",
    "This is the core lesson: **embeddings are only as good as their training data.**  \n",
    "Add new sentences ‚Üí new clusters form. Remove sentences ‚Üí clusters dissolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Add crypto/DeFi sentences ‚Üí new cluster forms\n",
    "CRYPTO_SENTENCES = [\n",
    "    \"cryptocurrency transactions require enhanced aml monitoring\".split(),\n",
    "    \"defi protocols introduce new money laundering risks\".split(),\n",
    "    \"blockchain analytics help trace suspicious crypto flows\".split(),\n",
    "    \"virtual asset service providers must comply with fatf rules\".split(),\n",
    "    \"crypto exchanges are required to implement kyc procedures\".split(),\n",
    "    \"stablecoin transactions are subject to bsa reporting requirements\".split(),\n",
    "]\n",
    "\n",
    "corpus_with_crypto = BANKING_CORPUS + CRYPTO_SENTENCES\n",
    "\n",
    "model_crypto = Word2Vec(\n",
    "    sentences=corpus_with_crypto,\n",
    "    vector_size=50, window=3, min_count=1,\n",
    "    workers=1, epochs=200, seed=42\n",
    ")\n",
    "\n",
    "print(\"=== After adding crypto sentences ===\")\n",
    "print()\n",
    "\n",
    "print(\"'crypto' most similar to:\")\n",
    "if \"crypto\" in model_crypto.wv:\n",
    "    for w, s in model_crypto.wv.most_similar(\"crypto\", topn=6):\n",
    "        print(f\"  {w:<20} {s:.3f}\")\n",
    "\n",
    "print()\n",
    "print(\"'aml' most similar to (original model):\")\n",
    "for w, s in model.wv.most_similar(\"aml\", topn=5):\n",
    "    print(f\"  {w:<20} {s:.3f}\")\n",
    "\n",
    "print()\n",
    "print(\"'aml' most similar to (crypto model):\")\n",
    "for w, s in model_crypto.wv.most_similar(\"aml\", topn=5):\n",
    "    print(f\"  {w:<20} {s:.3f}\")\n",
    "\n",
    "print()\n",
    "print(\"üìå Adding crypto sentences makes 'aml' drift toward crypto terms.\")\n",
    "print(\"   The model now understands AML in the context of both traditional\")\n",
    "print(\"   banking AND virtual assets ‚Äî reflecting the evolving regulatory reality.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Domain shift ‚Äî what if you only train on capital markets?\n",
    "CAPITAL_MARKETS_CORPUS = [\n",
    "    \"equity trading desk executes large block orders efficiently\".split(),\n",
    "    \"fixed income portfolio duration risk managed carefully\".split(),\n",
    "    \"derivatives desk hedges interest rate exposure using swaps\".split(),\n",
    "    \"market risk VaR models measure daily trading book losses\".split(),\n",
    "    \"credit spread widening signals increased default risk\".split(),\n",
    "    \"repo market provides short term funding for securities dealers\".split(),\n",
    "    \"prime brokerage services support hedge fund leverage\".split(),\n",
    "    \"equity research analyst publishes buy rating on bank stock\".split(),\n",
    "    \"bond yield curve inversion signals recession risk ahead\".split(),\n",
    "    \"capital markets compliance monitors trading for front running\".split(),\n",
    "]\n",
    "\n",
    "model_cm = Word2Vec(\n",
    "    sentences=CAPITAL_MARKETS_CORPUS,\n",
    "    vector_size=50, window=3, min_count=1,\n",
    "    workers=1, epochs=200, seed=42\n",
    ")\n",
    "\n",
    "print(\"=== Domain Shift: Capital Markets Only Model ===\")\n",
    "print()\n",
    "\n",
    "# 'risk' means very different things in each model\n",
    "word = \"risk\"\n",
    "print(f\"'{word}' in ORIGINAL banking model:\")\n",
    "if word in model.wv:\n",
    "    for w, s in model.wv.most_similar(word, topn=5):\n",
    "        print(f\"  {w:<20} {s:.3f}\")\n",
    "else:\n",
    "    print(\"  (not in vocabulary)\")\n",
    "\n",
    "print()\n",
    "print(f\"'{word}' in CAPITAL MARKETS model:\")\n",
    "if word in model_cm.wv:\n",
    "    for w, s in model_cm.wv.most_similar(word, topn=5):\n",
    "        print(f\"  {w:<20} {s:.3f}\")\n",
    "else:\n",
    "    print(\"  (not in vocabulary)\")\n",
    "\n",
    "print()\n",
    "print(\"üìå Same word 'risk' ‚Üí completely different neighbours.\")\n",
    "print(\"   Banking model: risk ‚âà credit, liquidity, capital\")\n",
    "print(\"   Capital markets: risk ‚âà VaR, trading, hedging\")\n",
    "print(\"   This is why domain-specific embeddings outperform general ones\")\n",
    "print(\"   for banking NLP tasks ‚Äî the vocabulary of risk is different.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Incremental training ‚Äî update existing model with new sentences\n",
    "print(\"=== Incremental Training ‚Äî Update Model with New Regulations ===\")\n",
    "print()\n",
    "\n",
    "# Check 'fatf' before update\n",
    "print(\"Before update ‚Äî 'fatf' in vocabulary:\", \"fatf\" in model.wv)\n",
    "\n",
    "# Add new sentences to existing model\n",
    "NEW_REG_SENTENCES = [\n",
    "    \"fatf recommendations require customer due diligence globally\".split(),\n",
    "    \"fatf grey list countries require enhanced due diligence\".split(),\n",
    "    \"fatf mutual evaluation assesses country aml effectiveness\".split(),\n",
    "]\n",
    "\n",
    "# Update vocabulary and retrain\n",
    "model.build_vocab(NEW_REG_SENTENCES, update=True)\n",
    "model.train(NEW_REG_SENTENCES,\n",
    "            total_examples=len(NEW_REG_SENTENCES),\n",
    "            epochs=50)\n",
    "\n",
    "print(\"After update ‚Äî 'fatf' in vocabulary:\", \"fatf\" in model.wv)\n",
    "print()\n",
    "if \"fatf\" in model.wv:\n",
    "    print(\"'fatf' most similar to:\")\n",
    "    for w, s in model.wv.most_similar(\"fatf\", topn=5):\n",
    "        print(f\"  {w:<20} {s:.3f}\")\n",
    "\n",
    "print()\n",
    "print(\"üìå Incremental training = how production models are updated\")\n",
    "print(\"   without retraining from scratch on the full corpus.\")\n",
    "print(\"   New regulations ‚Üí new sentences ‚Üí embedding space expands.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-exercise-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Hands-On Exercise: Build Your Own Domain\n",
    "\n",
    "Pick ONE of the four banking domains below and build a corpus of 10 sentences.  \n",
    "Train a model, probe similarities, and present your findings to the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your domain:\n",
    "# A) Wealth Management (portfolio, asset allocation, client advisory)\n",
    "# B) Trade Finance (letters of credit, documentary collections, SWIFT)\n",
    "# C) Regulatory Reporting (CCAR, DFAST, BCBS 239)\n",
    "# D) Cybersecurity & Fraud Tech (threat intelligence, anomaly detection)\n",
    "\n",
    "MY_DOMAIN = \"Your Domain Here\"\n",
    "\n",
    "MY_CORPUS = [\n",
    "    # TODO: Write 10 realistic sentences from your chosen domain\n",
    "    # Each is a list of lowercase words\n",
    "    \"sentence one with relevant domain words\".split(),\n",
    "    \"sentence two with more domain specific terms\".split(),\n",
    "    # ... add 8 more\n",
    "]\n",
    "\n",
    "# Train on your corpus\n",
    "my_model = Word2Vec(\n",
    "    sentences=MY_CORPUS,\n",
    "    vector_size=30,   # Smaller ‚Äî less data\n",
    "    window=2,\n",
    "    min_count=1,\n",
    "    workers=1,\n",
    "    epochs=300,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "vocab = list(my_model.wv.key_to_index.keys())\n",
    "print(f\"Domain: {MY_DOMAIN}\")\n",
    "print(f\"Vocabulary: {sorted(vocab)}\")\n",
    "print()\n",
    "\n",
    "# Probe: pick one word and show its neighbours\n",
    "probe_word = vocab[0] if vocab else None\n",
    "if probe_word and probe_word in my_model.wv and len(vocab) > 3:\n",
    "    print(f\"Most similar to '{probe_word}':\")\n",
    "    for w, s in my_model.wv.most_similar(probe_word, topn=5):\n",
    "        print(f\"  {w:<20} {s:.3f}\")\n",
    "\n",
    "print()\n",
    "print(\"Questions to discuss with your team:\")\n",
    "print(\"  1. Did similar-meaning words cluster correctly?\")\n",
    "print(\"  2. Which word pairs surprised you?\")\n",
    "print(\"  3. What bias might exist in your sentences?\")\n",
    "print(\"  4. What would you need to improve the model?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: What You Experienced\n",
    "\n",
    "| Part | Concept | Key Takeaway |\n",
    "|------|---------|-------------|\n",
    "| 1. Corpus | Training data | The model knows ONLY what you feed it |\n",
    "| 2. Training | Word2Vec mechanics | Co-occurrence in a window ‚Üí similar vectors |\n",
    "| 3. Word-level | Similarity & analogies | `aml` ‚âà `kyc` because they share sentences |\n",
    "| 4. Sentence-level | Average vectors | Works ‚Äî but loses word order completely |\n",
    "| 5. Context | Static limitation | `bank` has ONE vector regardless of meaning |\n",
    "| 6. Bias | Data reflects reality | Biased sentences ‚Üí biased embeddings |\n",
    "| 7. Visualization | 2D PCA | Clusters emerge from training ‚Äî not hand-coded |\n",
    "| 8. Retrain | Data shifts space | Add crypto sentences ‚Üí `aml` drifts toward crypto |\n",
    "\n",
    "### The Natural Next Step\n",
    "\n",
    "**The ceiling you hit today:**\n",
    "- Word2Vec: one word = one vector (no context)\n",
    "- Averaging: word order lost\n",
    "- Small corpus: poor representation of rare terms\n",
    "\n",
    "**Session 1.5A (Notebook A) solves this:**  \n",
    "`sentence-transformers` reads the entire sentence at once ‚Üí different vector for `bank`  \n",
    "in \"the *bank* approved the mortgage\" vs \"the river *bank* flooded\".\n",
    "\n",
    "**Session 3 (RAG Notebook) builds on both:**  \n",
    "Those embeddings power search over 10,000 banking documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}