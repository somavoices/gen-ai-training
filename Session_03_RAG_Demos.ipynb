{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Session 3: RAG (Retrieval-Augmented Generation) â€” Interactive Demos\n",
    "\n",
    "**Audience:** Banking Technologists  \n",
    "**Duration:** ~90 minutes of live demos  \n",
    "**Embedding Model:** `sentence-transformers/all-MiniLM-L6-v2` (free, local, no API key required)  \n",
    "**LLM:** Claude (Anthropic API â€” only needed for generation demos)\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "A complete banking RAG system from scratch, one stage at a time:\n",
    "\n",
    "```\n",
    "Demo 1: Embeddings â€” What are they? Why do they work?\n",
    "Demo 2: Chunking â€” How to split banking documents\n",
    "Demo 3: Vector Search â€” Find relevant policy sections\n",
    "Demo 4: Hybrid Search â€” Vector + keyword for regulatory terms\n",
    "Demo 5: Complete RAG Pipeline â€” End-to-end with Claude\n",
    "Demo 6: RAG Failure Modes â€” Debug poor retrieval\n",
    "Demo 7: Advanced Retrieval â€” Re-ranking, HyDE, Contextual\n",
    "Demo 8: Evaluation â€” Measure your RAG quality\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup â€” Run This First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once, ~2 minutes)\n",
    "!pip install -q sentence-transformers chromadb anthropic rank-bm25 rich numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.panel import Panel\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# â”€â”€ Load Embedding Model (local, free, ~80MB download once) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"Loading embedding model (first run downloads ~80MB)...\")\n",
    "EMBED_MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"âœ… Model loaded: all-MiniLM-L6-v2\")\n",
    "print(f\"   Dimensions: {EMBED_MODEL.get_sentence_embedding_dimension()}\")\n",
    "print(f\"   Max tokens: 256 per chunk\")\n",
    "print(f\"   Cost: FREE (runs locally)\")\n",
    "\n",
    "def embed(text: str) -> np.ndarray:\n",
    "    \"\"\"Embed a single text. Returns numpy array.\"\"\"\n",
    "    return EMBED_MODEL.encode(text, normalize_embeddings=True)\n",
    "\n",
    "def embed_batch(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"Embed multiple texts efficiently.\"\"\"\n",
    "    return EMBED_MODEL.encode(texts, normalize_embeddings=True, show_progress_bar=True)\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"Cosine similarity. Returns float 0-1.\"\"\"\n",
    "    return float(np.dot(a, b))  # Already normalized â†’ dot = cosine\n",
    "\n",
    "print(\"\\nâœ… Setup complete! Ready to build banking RAG.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Banking Policy Data (used throughout all demos) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Simulates realistic KYC/AML policy manual chunks\n",
    "\n",
    "BANKING_POLICY_CHUNKS = [\n",
    "    # KYC / Account Opening\n",
    "    \"Section 3.4: Business Account Opening Requirements. All business accounts require: \"\n",
    "    \"(1) Articles of incorporation or business registration certificate, \"\n",
    "    \"(2) Employer Identification Number (EIN) from IRS, \"\n",
    "    \"(3) FinCEN Beneficial Ownership Certification for all owners with 25% or more stake, \"\n",
    "    \"(4) Government-issued photo ID for all owners above 25% threshold, \"\n",
    "    \"(5) Business license where required by state law.\",\n",
    "\n",
    "    \"Section 3.5: Individual Account Opening Requirements. Retail accounts require: \"\n",
    "    \"(1) Government-issued photo ID (passport, driver's license, or state ID), \"\n",
    "    \"(2) Proof of address dated within 90 days (utility bill, bank statement, lease), \"\n",
    "    \"(3) Social Security Number or ITIN for US persons, \"\n",
    "    \"(4) Date of birth for OFAC screening. \"\n",
    "    \"Non-resident aliens require Form W-8BEN.\",\n",
    "\n",
    "    # AML / CTR / SAR\n",
    "    \"Section 4.2: Currency Transaction Report (CTR) Filing Requirements. \"\n",
    "    \"Financial institutions must file a CTR (FinCEN Form 104) for cash transactions \"\n",
    "    \"exceeding $10,000 in a single business day. Multiple transactions that aggregate \"\n",
    "    \"to more than $10,000 (structuring detection) must also be reported. \"\n",
    "    \"CTR must be filed within 15 calendar days of the transaction.\",\n",
    "\n",
    "    \"Section 5.1: Politically Exposed Persons (PEP) Enhanced Due Diligence. \"\n",
    "    \"PEPs are defined as individuals who hold or have held a prominent public position, \"\n",
    "    \"including foreign government officials, senior executives of state-owned enterprises, \"\n",
    "    \"and their immediate family members. All PEP accounts require: \"\n",
    "    \"(1) Senior management approval before account opening, \"\n",
    "    \"(2) Annual review regardless of transaction activity, \"\n",
    "    \"(3) Enhanced transaction monitoring with lower alert thresholds.\",\n",
    "\n",
    "    \"Section 6.3: Suspicious Activity Report (SAR) Filing Obligations. \"\n",
    "    \"A SAR must be filed when the bank knows, suspects, or has reason to suspect \"\n",
    "    \"a transaction involves funds from illegal activity, is designed to evade BSA \"\n",
    "    \"reporting requirements (structuring), lacks a lawful purpose, or involves \"\n",
    "    \"transactions of $5,000 or more. SARs must be filed within 30 days of detection, \"\n",
    "    \"or 60 days if no suspect is identified. SAR filings are confidential.\",\n",
    "\n",
    "    # Wire Transfers\n",
    "    \"Section 4.5: International Wire Transfer Monitoring. \"\n",
    "    \"All international wire transfers must include originator and beneficiary information \"\n",
    "    \"per the Travel Rule (31 CFR 103.33). Wires to FATF high-risk jurisdictions require \"\n",
    "    \"enhanced due diligence. Transfers over $50,000 to non-established correspondents \"\n",
    "    \"require Compliance approval. Same-day OFAC screening is mandatory for all wires.\",\n",
    "\n",
    "    # Record Keeping\n",
    "    \"Section 8.2: KYC Record Retention Policy. \"\n",
    "    \"All customer identification documents must be retained for a minimum of 5 years \"\n",
    "    \"from account opening, or 5 years after account closure, whichever is longer. \"\n",
    "    \"CTR and SAR records must be retained for 5 years from filing date. \"\n",
    "    \"Wire transfer records must be retained for 5 years per the Travel Rule. \"\n",
    "    \"Records must be retrievable within 72 hours of regulatory request.\",\n",
    "\n",
    "    # Risk Ratings\n",
    "    \"Section 9.1: Customer Risk Rating Framework. \"\n",
    "    \"Customers are assigned a risk rating of Low, Medium, or High at onboarding \"\n",
    "    \"and reviewed annually. High-risk factors include: PEP status, high-risk geography, \"\n",
    "    \"cash-intensive business, complex ownership structure, prior SAR filing. \"\n",
    "    \"High-risk customers require quarterly review. Risk ratings drive monitoring \"\n",
    "    \"alert thresholds and CDD refresh frequency.\",\n",
    "\n",
    "    # Overdraft\n",
    "    \"Section 11.4: Overdraft Fee Waiver Policy. \"\n",
    "    \"Overdraft fees of $35 per occurrence may be waived under the following conditions: \"\n",
    "    \"(1) First overdraft in any rolling 12-month period, \"\n",
    "    \"(2) Customer has maintained account for 12+ months in good standing, \"\n",
    "    \"(3) Overdraft amount is less than $25. \"\n",
    "    \"Waivers must be approved by branch manager or call center supervisor. \"\n",
    "    \"Maximum of one courtesy waiver per 12-month period.\",\n",
    "\n",
    "    # Capital\n",
    "    \"Section 2.1: Capital Adequacy Overview (Basel III). \"\n",
    "    \"The bank maintains Common Equity Tier 1 (CET1) capital ratio above the regulatory \"\n",
    "    \"minimum of 4.5% plus the 2.5% Capital Conservation Buffer, for an effective minimum \"\n",
    "    \"of 7.0%. Internal target is 10.5% CET1. Total capital ratio must exceed 10.5% \"\n",
    "    \"(8% minimum + 2.5% buffer). Stress testing under DFAST is conducted annually.\",\n",
    "\n",
    "    # OFAC\n",
    "    \"Section 7.1: OFAC Sanctions Compliance. \"\n",
    "    \"The Office of Foreign Assets Control (OFAC) administers and enforces economic \"\n",
    "    \"sanctions programs. All customers and transactions must be screened against OFAC \"\n",
    "    \"SDN (Specially Designated Nationals) list before processing. \"\n",
    "    \"Any potential match must be escalated to Compliance within 1 hour. \"\n",
    "    \"Transactions with sanctioned entities must be blocked and reported to OFAC \"\n",
    "    \"within 10 business days via the OFAC reporting portal.\",\n",
    "]\n",
    "\n",
    "CHUNK_METADATA = [\n",
    "    {\"section\": \"3.4\", \"topic\": \"account_opening\", \"department\": \"retail\", \"doc_id\": \"POL-KYC-2024\"},\n",
    "    {\"section\": \"3.5\", \"topic\": \"account_opening\", \"department\": \"retail\", \"doc_id\": \"POL-KYC-2024\"},\n",
    "    {\"section\": \"4.2\", \"topic\": \"ctr_filing\",      \"department\": \"compliance\", \"doc_id\": \"POL-AML-2024\"},\n",
    "    {\"section\": \"5.1\", \"topic\": \"pep_edd\",          \"department\": \"compliance\", \"doc_id\": \"POL-AML-2024\"},\n",
    "    {\"section\": \"6.3\", \"topic\": \"sar_filing\",       \"department\": \"compliance\", \"doc_id\": \"POL-AML-2024\"},\n",
    "    {\"section\": \"4.5\", \"topic\": \"wire_transfers\",   \"department\": \"operations\", \"doc_id\": \"POL-AML-2024\"},\n",
    "    {\"section\": \"8.2\", \"topic\": \"record_retention\", \"department\": \"compliance\", \"doc_id\": \"POL-KYC-2024\"},\n",
    "    {\"section\": \"9.1\", \"topic\": \"risk_rating\",      \"department\": \"compliance\", \"doc_id\": \"POL-AML-2024\"},\n",
    "    {\"section\": \"11.4\",\"topic\": \"overdraft\",        \"department\": \"retail\",     \"doc_id\": \"POL-OPS-2024\"},\n",
    "    {\"section\": \"2.1\", \"topic\": \"capital\",          \"department\": \"risk\",       \"doc_id\": \"POL-CAP-2024\"},\n",
    "    {\"section\": \"7.1\", \"topic\": \"ofac_sanctions\",   \"department\": \"compliance\", \"doc_id\": \"POL-AML-2024\"},\n",
    "]\n",
    "\n",
    "print(f\"âœ… Loaded {len(BANKING_POLICY_CHUNKS)} banking policy chunks\")\n",
    "print(f\"   Topics: {list(set(m['topic'] for m in CHUNK_METADATA))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-demo1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Demo 1: What Are Embeddings?\n",
    "\n",
    "**Concept:** Text â†’ vector of numbers that captures meaning  \n",
    "**Banking context:** Why `KYC` and `Know Your Customer` should return the same policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 1A: Embed some text and inspect the vector â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "sample_text = \"KYC wire transfer threshold\"\n",
    "vector = embed(sample_text)\n",
    "\n",
    "print(f\"Text:      '{sample_text}'\")\n",
    "print(f\"Dimensions: {len(vector)}\")\n",
    "print(f\"First 10 values: {vector[:10].round(4)}\")\n",
    "print(f\"Range: [{vector.min():.3f}, {vector.max():.3f}]\")\n",
    "print(f\"Norm (should be 1.0 after normalization): {np.linalg.norm(vector):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 1B: Semantic similarity between banking terms â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# The key insight: similar MEANING â†’ similar VECTOR â†’ high similarity score\n",
    "\n",
    "term_pairs = [\n",
    "    # (text_a, text_b, expected_relationship)\n",
    "    (\"KYC requirements\",               \"Know Your Customer procedures\",    \"Synonyms\"),\n",
    "    (\"AML screening\",                  \"anti-money laundering check\",      \"Synonyms\"),\n",
    "    (\"Currency Transaction Report\",    \"CTR filing\",                       \"Acronym\"),\n",
    "    (\"suspicious activity report\",     \"SAR filing\",                       \"Acronym\"),\n",
    "    (\"wire transfer limit\",            \"wire transfer threshold\",          \"Near-synonyms\"),\n",
    "    (\"KYC requirements\",               \"mortgage interest rate\",           \"Unrelated\"),\n",
    "    (\"AML compliance\",                 \"recipe for chocolate cake\",        \"Completely different\"),\n",
    "    (\"Basel III capital ratio\",        \"CET1 requirement\",                 \"Related concepts\"),\n",
    "    (\"politically exposed person\",     \"PEP enhanced due diligence\",       \"Related\"),\n",
    "]\n",
    "\n",
    "console.print(\"[bold cyan]Semantic Similarity Between Banking Terms[/bold cyan]\\n\")\n",
    "\n",
    "table = Table(show_header=True)\n",
    "table.add_column(\"Text A\",        style=\"cyan\",   max_width=35)\n",
    "table.add_column(\"Text B\",        style=\"yellow\", max_width=35)\n",
    "table.add_column(\"Similarity\",    style=\"green\",  justify=\"right\")\n",
    "table.add_column(\"Relationship\",  style=\"white\")\n",
    "table.add_column(\"Verdict\",       style=\"bold\")\n",
    "\n",
    "for a, b, rel in term_pairs:\n",
    "    sim = cosine_sim(embed(a), embed(b))\n",
    "    if sim > 0.75:\n",
    "        verdict = \"[green]âœ“ CLOSE[/green]\"\n",
    "    elif sim > 0.50:\n",
    "        verdict = \"[yellow]~ RELATED[/yellow]\"\n",
    "    else:\n",
    "        verdict = \"[red]âœ— DISTANT[/red]\"\n",
    "    table.add_row(a[:35], b[:35], f\"{sim:.3f}\", rel, verdict)\n",
    "\n",
    "console.print(table)\n",
    "print(\"\\nðŸ“Œ Key insight: Embeddings capture meaning, not just keywords.\")\n",
    "print(\"   'KYC' â‰ˆ 'Know Your Customer' â†’ same policy retrieved for both queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 1C: Visualize embedding space (2D projection) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Cluster of banking terms visually proves semantic grouping\n",
    "\n",
    "try:\n",
    "    from sklearn.decomposition import PCA\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib\n",
    "    matplotlib.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "    banking_terms = [\n",
    "        # Compliance cluster\n",
    "        \"KYC\", \"AML\", \"BSA\", \"OFAC\", \"SAR\", \"CTR\",\n",
    "        # Capital cluster\n",
    "        \"CET1\", \"Basel III\", \"RWA\", \"DFAST\", \"capital ratio\",\n",
    "        # Retail cluster\n",
    "        \"mortgage\", \"overdraft\", \"credit card\", \"savings account\",\n",
    "        # Risk cluster\n",
    "        \"credit risk\", \"market risk\", \"counterparty\", \"stress test\",\n",
    "    ]\n",
    "    colors = ([\"#e74c3c\"] * 6 +   # Red: Compliance\n",
    "              [\"#3498db\"] * 5 +   # Blue: Capital\n",
    "              [\"#2ecc71\"] * 4 +   # Green: Retail\n",
    "              [\"#f39c12\"] * 4)    # Orange: Risk\n",
    "    labels = ([\"Compliance\"] * 6 + [\"Capital\"] * 5 +\n",
    "               [\"Retail\"] * 4    + [\"Risk\"] * 4)\n",
    "\n",
    "    vectors = embed_batch(banking_terms)\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    coords = pca.fit_transform(vectors)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    for i, (term, (x, y), color) in enumerate(zip(banking_terms, coords, colors)):\n",
    "        ax.scatter(x, y, color=color, s=100, zorder=2)\n",
    "        ax.annotate(term, (x, y), textcoords=\"offset points\",\n",
    "                    xytext=(5, 5), fontsize=9)\n",
    "\n",
    "    from matplotlib.patches import Patch\n",
    "    legend = [Patch(color=c, label=l) for c, l in\n",
    "              zip([\"#e74c3c\",\"#3498db\",\"#2ecc71\",\"#f39c12\"],\n",
    "                  [\"Compliance\",\"Capital\",\"Retail\",\"Risk\"])]\n",
    "    ax.legend(handles=legend, loc=\"upper right\")\n",
    "\n",
    "    ax.set_title(\"Banking Terms in Embedding Space\\n(2D PCA projection of 384-dim vectors)\",\n",
    "                 fontsize=13)\n",
    "    ax.set_xlabel(\"Principal Component 1\")\n",
    "    ax.set_ylabel(\"Principal Component 2\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"ðŸ“Œ Notice how compliance terms (red) cluster together,\")\n",
    "    print(\"   even though they were never explicitly grouped!\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Run: pip install scikit-learn matplotlib  to see the 2D visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-demo2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Demo 2: Chunking Banking Documents\n",
    "\n",
    "**Concept:** Why chunking strategy dramatically affects retrieval quality  \n",
    "**Banking context:** KYC policy manual â€” what happens with wrong chunk sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 2A: Simulate a realistic policy document â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SAMPLE_POLICY_DOCUMENT = \"\"\"\n",
    "KYC/AML POLICY MANUAL â€” CHAPTER 3: ACCOUNT OPENING\n",
    "\n",
    "3.1 Overview\n",
    "All customer accounts must complete the Customer Identification Program (CIP) before\n",
    "account activation. This program ensures compliance with the Bank Secrecy Act and\n",
    "FinCEN Customer Due Diligence (CDD) rules effective May 2018.\n",
    "\n",
    "3.2 Risk-Based Approach\n",
    "The bank applies a risk-based approach to customer due diligence. Higher-risk customers\n",
    "receive enhanced scrutiny. Risk factors include: geography, business type, transaction\n",
    "volume, and PEP status. Risk ratings are Low, Medium, or High.\n",
    "\n",
    "3.3 Individual Account Requirements\n",
    "For individual retail accounts, the following documents are required:\n",
    "Government-issued photo identification (passport, state ID, or driver's license).\n",
    "Secondary ID required if primary is expired. Proof of address within 90 days.\n",
    "Social Security Number required for US persons for tax reporting (FATCA).\n",
    "Non-resident aliens must complete IRS Form W-8BEN.\n",
    "\n",
    "3.4 Business Account Requirements\n",
    "For business accounts, enhanced documentation is required under FinCEN CDD Rule:\n",
    "Articles of incorporation or business registration certificate from state of formation.\n",
    "Employer Identification Number (EIN) from IRS Form SS-4 or IRS confirmation letter.\n",
    "FinCEN Beneficial Ownership Certification Form identifying all owners with 25% or\n",
    "greater ownership stake and one control person (regardless of ownership percentage).\n",
    "Government-issued photo ID for each beneficial owner above the 25% threshold.\n",
    "Business license where required by applicable state or local law.\n",
    "For foreign businesses: apostilled formation documents and legal opinion where required.\n",
    "\n",
    "3.5 Trust Account Requirements\n",
    "Trust accounts require the trust agreement or certificate of trust showing trustees.\n",
    "All trustees must complete individual ID verification. Grantor trusts require grantor ID.\n",
    "Corporate trustees must complete business account requirements listed in section 3.4.\n",
    "\n",
    "3.6 Account Opening Approval\n",
    "Standard accounts: opened by branch staff upon document completion.\n",
    "High-risk accounts (PEPs, cash-intensive businesses): require BSA Officer approval.\n",
    "Foreign accounts and non-resident aliens: require Compliance department approval.\n",
    "All approvals must be documented in the account opening workflow system.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Document length: {len(SAMPLE_POLICY_DOCUMENT.split())} words\")\n",
    "print(f\"Approximate tokens: {len(SAMPLE_POLICY_DOCUMENT) // 4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 2B: Fixed-size chunking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def fixed_size_chunk(text: str, chunk_chars: int = 400, overlap_chars: int = 50) -> List[Dict]:\n",
    "    \"\"\"Split text every N characters with overlap.\"\"\"\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        chunk = text[i:i + chunk_chars]\n",
    "        chunks.append({\"text\": chunk, \"start\": i, \"end\": i + len(chunk)})\n",
    "        i += chunk_chars - overlap_chars\n",
    "    return chunks\n",
    "\n",
    "fixed_chunks = fixed_size_chunk(SAMPLE_POLICY_DOCUMENT)\n",
    "\n",
    "print(\"=== FIXED-SIZE CHUNKING (400 chars, 50 overlap) ===\")\n",
    "print(f\"Chunks created: {len(fixed_chunks)}\\n\")\n",
    "\n",
    "# Show the problem: section 3.4 is split mid-content\n",
    "for i, chunk in enumerate(fixed_chunks[2:5], 3):  # Show middle chunks\n",
    "    print(f\"--- Chunk {i} ---\")\n",
    "    print(chunk['text'][:300])\n",
    "    print()\n",
    "\n",
    "print(\"\\nâš ï¸  Problem: Section 3.4 requirements are split across multiple chunks\")\n",
    "print(\"   Query 'business account requirements' may miss half the answer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 2C: Semantic chunking (split at section boundaries) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import re\n",
    "\n",
    "def semantic_chunk_policy(text: str) -> List[Dict]:\n",
    "    \"\"\"Split at section headers â€” preserves complete policy sections.\"\"\"\n",
    "    section_pattern = re.compile(r'(?=^\\d+\\.\\d+\\s)', re.MULTILINE)\n",
    "    parts = section_pattern.split(text)\n",
    "    chunks = []\n",
    "    for i, part in enumerate(parts):\n",
    "        part = part.strip()\n",
    "        if not part or len(part) < 50:\n",
    "            continue\n",
    "        # Extract section number\n",
    "        section_match = re.match(r'^(\\d+\\.\\d+)', part)\n",
    "        section_num = section_match.group(1) if section_match else \"intro\"\n",
    "        chunks.append({\n",
    "            \"text\": part,\n",
    "            \"section\": section_num,\n",
    "            \"char_count\": len(part),\n",
    "            \"approx_tokens\": len(part) // 4\n",
    "        })\n",
    "    return chunks\n",
    "\n",
    "semantic_chunks = semantic_chunk_policy(SAMPLE_POLICY_DOCUMENT)\n",
    "\n",
    "print(\"=== SEMANTIC CHUNKING (section-based) ===\")\n",
    "print(f\"Chunks created: {len(semantic_chunks)}\\n\")\n",
    "\n",
    "for chunk in semantic_chunks:\n",
    "    print(f\"Section {chunk['section']}: {chunk['approx_tokens']} tokens\")\n",
    "    print(f\"  Preview: {chunk['text'][:120].strip()}...\")\n",
    "    print()\n",
    "\n",
    "print(\"âœ… Each section is a complete, self-contained chunk\")\n",
    "print(\"   Query 'business account requirements' â†’ retrieves FULL Section 3.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 2D: Head-to-head comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "QUERY = \"What documents are needed for business account opening?\"\n",
    "query_vec = embed(QUERY)\n",
    "\n",
    "# Search in fixed-size chunks\n",
    "fixed_vecs = embed_batch([c['text'] for c in fixed_chunks])\n",
    "fixed_scores = [(cosine_sim(query_vec, v), c['text'][:120]) for v, c in zip(fixed_vecs, fixed_chunks)]\n",
    "fixed_top = sorted(fixed_scores, key=lambda x: x[0], reverse=True)[:3]\n",
    "\n",
    "# Search in semantic chunks\n",
    "semantic_vecs = embed_batch([c['text'] for c in semantic_chunks])\n",
    "semantic_scores = [(cosine_sim(query_vec, v), c['section'], c['text'][:120])\n",
    "                   for v, c in zip(semantic_vecs, semantic_chunks)]\n",
    "semantic_top = sorted(semantic_scores, key=lambda x: x[0], reverse=True)[:3]\n",
    "\n",
    "print(f\"Query: '{QUERY}'\\n\")\n",
    "print(\"=== FIXED-SIZE: Top 3 Results ===\")\n",
    "for score, preview in fixed_top:\n",
    "    print(f\"  [{score:.3f}] {preview}...\")\n",
    "print()\n",
    "print(\"=== SEMANTIC: Top 3 Results ===\")\n",
    "for score, section, preview in semantic_top:\n",
    "    print(f\"  [{score:.3f}] Section {section}: {preview}...\")\n",
    "\n",
    "print(\"\\nðŸ“Œ Semantic chunks return complete policy sections â€” no information loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-demo3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Demo 3: Vector Search â€” Build a Banking Policy Index\n",
    "\n",
    "**Concept:** Store embeddings in ChromaDB, search by meaning  \n",
    "**Banking context:** KYC chatbot â€” find relevant policy for any employee question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 3A: Index all policy chunks into ChromaDB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import chromadb\n",
    "\n",
    "chroma = chromadb.EphemeralClient()  # In-memory, no files needed\n",
    "\n",
    "# Delete if exists (for clean re-runs)\n",
    "try:\n",
    "    chroma.delete_collection(\"banking_policies\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "collection = chroma.create_collection(\n",
    "    name=\"banking_policies\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "# Embed all chunks\n",
    "print(\"Embedding 11 policy chunks...\")\n",
    "t0 = time.time()\n",
    "chunk_embeddings = embed_batch(BANKING_POLICY_CHUNKS)\n",
    "elapsed = time.time() - t0\n",
    "print(f\"âœ… Embedded {len(BANKING_POLICY_CHUNKS)} chunks in {elapsed:.2f}s (local, free!)\")\n",
    "\n",
    "# Store in ChromaDB\n",
    "collection.add(\n",
    "    ids=[f\"chunk_{i}\" for i in range(len(BANKING_POLICY_CHUNKS))],\n",
    "    documents=BANKING_POLICY_CHUNKS,\n",
    "    embeddings=chunk_embeddings.tolist(),\n",
    "    metadatas=CHUNK_METADATA\n",
    ")\n",
    "print(f\"âœ… Indexed into ChromaDB â€” {collection.count()} vectors stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 3B: Run policy queries and inspect results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def vector_search(query: str, n: int = 3, department_filter: str = None) -> List[Dict]:\n",
    "    \"\"\"Semantic search over banking policy chunks.\"\"\"\n",
    "    q_vec = embed(query).tolist()\n",
    "    where = {\"department\": department_filter} if department_filter else None\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[q_vec],\n",
    "        n_results=n,\n",
    "        where=where,\n",
    "        include=[\"documents\", \"distances\", \"metadatas\"]\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"content\":    doc,\n",
    "            \"similarity\": round(1 - dist, 3),\n",
    "            \"section\":    meta.get(\"section\", \"?\"),\n",
    "            \"topic\":      meta.get(\"topic\", \"?\"),\n",
    "            \"doc_id\":     meta.get(\"doc_id\", \"?\"),\n",
    "        }\n",
    "        for doc, dist, meta in zip(\n",
    "            results[\"documents\"][0],\n",
    "            results[\"distances\"][0],\n",
    "            results[\"metadatas\"][0],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# --- Run a battery of banking queries ---\n",
    "BANKING_QUERIES = [\n",
    "    \"What documents do I need to open a business account?\",\n",
    "    \"When must we file a CTR?\",\n",
    "    \"How do we handle PEP customers?\",\n",
    "    \"What are the requirements for suspicious activity reports?\",\n",
    "    \"How long do we keep KYC records?\",\n",
    "    \"OFAC screening process\",\n",
    "    \"Can we waive an overdraft fee?\",\n",
    "]\n",
    "\n",
    "console.print(\"[bold cyan]Banking Policy Vector Search Results[/bold cyan]\\n\")\n",
    "\n",
    "for query in BANKING_QUERIES:\n",
    "    results = vector_search(query, n=2)\n",
    "    print(f\"Q: {query}\")\n",
    "    for r in results:\n",
    "        print(f\"   [{r['similarity']:.3f}] Section {r['section']}: {r['content'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 3C: Metadata filtering (department isolation = security) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=== Metadata Filtering Demo: Department Isolation ===\")\n",
    "print(\"Scenario: Retail branch employee should NOT see Compliance-only policies\\n\")\n",
    "\n",
    "query = \"customer account requirements\"\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print()\n",
    "\n",
    "print(\"WITHOUT filter (all departments):\")\n",
    "for r in vector_search(query, n=3):\n",
    "    print(f\"  [{r['similarity']:.3f}] Sec {r['section']} ({r['topic']}): {r['content'][:80]}...\")\n",
    "\n",
    "print()\n",
    "print(\"WITH filter (retail department only):\")\n",
    "for r in vector_search(query, n=3, department_filter=\"retail\"):\n",
    "    print(f\"  [{r['similarity']:.3f}] Sec {r['section']} ({r['topic']}): {r['content'][:80]}...\")\n",
    "\n",
    "print(\"\\nðŸ“Œ Metadata filtering = row-level security in banking RAG\")\n",
    "print(\"   Retail staff see only retail policies â€” compliance docs hidden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-demo4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Demo 4: Hybrid Search â€” Vector + BM25 Keyword\n",
    "\n",
    "**Concept:** Exact regulatory terms need keyword matching; semantics alone isn't enough  \n",
    "**Banking context:** \"Regulation E\" won't be found by pure vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Pre-build BM25 index over all chunks\n",
    "tokenized_chunks = [chunk.lower().split() for chunk in BANKING_POLICY_CHUNKS]\n",
    "bm25_index = BM25Okapi(tokenized_chunks)\n",
    "\n",
    "def bm25_search(query: str, n: int = 5) -> List[Dict]:\n",
    "    \"\"\"BM25 keyword search.\"\"\"\n",
    "    scores = bm25_index.get_scores(query.lower().split())\n",
    "    top_idx = np.argsort(scores)[-n:][::-1]\n",
    "    max_score = scores.max() or 1.0\n",
    "    return [\n",
    "        {\n",
    "            \"content\":    BANKING_POLICY_CHUNKS[i],\n",
    "            \"bm25_score\": round(float(scores[i] / max_score), 3),  # Normalized 0-1\n",
    "            \"section\":    CHUNK_METADATA[i][\"section\"],\n",
    "            \"topic\":      CHUNK_METADATA[i][\"topic\"],\n",
    "        }\n",
    "        for i in top_idx if scores[i] > 0\n",
    "    ]\n",
    "\n",
    "def hybrid_search(query: str, alpha: float = 0.7, n: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Combine vector + BM25. alpha=1.0 â†’ pure vector, alpha=0.0 â†’ pure BM25.\n",
    "    Reciprocal Rank Fusion approach.\n",
    "    \"\"\"\n",
    "    # Vector scores\n",
    "    q_vec = embed(query)\n",
    "    vec_scores = np.array([cosine_sim(q_vec, embed(c)) for c in BANKING_POLICY_CHUNKS])\n",
    "\n",
    "    # BM25 scores (normalized)\n",
    "    bm25_raw = bm25_index.get_scores(query.lower().split())\n",
    "    bm25_max = bm25_raw.max() or 1.0\n",
    "    bm25_scores = bm25_raw / bm25_max\n",
    "\n",
    "    # Combine\n",
    "    combined = alpha * vec_scores + (1 - alpha) * bm25_scores\n",
    "    top_idx = np.argsort(combined)[-n:][::-1]\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"content\":      BANKING_POLICY_CHUNKS[i],\n",
    "            \"hybrid_score\": round(float(combined[i]), 3),\n",
    "            \"vec_score\":    round(float(vec_scores[i]), 3),\n",
    "            \"bm25_score\":   round(float(bm25_scores[i]), 3),\n",
    "            \"section\":      CHUNK_METADATA[i][\"section\"],\n",
    "            \"topic\":        CHUNK_METADATA[i][\"topic\"],\n",
    "        }\n",
    "        for i in top_idx\n",
    "    ]\n",
    "\n",
    "print(\"âœ… BM25 index and hybrid search ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 4A: Compare pure vector vs hybrid for exact regulatory terms â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "test_queries = [\n",
    "    # (query, alpha, reason)\n",
    "    (\"CTR filing $10,000\",           0.4, \"Exact regulatory threshold â€” BM25 helps\"),\n",
    "    (\"SAR 30 days deadline\",         0.4, \"Exact deadline â€” BM25 critical\"),\n",
    "    (\"enhanced due diligence\",       0.7, \"Semantic concept â€” vector sufficient\"),\n",
    "    (\"beneficial ownership 25%\",     0.5, \"Mix of exact number + concept\"),\n",
    "]\n",
    "\n",
    "for query, alpha, reason in test_queries:\n",
    "    vec_top = vector_search(query, n=1)\n",
    "    hyb_top = hybrid_search(query, alpha=alpha, n=1)\n",
    "\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"Reason: {reason}\")\n",
    "    print(f\"  Vector  [{vec_top[0]['similarity']:.3f}]: Sec {vec_top[0]['section']} â€” {vec_top[0]['content'][:80]}...\")\n",
    "    print(f\"  Hybrid  [{hyb_top[0]['hybrid_score']:.3f}]: Sec {hyb_top[0]['section']} â€” {hyb_top[0]['content'][:80]}...\")\n",
    "    same = vec_top[0]['section'] == hyb_top[0]['section']\n",
    "    print(f\"  Same result: {'Yes âœ“' if same else 'No â€” hybrid found different (potentially better) result'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 4B: Alpha tuning visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Show how different alpha values change which document ranks #1\n",
    "query = \"CTR FinCEN Form 104 $10,000 cash reporting\"\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"\\nHow alpha affects results (0=pure BM25, 1=pure vector):\")\n",
    "print(f\"{'Alpha':<8} {'Top Section':<12} {'Hybrid Score':<14} {'Vec Score':<12} {'BM25 Score'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for alpha in [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "    result = hybrid_search(query, alpha=alpha, n=1)[0]\n",
    "    print(f\"{alpha:<8.1f} {result['section']:<12} {result['hybrid_score']:<14.3f} \"\n",
    "          f\"{result['vec_score']:<12.3f} {result['bm25_score']:.3f}\")\n",
    "\n",
    "print(\"\\nðŸ“Œ For regulatory exact-term queries: alpha=0.3-0.5 (more BM25 weight)\")\n",
    "print(\"   For general policy questions: alpha=0.7-0.9 (more vector weight)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-demo5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Demo 5: Complete RAG Pipeline with Claude\n",
    "\n",
    "**Concept:** Retrieved chunks + Claude = cited, accurate banking answers  \n",
    "**Requires:** `ANTHROPIC_API_KEY` environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "# Set your key: os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-ant-...\"\n",
    "claude = anthropic.Anthropic()  # Reads ANTHROPIC_API_KEY from env\n",
    "\n",
    "def banking_rag(query: str,\n",
    "                top_k: int = 3,\n",
    "                use_hybrid: bool = True,\n",
    "                department_filter: str = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline:\n",
    "      1. Retrieve relevant chunks (hybrid or vector)\n",
    "      2. Build prompt with citations\n",
    "      3. Generate answer with Claude\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Step 1: Retrieve\n",
    "    if use_hybrid:\n",
    "        chunks = hybrid_search(query, alpha=0.6, n=top_k)\n",
    "        method = \"hybrid\"\n",
    "    else:\n",
    "        chunks = vector_search(query, n=top_k, department_filter=department_filter)\n",
    "        method = \"vector\"\n",
    "    retrieve_ms = (time.time() - t0) * 1000\n",
    "\n",
    "    # Step 2: Build context with citations\n",
    "    context_blocks = [\n",
    "        f\"[Source {i+1}: {c.get('doc_id','POL-KYC-2024')}, \"\n",
    "        f\"Section {c['section']}]\\n{c['content']}\"\n",
    "        for i, c in enumerate(chunks)\n",
    "    ]\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_blocks)\n",
    "\n",
    "    # Step 3: Generate with Claude\n",
    "    t1 = time.time()\n",
    "    response = claude.messages.create(\n",
    "        model=\"claude-haiku-4-5-20251001\",  # Fast + cheap for demos\n",
    "        max_tokens=400,\n",
    "        system=(\n",
    "            \"You are a banking compliance assistant. \"\n",
    "            \"Answer using ONLY the provided policy sources. \"\n",
    "            \"Always cite the Source number and Section. \"\n",
    "            \"If the answer is not in the sources, say so clearly.\"\n",
    "        ),\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Policy Sources:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "        }]\n",
    "    )\n",
    "    generate_ms = (time.time() - t1) * 1000\n",
    "    total_ms = (time.time() - t0) * 1000\n",
    "\n",
    "    answer = response.content[0].text\n",
    "    input_tokens  = response.usage.input_tokens\n",
    "    output_tokens = response.usage.output_tokens\n",
    "    cost = (input_tokens * 0.00025 + output_tokens * 0.00125) / 1000  # Haiku pricing\n",
    "\n",
    "    return {\n",
    "        \"query\":        query,\n",
    "        \"answer\":       answer,\n",
    "        \"chunks_used\":  len(chunks),\n",
    "        \"method\":       method,\n",
    "        \"retrieve_ms\":  round(retrieve_ms),\n",
    "        \"generate_ms\":  round(generate_ms),\n",
    "        \"total_ms\":     round(total_ms),\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\":output_tokens,\n",
    "        \"cost_usd\":     round(cost, 5),\n",
    "        \"sources\":      [(c['section'], c['topic']) for c in chunks],\n",
    "    }\n",
    "\n",
    "print(\"âœ… RAG pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 5A: Run banking policy Q&A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "questions = [\n",
    "    \"What documents are required to open a business account?\",\n",
    "    \"When must we file a Currency Transaction Report?\",\n",
    "    \"How should we handle a customer who is a PEP?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(\"=\" * 70)\n",
    "    result = banking_rag(question)\n",
    "    print(f\"Q: {result['query']}\")\n",
    "    print(f\"\\nA: {result['answer']}\")\n",
    "    print(f\"\\nðŸ“Š Sources used: {result['sources']}\")\n",
    "    print(f\"âš¡ Latency: {result['total_ms']}ms \"\n",
    "          f\"(retrieve: {result['retrieve_ms']}ms, generate: {result['generate_ms']}ms)\")\n",
    "    print(f\"ðŸ’° Cost: ${result['cost_usd']:.5f} | \"\n",
    "          f\"Tokens: {result['input_tokens']} in / {result['output_tokens']} out\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 5B: What happens when the answer is NOT in the database? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "out_of_scope = [\n",
    "    \"What is the current Fed Funds rate?\",\n",
    "    \"How do I process a mortgage application?\",\n",
    "]\n",
    "\n",
    "print(\"=== RAG Handles Unknown Questions Gracefully ===\")\n",
    "for q in out_of_scope:\n",
    "    result = banking_rag(q)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(f\"Sources searched: {result['sources']}\")\n",
    "    print()\n",
    "\n",
    "print(\"ðŸ“Œ Claude correctly says it cannot answer from the provided sources\")\n",
    "print(\"   This is the right behavior â€” no hallucination!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-demo6-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Demo 6: RAG Failure Modes â€” Learn to Debug\n",
    "\n",
    "**Concept:** See what goes wrong and WHY â€” then fix it  \n",
    "**Banking context:** Common failures in compliance chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 6A: Failure Mode 1 â€” Wrong chunk size (too large) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 60)\n",
    "print(\"FAILURE 1: Chunk size too large (noisy context)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate large chunks by concatenating 3 real chunks together\n",
    "LARGE_CHUNKS = [\n",
    "    \" \".join(BANKING_POLICY_CHUNKS[i:i+3])\n",
    "    for i in range(0, len(BANKING_POLICY_CHUNKS), 3)\n",
    "]\n",
    "\n",
    "large_vecs = embed_batch(LARGE_CHUNKS)\n",
    "query = \"CTR filing deadline\"\n",
    "q_vec = embed(query)\n",
    "\n",
    "# Search large chunks\n",
    "large_scores = [(cosine_sim(q_vec, v), chunk[:200]) for v, chunk in zip(large_vecs, LARGE_CHUNKS)]\n",
    "large_top = sorted(large_scores, reverse=True)[0]\n",
    "\n",
    "# Search normal chunks\n",
    "normal_scores = [(cosine_sim(q_vec, embed(c)), c[:200]) for c in BANKING_POLICY_CHUNKS]\n",
    "normal_top = sorted(normal_scores, reverse=True)[0]\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(f\"LARGE chunks (1500+ chars): score={large_top[0]:.3f}\")\n",
    "print(f\"  Content: {large_top[1]}...\")\n",
    "print()\n",
    "print(f\"NORMAL chunks (~500 chars): score={normal_top[0]:.3f}\")\n",
    "print(f\"  Content: {normal_top[1]}...\")\n",
    "print()\n",
    "print(\"ðŸ“Œ Large chunks dilute the relevant signal with unrelated content\")\n",
    "print(\"   Lower similarity score â†’ may not rank #1 â†’ missed retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 6B: Failure Mode 2 â€” No overlap (concept split at boundary) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 60)\n",
    "print(\"FAILURE 2: Zero overlap â€” concepts split at boundaries\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a document where key info straddles two fixed chunks\n",
    "doc_with_split = (\n",
    "    \"The Bank Secrecy Act requires financial institutions to maintain detailed records \"\n",
    "    \"of all customer transactions. Compliance officers must ensure that all reporting \"\n",
    "    \"thresholds are properly monitored. \" * 5 +  # Padding to push next sentence to chunk 2\n",
    "    \"Currency Transaction Reports must be filed for cash transactions exceeding $10,000 \"\n",
    "    \"within 15 calendar days of the transaction date.\"\n",
    ")\n",
    "\n",
    "chunk_size = 400\n",
    "\n",
    "# No overlap\n",
    "no_overlap = [\n",
    "    doc_with_split[i:i+chunk_size]\n",
    "    for i in range(0, len(doc_with_split), chunk_size)\n",
    "]\n",
    "\n",
    "# With overlap\n",
    "overlap = 80\n",
    "with_overlap = [\n",
    "    doc_with_split[i:i+chunk_size]\n",
    "    for i in range(0, len(doc_with_split), chunk_size - overlap)\n",
    "]\n",
    "\n",
    "# Find the chunk containing CTR info in each\n",
    "ctr_query = embed(\"CTR filing $10,000 deadline\")\n",
    "\n",
    "no_overlap_scores = [cosine_sim(ctr_query, embed(c)) for c in no_overlap]\n",
    "with_overlap_scores = [cosine_sim(ctr_query, embed(c)) for c in with_overlap]\n",
    "\n",
    "print(f\"Query: 'CTR filing $10,000 deadline'\")\n",
    "print(f\"\\nNO OVERLAP chunks ({len(no_overlap)} chunks):\")\n",
    "for i, (score, chunk) in enumerate(zip(no_overlap_scores, no_overlap)):\n",
    "    ctr_present = '$10,000' in chunk or 'CTR' in chunk\n",
    "    print(f\"  Chunk {i+1}: score={score:.3f} | CTR info present: {ctr_present}\")\n",
    "\n",
    "print(f\"\\nWITH OVERLAP ({overlap} chars) chunks ({len(with_overlap)} chunks):\")\n",
    "for i, (score, chunk) in enumerate(zip(with_overlap_scores, with_overlap)):\n",
    "    ctr_present = '$10,000' in chunk or 'CTR' in chunk\n",
    "    print(f\"  Chunk {i+1}: score={score:.3f} | CTR info present: {ctr_present}\")\n",
    "\n",
    "print(\"\\nðŸ“Œ Overlap ensures key sentences appear in multiple chunks\")\n",
    "print(\"   Recommendation: 10-20% overlap for banking policy documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 6C: Failure Mode 3 â€” K too high (context flooding) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 60)\n",
    "print(\"FAILURE 3: K too high â€” noisy context causes confusion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "query = \"How do I open a business account?\"\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "for k in [1, 3, 5, 8]:\n",
    "    results = vector_search(query, n=k)\n",
    "    relevant = sum(1 for r in results if r['topic'] in ['account_opening'])\n",
    "    precision = relevant / k\n",
    "    total_chars = sum(len(r['content']) for r in results)\n",
    "    print(f\"K={k}: Precision={precision:.0%} | \"\n",
    "          f\"Relevant={relevant}/{k} | \"\n",
    "          f\"Context size={total_chars} chars | \"\n",
    "          f\"Topics: {[r['topic'] for r in results]}\")\n",
    "\n",
    "print(\"\\nðŸ“Œ K=3-5 is the sweet spot for banking compliance queries\")\n",
    "print(\"   K>5 introduces noise: overdraft policy mixed into account-opening answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 6D: Debug helper â€” show exactly what RAG sees â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def debug_retrieval(query: str, n: int = 5):\n",
    "    \"\"\"Full retrieval debug: scores, content, topics.\"\"\"\n",
    "    q_vec = embed(query)\n",
    "    all_scores = [\n",
    "        (cosine_sim(q_vec, embed(c)), i, c[:120], CHUNK_METADATA[i])\n",
    "        for i, c in enumerate(BANKING_POLICY_CHUNKS)\n",
    "    ]\n",
    "    all_scores.sort(reverse=True)\n",
    "\n",
    "    console.print(f\"\\n[bold cyan]Debug Retrieval: '{query}'[/bold cyan]\")\n",
    "    t = Table(show_header=True)\n",
    "    t.add_column(\"Rank\", justify=\"right\", width=5)\n",
    "    t.add_column(\"Score\", justify=\"right\", width=7)\n",
    "    t.add_column(\"Section\", width=8)\n",
    "    t.add_column(\"Topic\", width=16)\n",
    "    t.add_column(\"Preview\", max_width=60)\n",
    "\n",
    "    for rank, (score, idx, preview, meta) in enumerate(all_scores, 1):\n",
    "        color = \"green\" if score > 0.6 else \"yellow\" if score > 0.45 else \"red\"\n",
    "        t.add_row(\n",
    "            str(rank),\n",
    "            f\"[{color}]{score:.3f}[/{color}]\",\n",
    "            meta[\"section\"],\n",
    "            meta[\"topic\"],\n",
    "            preview + \"...\"\n",
    "        )\n",
    "        if rank == n:\n",
    "            t.add_row(\"...\", \"---\", \"---\", \"---\", f\"({len(all_scores)-n} more not shown)\")\n",
    "            break\n",
    "\n",
    "    console.print(t)\n",
    "\n",
    "# Run debug on a few queries\n",
    "debug_retrieval(\"CTR filing requirements\")\n",
    "debug_retrieval(\"employee leave policy\")  # Not in our KB â€” watch scores drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-demo7-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Demo 7: Advanced Retrieval â€” Re-ranking, HyDE, Contextual RAG\n",
    "\n",
    "**Concept:** Push precision above 85% with post-retrieval techniques  \n",
    "**Banking context:** Critical compliance queries where accuracy is non-negotiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 7A: Re-ranking with cross-encoder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Cross-encoders score (query, document) PAIRS â€” slower but more accurate\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import CrossEncoder\n",
    "    reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "    RERANKER_AVAILABLE = True\n",
    "    print(\"âœ… Cross-encoder loaded (downloads ~70MB once)\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Cross-encoder not available: {e}\")\n",
    "    print(\"   Install: pip install sentence-transformers\")\n",
    "    RERANKER_AVAILABLE = False\n",
    "\n",
    "def retrieve_and_rerank(query: str, initial_k: int = 8, final_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Two-stage retrieval:\n",
    "      Stage 1: Fast vector search (broad)\n",
    "      Stage 2: Cross-encoder re-ranking (precise)\n",
    "    \"\"\"\n",
    "    # Stage 1: Broad retrieval\n",
    "    initial = vector_search(query, n=initial_k)\n",
    "\n",
    "    if not RERANKER_AVAILABLE:\n",
    "        return initial[:final_k]\n",
    "\n",
    "    # Stage 2: Re-rank with cross-encoder\n",
    "    pairs = [(query, r['content']) for r in initial]\n",
    "    rerank_scores = reranker.predict(pairs)\n",
    "\n",
    "    # Sort by cross-encoder score\n",
    "    reranked = sorted(\n",
    "        zip(rerank_scores, initial),\n",
    "        key=lambda x: x[0],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        {**item, \"rerank_score\": round(float(score), 3)}\n",
    "        for score, item in reranked[:final_k]\n",
    "    ]\n",
    "\n",
    "# Compare: vector vs reranked\n",
    "query = \"suspicious activity SAR filing deadline compliance\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "print(\"Vector search (top 3):\")\n",
    "for r in vector_search(query, n=3):\n",
    "    print(f\"  [{r['similarity']:.3f}] Sec {r['section']} ({r['topic']}): {r['content'][:80]}...\")\n",
    "\n",
    "print()\n",
    "print(\"After re-ranking (top 3 from initial 8):\")\n",
    "for r in retrieve_and_rerank(query, initial_k=8, final_k=3):\n",
    "    score_str = f\"{r.get('rerank_score', r['similarity']):.3f}\"\n",
    "    print(f\"  [{score_str}] Sec {r['section']} ({r['topic']}): {r['content'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 7B: HyDE â€” Hypothetical Document Embeddings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Insight: A hypothetical answer is MORE similar to real docs than the question itself\n",
    "\n",
    "def hyde_retrieve(question: str, n: int = 3) -> List[Dict]:\n",
    "    \"\"\"Generate a hypothetical answer, embed it, search with that embedding.\"\"\"\n",
    "\n",
    "    # Step 1: Generate hypothetical answer with Claude\n",
    "    response = claude.messages.create(\n",
    "        model=\"claude-haiku-4-5-20251001\",\n",
    "        max_tokens=150,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"You are a banking compliance expert. Write a concise, \"\n",
    "                f\"factual 2-3 sentence answer to this question as if from \"\n",
    "                f\"official banking policy. Include specific numbers and terms.\\n\\n\"\n",
    "                f\"Question: {question}\\n\\nAnswer:\"\n",
    "            )\n",
    "        }]\n",
    "    )\n",
    "    hypothetical = response.content[0].text\n",
    "\n",
    "    # Step 2: Embed the hypothetical answer\n",
    "    hyp_vec = embed(hypothetical)\n",
    "\n",
    "    # Step 3: Search using hypothetical answer embedding\n",
    "    q_vec_scores = [\n",
    "        (cosine_sim(hyp_vec, embed(c)), i)\n",
    "        for i, c in enumerate(BANKING_POLICY_CHUNKS)\n",
    "    ]\n",
    "    top = sorted(q_vec_scores, reverse=True)[:n]\n",
    "\n",
    "    return {\n",
    "        \"hypothetical_answer\": hypothetical,\n",
    "        \"results\": [\n",
    "            {\n",
    "                \"content\":    BANKING_POLICY_CHUNKS[i],\n",
    "                \"similarity\": round(score, 3),\n",
    "                \"section\":    CHUNK_METADATA[i][\"section\"],\n",
    "                \"topic\":      CHUNK_METADATA[i][\"topic\"],\n",
    "            }\n",
    "            for score, i in top\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Demo\n",
    "question = \"What is the exact dollar threshold for filing a Currency Transaction Report?\"\n",
    "\n",
    "print(f\"Question: '{question}'\\n\")\n",
    "\n",
    "# Standard search\n",
    "std = vector_search(question, n=1)\n",
    "print(f\"Standard vector search top result: [{std[0]['similarity']:.3f}]\")\n",
    "print(f\"  {std[0]['content'][:120]}...\")\n",
    "\n",
    "# HyDE search\n",
    "hyde = hyde_retrieve(question, n=1)\n",
    "print(f\"\\nHyDE hypothetical answer generated:\")\n",
    "print(f\"  '{hyde['hypothetical_answer']}'\")\n",
    "print(f\"\\nHyDE top result: [{hyde['results'][0]['similarity']:.3f}]\")\n",
    "print(f\"  {hyde['results'][0]['content'][:120]}...\")\n",
    "\n",
    "std_score = std[0]['similarity']\n",
    "hyde_score = hyde['results'][0]['similarity']\n",
    "print(f\"\\nðŸ“Œ HyDE similarity: {hyde_score:.3f} vs Standard: {std_score:.3f} \"\n",
    "      f\"({'better' if hyde_score > std_score else 'similar'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 7C: Contextual RAG â€” add context to chunks before embedding â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Anthropic research: prepending document context reduces retrieval failures by 35%\n",
    "\n",
    "def add_chunk_context(chunk: str, doc_title: str, section_title: str) -> str:\n",
    "    \"\"\"\n",
    "    Prepend context so isolated chunks carry document-level information.\n",
    "    This makes each chunk self-describing.\n",
    "    \"\"\"\n",
    "    context = (\n",
    "        f\"This excerpt is from {doc_title}. \"\n",
    "        f\"It covers {section_title}. \"\n",
    "    )\n",
    "    return context + chunk\n",
    "\n",
    "SECTION_TITLES = {\n",
    "    \"3.4\": \"business account opening requirements and FinCEN CDD compliance\",\n",
    "    \"3.5\": \"individual retail account opening identity verification\",\n",
    "    \"4.2\": \"Currency Transaction Report CTR filing thresholds and deadlines\",\n",
    "    \"5.1\": \"Politically Exposed Person PEP enhanced due diligence\",\n",
    "    \"6.3\": \"Suspicious Activity Report SAR filing obligations\",\n",
    "    \"4.5\": \"international wire transfer monitoring and Travel Rule compliance\",\n",
    "    \"8.2\": \"KYC record retention requirements\",\n",
    "    \"9.1\": \"customer risk rating framework CDD\",\n",
    "    \"11.4\": \"overdraft fee waiver policy\",\n",
    "    \"2.1\": \"Basel III capital adequacy CET1 requirements\",\n",
    "    \"7.1\": \"OFAC sanctions compliance SDN screening\",\n",
    "}\n",
    "\n",
    "# Create contextual versions of all chunks\n",
    "CONTEXTUAL_CHUNKS = [\n",
    "    add_chunk_context(\n",
    "        chunk,\n",
    "        doc_title=meta[\"doc_id\"],\n",
    "        section_title=SECTION_TITLES.get(meta[\"section\"], meta[\"topic\"])\n",
    "    )\n",
    "    for chunk, meta in zip(BANKING_POLICY_CHUNKS, CHUNK_METADATA)\n",
    "]\n",
    "\n",
    "# Embed contextual chunks\n",
    "contextual_vecs = embed_batch(CONTEXTUAL_CHUNKS)\n",
    "original_vecs   = embed_batch(BANKING_POLICY_CHUNKS)\n",
    "\n",
    "# Compare on ambiguous short query\n",
    "short_queries = [\n",
    "    \"reporting threshold\",       # Could be CTR or SAR\n",
    "    \"ownership documentation\",   # Could be business or trust\n",
    "    \"filing deadline\",           # Could be CTR, SAR, or OFAC\n",
    "]\n",
    "\n",
    "print(\"=== Contextual RAG: Before vs After Adding Context ===\")\n",
    "for q in short_queries:\n",
    "    q_vec = embed(q)\n",
    "\n",
    "    orig_scores  = [(cosine_sim(q_vec, v), CHUNK_METADATA[i][\"section\"])\n",
    "                    for i, v in enumerate(original_vecs)]\n",
    "    ctx_scores   = [(cosine_sim(q_vec, v), CHUNK_METADATA[i][\"section\"])\n",
    "                    for i, v in enumerate(contextual_vecs)]\n",
    "\n",
    "    orig_top  = sorted(orig_scores,  reverse=True)[0]\n",
    "    ctx_top   = sorted(ctx_scores,   reverse=True)[0]\n",
    "\n",
    "    print(f\"\\nQuery: '{q}'\")\n",
    "    print(f\"  Original   â†’ Sec {orig_top[1]} (score: {orig_top[0]:.3f})\")\n",
    "    print(f\"  Contextual â†’ Sec {ctx_top[1]}  (score: {ctx_top[0]:.3f})\",\n",
    "          \"âœ“ improved\" if ctx_top[0] > orig_top[0] else \"\")\n",
    "\n",
    "print(\"\\nðŸ“Œ Contextual RAG: prepend 2-3 sentences of document context to each chunk\")\n",
    "print(\"   Anthropic reports 35% fewer retrieval failures with this technique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-demo8-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Demo 8: Evaluation â€” Measure RAG Quality\n",
    "\n",
    "**Concept:** Quantify how good your RAG system is with real metrics  \n",
    "**Banking context:** Build a golden test set and track quality over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 8A: Golden test set â€” query â†’ expected sections â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# In production: compliance experts label these\n",
    "\n",
    "GOLDEN_TEST_SET = [\n",
    "    {\n",
    "        \"query\":            \"What documents are required for business account opening?\",\n",
    "        \"relevant_sections\": {\"3.4\"},  # Section 3.4 is the correct answer\n",
    "    },\n",
    "    {\n",
    "        \"query\":            \"When must a CTR be filed?\",\n",
    "        \"relevant_sections\": {\"4.2\"},\n",
    "    },\n",
    "    {\n",
    "        \"query\":            \"How do we handle PEP customers?\",\n",
    "        \"relevant_sections\": {\"5.1\"},\n",
    "    },\n",
    "    {\n",
    "        \"query\":            \"What are the SAR filing requirements?\",\n",
    "        \"relevant_sections\": {\"6.3\"},\n",
    "    },\n",
    "    {\n",
    "        \"query\":            \"OFAC sanctions screening process\",\n",
    "        \"relevant_sections\": {\"7.1\"},\n",
    "    },\n",
    "    {\n",
    "        \"query\":            \"Record retention for wire transfers and CTR\",\n",
    "        \"relevant_sections\": {\"8.2\", \"4.2\"},  # Both sections relevant\n",
    "    },\n",
    "    {\n",
    "        \"query\":            \"Can we waive an overdraft fee for a good customer?\",\n",
    "        \"relevant_sections\": {\"11.4\"},\n",
    "    },\n",
    "    {\n",
    "        \"query\":            \"Basel III CET1 capital ratio requirement\",\n",
    "        \"relevant_sections\": {\"2.1\"},\n",
    "    },\n",
    "]\n",
    "\n",
    "def evaluate_retrieval(test_set: List[Dict], search_fn, k: int = 3) -> Dict:\n",
    "    \"\"\"Calculate Precision@K and Recall@K over a golden test set.\"\"\"\n",
    "    precisions, recalls, mrrs = [], [], []\n",
    "\n",
    "    for test in test_set:\n",
    "        query    = test[\"query\"]\n",
    "        relevant = test[\"relevant_sections\"]\n",
    "        results  = search_fn(query, n=k)\n",
    "\n",
    "        retrieved_sections = [r[\"section\"] for r in results]\n",
    "\n",
    "        hits = sum(1 for s in retrieved_sections if s in relevant)\n",
    "        precision = hits / k\n",
    "        recall    = hits / len(relevant)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "        # MRR: reciprocal rank of first relevant result\n",
    "        mrr = 0.0\n",
    "        for rank, section in enumerate(retrieved_sections, 1):\n",
    "            if section in relevant:\n",
    "                mrr = 1.0 / rank\n",
    "                break\n",
    "        mrrs.append(mrr)\n",
    "\n",
    "    return {\n",
    "        f\"Precision@{k}\": round(np.mean(precisions), 3),\n",
    "        f\"Recall@{k}\":    round(np.mean(recalls), 3),\n",
    "        \"MRR\":           round(np.mean(mrrs), 3),\n",
    "    }\n",
    "\n",
    "# Evaluate different strategies\n",
    "K = 3\n",
    "\n",
    "vector_metrics = evaluate_retrieval(\n",
    "    GOLDEN_TEST_SET,\n",
    "    lambda q, n: vector_search(q, n=n),\n",
    "    k=K\n",
    ")\n",
    "hybrid_metrics = evaluate_retrieval(\n",
    "    GOLDEN_TEST_SET,\n",
    "    lambda q, n: hybrid_search(q, alpha=0.6, n=n),\n",
    "    k=K\n",
    ")\n",
    "rerank_metrics = evaluate_retrieval(\n",
    "    GOLDEN_TEST_SET,\n",
    "    lambda q, n: retrieve_and_rerank(q, initial_k=8, final_k=n),\n",
    "    k=K\n",
    ")\n",
    "\n",
    "# Display comparison\n",
    "console.print(\"\\n[bold cyan]RAG Strategy Comparison (Banking Golden Test Set)[/bold cyan]\\n\")\n",
    "\n",
    "table = Table(show_header=True)\n",
    "table.add_column(\"Strategy\",         style=\"cyan\",   width=22)\n",
    "table.add_column(f\"Precision@{K}\",   style=\"green\",  justify=\"right\")\n",
    "table.add_column(f\"Recall@{K}\",      style=\"yellow\", justify=\"right\")\n",
    "table.add_column(\"MRR\",              style=\"magenta\",justify=\"right\")\n",
    "table.add_column(\"Approx Latency\",   style=\"white\",  justify=\"right\")\n",
    "\n",
    "strategies = [\n",
    "    (\"Vector Search\",          vector_metrics, \"~5ms\"),\n",
    "    (\"Hybrid (Î±=0.6)\",         hybrid_metrics, \"~10ms\"),\n",
    "    (\"Hybrid + Re-rank\",        rerank_metrics, \"~200ms\"),\n",
    "]\n",
    "\n",
    "for name, metrics, latency in strategies:\n",
    "    table.add_row(\n",
    "        name,\n",
    "        str(metrics[f\"Precision@{K}\"]),\n",
    "        str(metrics[f\"Recall@{K}\"]),\n",
    "        str(metrics[\"MRR\"]),\n",
    "        latency\n",
    "    )\n",
    "\n",
    "console.print(table)\n",
    "print(\"\\nðŸ“Œ Higher is better. Banking compliance target: Precision@3 > 0.80, MRR > 0.80\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 8B: Per-query breakdown â€” find where retrieval fails â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=== Per-Query Retrieval Analysis ===\")\n",
    "print(f\"{'Query':<50} {'Expected':<10} {'Got':<25} {'Pass'}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for test in GOLDEN_TEST_SET:\n",
    "    results = hybrid_search(test[\"query\"], alpha=0.6, n=3)\n",
    "    retrieved_sections = set(r[\"section\"] for r in results)\n",
    "    expected = test[\"relevant_sections\"]\n",
    "    hit = bool(expected & retrieved_sections)  # At least one relevant section found\n",
    "\n",
    "    status = \"âœ…\" if hit else \"âŒ\"\n",
    "    print(f\"{test['query'][:50]:<50} \"\n",
    "          f\"{str(expected):<10} \"\n",
    "          f\"{str(retrieved_sections):<25} \"\n",
    "          f\"{status}\")\n",
    "\n",
    "print(\"\\nðŸ“Œ Failed queries reveal where to improve:\")\n",
    "print(\"   â†’ Wrong embedding model (domain mismatch)\")\n",
    "print(\"   â†’ Chunk too large (signal diluted)\")\n",
    "print(\"   â†’ Missing metadata (wrong department filter)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 8C: Cost and latency benchmark â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=== Cost & Latency Benchmark ===\")\n",
    "print(\"Simulating 50 queries across strategies\\n\")\n",
    "\n",
    "N_QUERIES = 50\n",
    "TEST_QUERIES = [\n",
    "    \"CTR filing requirements\",\n",
    "    \"PEP customer handling\",\n",
    "    \"SAR deadline\",\n",
    "    \"business account documents\",\n",
    "    \"OFAC screening\",\n",
    "] * 10  # 50 queries\n",
    "\n",
    "def benchmark_strategy(name: str, search_fn, queries: List[str]):\n",
    "    times = []\n",
    "    for q in queries:\n",
    "        t0 = time.time()\n",
    "        search_fn(q)\n",
    "        times.append((time.time() - t0) * 1000)\n",
    "    return {\n",
    "        \"strategy\": name,\n",
    "        \"avg_ms\":   round(np.mean(times), 1),\n",
    "        \"p95_ms\":   round(np.percentile(times, 95), 1),\n",
    "        \"embed_cost_1k_queries\": \"$0.00 (local)\",\n",
    "    }\n",
    "\n",
    "results = [\n",
    "    benchmark_strategy(\"Vector (K=3)\",       lambda q: vector_search(q, n=3),           TEST_QUERIES),\n",
    "    benchmark_strategy(\"Hybrid (K=3)\",       lambda q: hybrid_search(q, n=3),           TEST_QUERIES),\n",
    "    benchmark_strategy(\"Hybrid + Rerank\",    lambda q: retrieve_and_rerank(q, 8, 3),    TEST_QUERIES),\n",
    "]\n",
    "\n",
    "table = Table(title=\"Retrieval Strategy Benchmark (50 queries)\")\n",
    "table.add_column(\"Strategy\",          style=\"cyan\")\n",
    "table.add_column(\"Avg Latency\",       style=\"green\",  justify=\"right\")\n",
    "table.add_column(\"P95 Latency\",       style=\"yellow\", justify=\"right\")\n",
    "table.add_column(\"Embed Cost/1K\",     style=\"magenta\")\n",
    "\n",
    "for r in results:\n",
    "    table.add_row(r[\"strategy\"], f\"{r['avg_ms']}ms\", f\"{r['p95_ms']}ms\",\n",
    "                  r[\"embed_cost_1k_queries\"])\n",
    "\n",
    "console.print(table)\n",
    "print(\"\\nðŸ“Œ sentence-transformers runs locally â€” zero embedding API cost\")\n",
    "print(\"   In production with OpenAI embeddings: ~$0.02 per 1M tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-exercise-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Hands-On Exercise: Build Your Own Banking RAG\n",
    "\n",
    "**Scenario:** The compliance team needs a SAR/CTR decision helper for call center agents.\n",
    "\n",
    "**Your task:** Extend the RAG system to:\n",
    "1. Add 3 new policy chunks (write them yourself)\n",
    "2. Choose the best retrieval strategy\n",
    "3. Tune K for best precision\n",
    "4. Test with 5 realistic call center queries\n",
    "5. Measure Precision@K on your test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Exercise: Your Turn â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# STEP 1: Add your own policy chunks (write realistic banking policy text)\n",
    "MY_POLICY_CHUNKS = [\n",
    "    # TODO: Write 3 realistic banking policy chunks below\n",
    "    \"Section X.X: [Your policy text here...]\",\n",
    "    \"Section X.X: [Your policy text here...]\",\n",
    "    \"Section X.X: [Your policy text here...]\",\n",
    "]\n",
    "\n",
    "MY_METADATA = [\n",
    "    {\"section\": \"X.1\", \"topic\": \"your_topic\", \"department\": \"compliance\", \"doc_id\": \"POL-MY-2024\"},\n",
    "    {\"section\": \"X.2\", \"topic\": \"your_topic\", \"department\": \"compliance\", \"doc_id\": \"POL-MY-2024\"},\n",
    "    {\"section\": \"X.3\", \"topic\": \"your_topic\", \"department\": \"compliance\", \"doc_id\": \"POL-MY-2024\"},\n",
    "]\n",
    "\n",
    "# STEP 2: Add to existing collection\n",
    "all_chunks = BANKING_POLICY_CHUNKS + MY_POLICY_CHUNKS\n",
    "all_metadata = CHUNK_METADATA + MY_METADATA\n",
    "all_vecs = embed_batch(all_chunks)\n",
    "print(f\"Total chunks in knowledge base: {len(all_chunks)}\")\n",
    "\n",
    "# STEP 3: Test your queries\n",
    "MY_TEST_QUERIES = [\n",
    "    # TODO: Write 5 realistic call center questions\n",
    "    \"Question 1\",\n",
    "    \"Question 2\",\n",
    "    \"Question 3\",\n",
    "    \"Question 4\",\n",
    "    \"Question 5\",\n",
    "]\n",
    "\n",
    "print(\"\\n=== Your RAG Results ===\")\n",
    "for q in MY_TEST_QUERIES:\n",
    "    q_vec = embed(q)\n",
    "    scores = [(cosine_sim(q_vec, v), i) for i, v in enumerate(all_vecs)]\n",
    "    top = sorted(scores, reverse=True)[:3]\n",
    "\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    for score, idx in top:\n",
    "        meta = all_metadata[idx]\n",
    "        print(f\"  [{score:.3f}] Sec {meta['section']}: {all_chunks[idx][:100]}...\")\n",
    "\n",
    "print(\"\\nâœ… Exercise complete! Discuss with your team:\")\n",
    "print(\"   - Which chunk size worked best?\")\n",
    "print(\"   - When did vector search fail? When did hybrid help?\")\n",
    "print(\"   - What metadata would you add for your bank's use case?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: What We Built\n",
    "\n",
    "| Demo | Concept | Key Insight |\n",
    "|------|---------|-------------|\n",
    "| 1. Embeddings | Text â†’ vectors | `KYC` â‰ˆ `Know Your Customer` in vector space |\n",
    "| 2. Chunking | Split strategies | Semantic chunks preserve complete policy sections |\n",
    "| 3. Vector Search | ChromaDB index | Metadata filters = department-level security |\n",
    "| 4. Hybrid Search | BM25 + vector | Regulatory exact terms need keyword matching |\n",
    "| 5. Full RAG | Claude generation | Cited answers from policy, no hallucination |\n",
    "| 6. Failure Modes | Debug retrieval | Large chunks & no overlap cause retrieval failures |\n",
    "| 7. Advanced RAG | Re-rank, HyDE, Contextual | +15-35% precision on compliance queries |\n",
    "| 8. Evaluation | Precision, Recall, MRR | Build golden test set â†’ track quality over time |\n",
    "\n",
    "### Production Banking RAG Stack Recommendation\n",
    "\n",
    "```\n",
    "Embedding:    sentence-transformers (local) or text-embedding-3-small (OpenAI)\n",
    "Chunking:     Semantic (512 tokens, 10% overlap) for policy docs\n",
    "Vector DB:    pgvector (already approved in most banks) or Chroma (prototypes)\n",
    "Retrieval:    Hybrid (Î±=0.5) + re-ranking for compliance queries\n",
    "Security:     Metadata filters by department + security_classification\n",
    "Generation:   Claude with citation requirements and grounding instruction\n",
    "Evaluation:   Weekly Precision@3 and MRR on golden test set\n",
    "```\n",
    "\n",
    "### Next: Session 4 â€” Context Engineering\n",
    "How to manage the context window that RAG fills â€” token budgeting, prompt structure, caching."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
