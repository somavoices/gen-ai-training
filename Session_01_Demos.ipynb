{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1: AI/GenAI Fundamentals â€” Interactive Demos\n",
    "\n",
    "**Audience:** Banking Technologists  \n",
    "**Duration:** Run cells live during training (~45 min of demos)  \n",
    "\n",
    "---\n",
    "\n",
    "## Setup â€” Run This First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install -q tiktoken scikit-learn numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports ready! Let's explore AI fundamentals.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import textwrap, random, time, json\n",
    "\n",
    "print(\"All imports ready! Let's explore AI fundamentals.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: How Machines Understand Words â€” Embeddings\n",
    "\n",
    "Before LLMs can predict the next token, they need to **convert words into numbers**.  \n",
    "These numbers are called **embeddings** â€” vectors that capture *meaning*.\n",
    "\n",
    "The famous result: **King - Man + Woman = Queen**  \n",
    "Let's see if it actually works â€” and what it reveals about bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Pre-Computed Word Embeddings\n",
    "\n",
    "We're using **pre-computed results from GloVe** (Global Vectors) â€” trained on billions of words.  \n",
    "Same concept as the embedding layer inside ChatGPT/Claude, just smaller and inspectable.\n",
    "\n",
    "**Note:** No gensim installation needed! Results are pre-computed for faster demos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "WORD EMBEDDINGS â€” Documented Research Biases\n",
      "======================================================================\n",
      "Using REAL biases from Word2Vec/GloVe research papers\n",
      "Source: Google News Word2Vec (3 billion words, 300 dimensions)\n",
      "\n",
      "âœ… Research-documented examples loaded\n",
      "   Total examples: 17\n",
      "\n",
      "ğŸ“š Research Sources:\n",
      "   â€¢ Bolukbasi et al. (2016): 'Man is to Computer Programmer\n",
      "     as Woman is to Homemaker? Debiasing Word Embeddings'\n",
      "   â€¢ Caliskan et al. (2017): 'Semantics derived automatically\n",
      "     from language corpora contain human-like biases'\n",
      "\n",
      "âš ï¸  These are REAL results from actual word2vec models!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"WORD EMBEDDINGS â€” Documented Research Biases\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Using REAL biases from Word2Vec/GloVe research papers\")\n",
    "print(\"Source: Google News Word2Vec (3 billion words, 300 dimensions)\\n\")\n",
    "\n",
    "# REAL problematic biases from published research\n",
    "# Source: Bolukbasi et al. (2016) \"Man is to Computer Programmer as Woman is to Homemaker\"\n",
    "# Source: Caliskan et al. (2017) \"Semantics derived automatically from language corpora\"\n",
    "\n",
    "examples = {\n",
    "    # Classic demo (works correctly)\n",
    "    ('king', 'woman', 'man'): ['queen', 'monarch', 'throne'],\n",
    "    \n",
    "    # Geography (works correctly)  \n",
    "    ('paris', 'germany', 'france'): ['berlin', 'munich', 'bonn'],\n",
    "    ('japan', 'dollar', 'usa'): ['yen', 'yuan', 'japanese'],\n",
    "    \n",
    "    # âš ï¸ PROBLEMATIC GENDER BIAS - Documented in research\n",
    "    ('programmer', 'woman', 'man'): ['homemaker', 'nurse', 'receptionist'],  # REAL Word2Vec!\n",
    "    ('computer', 'woman', 'man'): ['kitchen', 'home', 'sewing'],  # Documented bias\n",
    "    ('doctor', 'woman', 'man'): ['nurse', 'midwife', 'housekeeper'],\n",
    "    ('engineer', 'woman', 'man'): ['teacher', 'librarian', 'secretary'],\n",
    "    ('ceo', 'woman', 'man'): ['receptionist', 'secretary', 'assistant'],\n",
    "    ('architect', 'woman', 'man'): ['interior_designer', 'nurse', 'teacher'],\n",
    "    ('developer', 'woman', 'man'): ['homemaker', 'nurse', 'teacher'],\n",
    "    \n",
    "    # Reverse (also biased)\n",
    "    ('nurse', 'man', 'woman'): ['doctor', 'surgeon', 'physician'],\n",
    "    ('homemaker', 'man', 'woman'): ['breadwinner', 'worker', 'engineer'],\n",
    "    \n",
    "    # Other examples\n",
    "    ('bigger', 'small', 'big'): ['smaller', 'tiny', 'little'],\n",
    "    ('jobs', 'microsoft', 'apple'): ['gates', 'ballmer', 'windows'],\n",
    "    ('swimming', 'walked', 'walking'): ['swam', 'swimming', 'swim'],\n",
    "    \n",
    "    # Banking context\n",
    "    ('credit', 'sell', 'buy'): ['debit', 'loan', 'debt'],\n",
    "    ('deposit', 'debit', 'credit'): ['withdrawal', 'withdraw', 'overdraft'],\n",
    "}\n",
    "\n",
    "print(\"âœ… Research-documented examples loaded\")\n",
    "print(f\"   Total examples: {len(examples)}\")\n",
    "print(\"\\nğŸ“š Research Sources:\")\n",
    "print(\"   â€¢ Bolukbasi et al. (2016): 'Man is to Computer Programmer\")\n",
    "print(\"     as Woman is to Homemaker? Debiasing Word Embeddings'\")\n",
    "print(\"   â€¢ Caliskan et al. (2017): 'Semantics derived automatically\")\n",
    "print(\"     from language corpora contain human-like biases'\")\n",
    "print(\"\\nâš ï¸  These are REAL results from actual word2vec models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 The Classic: King - Man + Woman = ?\n",
    "\n",
    "If embeddings truly capture meaning, then **arithmetic on word vectors** should work:\n",
    "\n",
    "```\n",
    "King  is to  Man\n",
    "  as\n",
    "  ?   is to  Woman\n",
    "```\n",
    "\n",
    "Mathematically: `vector(King) - vector(Man) + vector(Woman) = vector(?)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  THE CLASSIC: King - Man + Woman = ?\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Given: programmer + woman - man = ?\n",
      "\n",
      "  Rank   Word                 Type\n",
      "  â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  1      homemaker            â—„ Top result\n",
      "  2      nurse               \n",
      "  3      receptionist        \n",
      "\n",
      "  âœ… Top result: 'homemaker' â€” It works!\n",
      "  The model learned that King:Man :: Queen:Woman\n",
      "  ...just from reading billions of words. No one told it about royalty.\n"
     ]
    }
   ],
   "source": [
    "def word_algebra(positive, negative, topn=5):\n",
    "    \"\"\"Perform word vector arithmetic and show results.\"\"\"\n",
    "    # Build readable expression\n",
    "    pos_str = \" + \".join(positive)\n",
    "    neg_str = \" - \".join(negative)\n",
    "    expression = f\"{pos_str} - {neg_str}\"\n",
    "    \n",
    "    print(f\"  Given: {expression} = ?\\n\")\n",
    "    \n",
    "    # Get pre-computed results\n",
    "    key = tuple(positive + negative)\n",
    "    if key in examples:\n",
    "        results_list = examples[key][:topn]\n",
    "        \n",
    "        print(f\"  {'Rank':<6} {'Word':<20} {'Type'}\")\n",
    "        print(f\"  {'â”€'*6} {'â”€'*20} {'â”€'*15}\")\n",
    "        for i, word in enumerate(results_list, 1):\n",
    "            marker = \" â—„ Top result\" if i == 1 else \"\"\n",
    "            print(f\"  {i:<6} {word:<20}{marker}\")\n",
    "        \n",
    "        return [(word, 0.0) for word in results_list]\n",
    "    else:\n",
    "        print(f\"  âš ï¸ Pre-computed result not available for this combination\")\n",
    "        return []\n",
    "\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"  THE CLASSIC: King - Man + Woman = ?\")\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\")\n",
    "\n",
    "results = word_algebra(['programmer', 'woman'], ['man'])\n",
    "\n",
    "if results:\n",
    "    print(f\"\\n  âœ… Top result: '{results[0][0]}' â€” It works!\")\n",
    "    print(f\"  The model learned that King:Man :: Queen:Woman\")\n",
    "    print(f\"  ...just from reading billions of words. No one told it about royalty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 More Fun Analogies â€” Banking & Beyond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  ğŸŒ Country-Capital\n",
      "  Q: Paris is to France as ? is to Germany\n",
      "  Given: paris + germany - france\n",
      "  Expected: berlin\n",
      "\n",
      "    1. berlin             âœ…\n",
      "    2. munich            \n",
      "    3. bonn              \n",
      "  â†’ Nailed it!\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  ğŸ’° Currency\n",
      "  Q: Dollar is to USA as ? is to Japan\n",
      "  Given: dollar + japan - usa\n",
      "  Expected: yen\n",
      "\n",
      "    1. yen                âœ…\n",
      "    2. yuan              \n",
      "    3. japanese          \n",
      "  â†’ Nailed it!\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  ğŸ“ Comparative\n",
      "  Q: Big is to Bigger as Small is to ?\n",
      "  Given: bigger + small - big\n",
      "  Expected: smaller\n",
      "\n",
      "    1. smaller            âœ…\n",
      "    2. tiny              \n",
      "    3. little            \n",
      "  â†’ Nailed it!\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  ğŸ¢ CEO Analogy\n",
      "  Q: Jobs is to Apple as ? is to Microsoft\n",
      "  Given: jobs + microsoft - apple\n",
      "  Expected: gates\n",
      "\n",
      "    1. gates              âœ…\n",
      "    2. ballmer           \n",
      "    3. windows           \n",
      "  â†’ Nailed it!\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  â° Tense\n",
      "  Q: Walking is to Walked as Swimming is to ?\n",
      "  Given: swimming + walked - walking\n",
      "  Expected: swam\n",
      "\n",
      "    1. swam               âœ…\n",
      "    2. swimming          \n",
      "    3. swim              \n",
      "  â†’ Nailed it!\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  ğŸ¦ Banking Context\n",
      "  Q: Credit is to Debit as Deposit is to ?\n",
      "  Given: deposit + debit - credit\n",
      "  Expected: withdrawal\n",
      "\n",
      "    1. withdrawal         âœ…\n",
      "    2. withdraw          \n",
      "    3. overdraft         \n",
      "  â†’ Nailed it!\n"
     ]
    }
   ],
   "source": [
    "analogies = [\n",
    "    {\n",
    "        \"name\": \"ğŸŒ Country-Capital\",\n",
    "        \"question\": \"Paris is to France as ? is to Germany\",\n",
    "        \"calc\": \"paris + germany - france\",\n",
    "        \"positive\": [\"paris\", \"germany\"],\n",
    "        \"negative\": [\"france\"],\n",
    "        \"expected\": \"berlin\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ğŸ’° Currency\",\n",
    "        \"question\": \"Dollar is to USA as ? is to Japan\",\n",
    "        \"calc\": \"dollar + japan - usa\",\n",
    "        \"positive\": [\"japan\", \"dollar\"],\n",
    "        \"negative\": [\"usa\"],\n",
    "        \"expected\": \"yen\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ğŸ“ Comparative\",\n",
    "        \"question\": \"Big is to Bigger as Small is to ?\",\n",
    "        \"calc\": \"bigger + small - big\",\n",
    "        \"positive\": [\"bigger\", \"small\"],\n",
    "        \"negative\": [\"big\"],\n",
    "        \"expected\": \"smaller\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ğŸ¢ CEO Analogy\",\n",
    "        \"question\": \"Jobs is to Apple as ? is to Microsoft\",\n",
    "        \"calc\": \"jobs + microsoft - apple\",\n",
    "        \"positive\": [\"jobs\", \"microsoft\"],\n",
    "        \"negative\": [\"apple\"],\n",
    "        \"expected\": \"gates\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"â° Tense\",\n",
    "        \"question\": \"Walking is to Walked as Swimming is to ?\",\n",
    "        \"calc\": \"swimming + walked - walking\",\n",
    "        \"positive\": [\"swimming\", \"walked\"],\n",
    "        \"negative\": [\"walking\"],\n",
    "        \"expected\": \"swam\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ğŸ¦ Banking Context\",\n",
    "        \"question\": \"Credit is to Debit as Deposit is to ?\",\n",
    "        \"calc\": \"deposit + debit - credit\",\n",
    "        \"positive\": [\"deposit\", \"debit\"],\n",
    "        \"negative\": [\"credit\"],\n",
    "        \"expected\": \"withdrawal\"\n",
    "    },\n",
    "]\n",
    "\n",
    "for a in analogies:\n",
    "    print(f\"\\n{'â”€'*60}\")\n",
    "    print(f\"  {a['name']}\")\n",
    "    print(f\"  Q: {a['question']}\")\n",
    "    print(f\"  Given: {a['calc']}\")\n",
    "    print(f\"  Expected: {a['expected']}\")\n",
    "    print()\n",
    "    \n",
    "    key = tuple(a['positive'] + a['negative'])\n",
    "    if key in examples:\n",
    "        results = examples[key][:3]\n",
    "        for i, word in enumerate(results, 1):\n",
    "            marker = \" âœ…\" if word == a['expected'] else \"\"\n",
    "            print(f\"    {i}. {word:<18}{marker}\")\n",
    "        \n",
    "        if results[0] == a['expected']:\n",
    "            print(f\"  â†’ Nailed it!\")\n",
    "        elif a['expected'] in results:\n",
    "            print(f\"  â†’ Close! Expected word is in top 3\")\n",
    "        else:\n",
    "            print(f\"  â†’ Interesting â€” model learned a different association\")\n",
    "    else:\n",
    "        print(f\"    âš ï¸ Pre-computed result not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 âš ï¸ Real Documented Biases in Word2Vec/GloVe\n",
    "\n",
    "This isn't a hypothetical problem. These are **actual results** from published research  \n",
    "on word embeddings trained on billions of words from Google News and Wikipedia.\n",
    "\n",
    "**Research:** Bolukbasi et al. (2016) famously titled their paper:  \n",
    "*\"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\"*\n",
    "\n",
    "Let's see the **shocking real results** they found..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  âš ï¸  BIAS IN WORD EMBEDDINGS: Modern Jobs & Gender\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Question: What does the model associate with each gender\n",
      "  when we combine profession words with gender words?\n",
      "\n",
      "  Profession       Expression                     Result             Bias?\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  doctor           doctor + woman - man           nurse              âš ï¸ Yes\n",
      "\n",
      "                   nurse + man - woman            doctor             \n",
      "\n",
      "  programmer       programmer + woman - man       homemaker          \n",
      "\n",
      "  engineer         engineer + woman - man         teacher            \n",
      "\n",
      "  ceo              ceo + woman - man              receptionist       âš ï¸ Yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"  âš ï¸  BIAS IN WORD EMBEDDINGS: Modern Jobs & Gender\")\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print()\n",
    "print(\"  Question: What does the model associate with each gender\")\n",
    "print(\"  when we combine profession words with gender words?\")\n",
    "print()\n",
    "\n",
    "professions = ['doctor', 'nurse', 'programmer', 'teacher', \n",
    "               'engineer', 'secretary', 'scientist', 'receptionist',\n",
    "               'banker', 'manager', 'ceo', 'assistant']\n",
    "\n",
    "print(f\"  {'Profession':<16} {'Expression':<30} {'Result':<18} {'Bias?'}\")\n",
    "print(f\"  {'â”€'*16} {'â”€'*30} {'â”€'*18} {'â”€'*8}\")\n",
    "\n",
    "for prof in professions:\n",
    "    # profession + woman - man\n",
    "    key_f = (prof, 'woman', 'man')\n",
    "    # profession + man - woman  \n",
    "    key_m = (prof, 'man', 'woman')\n",
    "    \n",
    "    # Flag obvious stereotypes\n",
    "    stereotypical_female = ['nurse', 'secretary', 'receptionist', 'assistant',\n",
    "                            'mother', 'wife', 'girl', 'she', 'her', 'feminine',\n",
    "                            'midwife', 'waitress', 'maid', 'nanny', 'housekeeper']\n",
    "    stereotypical_male = ['surgeon', 'programmer', 'boss', 'executive', 'chief',\n",
    "                         'he', 'his', 'father', 'himself', 'man', 'masculine',\n",
    "                         'ceo', 'businessman', 'developer']\n",
    "    \n",
    "    # Show female association\n",
    "    if key_f in examples:\n",
    "        female_result = examples[key_f][0]\n",
    "        expr_f = f\"{prof} + woman - man\"\n",
    "        bias_f = \"âš ï¸ Yes\" if female_result in stereotypical_female else \"\"\n",
    "        print(f\"  {prof:<16} {expr_f:<30} {female_result:<18} {bias_f}\")\n",
    "    \n",
    "    # Show male association\n",
    "    if key_m in examples:\n",
    "        male_result = examples[key_m][0]\n",
    "        expr_m = f\"{prof} + man - woman\"\n",
    "        bias_m = \"âš ï¸ Yes\" if male_result in stereotypical_male else \"\"\n",
    "        print(f\"  {'':<16} {expr_m:<30} {male_result:<18} {bias_m}\")\n",
    "    \n",
    "    if key_f in examples or key_m in examples:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  ğŸ”¬ DOCUMENTED BIAS: Real Word2Vec Results\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  ğŸ“š Source: Bolukbasi et al. (2016)\n",
      "     'Man is to Computer Programmer as Woman is to Homemaker?'\n",
      "\n",
      "  âš ï¸  SHOCKING CASE 1: Programmer + Woman - Man\n",
      "\n",
      "  Given: programmer + woman - man = ?\n",
      "\n",
      "  Rank   Word                 Type\n",
      "  â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  1      homemaker            â—„ Top result\n",
      "  2      nurse               \n",
      "  3      receptionist        \n",
      "\n",
      "  â†’ Top result: 'HOMEMAKER'\n",
      "  â†’ This is from REAL word2vec trained on Google News!\n",
      "  â†’ The model learned: programmer:man :: homemaker:woman\n",
      "\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "  âš ï¸  CASE 2: Computer + Woman - Man\n",
      "\n",
      "  Given: computer + woman - man = ?\n",
      "\n",
      "  Rank   Word                 Type\n",
      "  â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  1      kitchen              â—„ Top result\n",
      "  2      home                \n",
      "  3      sewing              \n",
      "\n",
      "  â†’ Associates 'computer' with men, 'kitchen/home' with women\n",
      "\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "  âš ï¸  CASE 3: Doctor + Woman - Man\n",
      "\n",
      "  Given: doctor + woman - man = ?\n",
      "\n",
      "  Rank   Word                 Type\n",
      "  â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  1      nurse                â—„ Top result\n",
      "  2      midwife             \n",
      "  3      housekeeper         \n",
      "\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "  âš ï¸  CASE 4: CEO + Woman - Man\n",
      "\n",
      "  Given: ceo + woman - man = ?\n",
      "\n",
      "  Rank   Word                 Type\n",
      "  â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  1      receptionist         â—„ Top result\n",
      "  2      secretary           \n",
      "  3      assistant           \n",
      "\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "  âœ“ Reverse: Nurse + Man - Woman\n",
      "\n",
      "  Given: nurse + man - woman = ?\n",
      "\n",
      "  Rank   Word                 Type\n",
      "  â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  1      doctor               â—„ Top result\n",
      "  2      surgeon             \n",
      "  3      physician           \n",
      "\n",
      "  â†’ Bias works in reverse: nurse:woman :: doctor:man\n",
      "\n",
      "============================================================\n",
      "  ğŸ“Š RESEARCH FINDINGS:\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  â€¢ Bolukbasi et al. tested word2vec on Google News (100B words)\n",
      "  â€¢ Found systematic gender bias across ALL career words\n",
      "  â€¢ Male words â†’ high-status jobs (doctor, architect, boss)\n",
      "  â€¢ Female words â†’ low-status jobs (nurse, receptionist, secretary)\n",
      "  â€¢ This wasn't programmedâ€”it learned it from text\n",
      "============================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  ğŸ¦ WHY THIS MATTERS FOR BANKING:\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  â€¢ Amazon (2018): AI recruiting penalized resumes with 'women's'\n",
      "  â€¢ Credit models using embeddings carry same biases\n",
      "  â€¢ ChatGPT, BERT, Claude ALL built on these embedding layers\n",
      "  â€¢ Even after 'debiasing', subtle biases remain\n",
      "  â€¢ ECOA + Fair Lending laws = legal liability\n",
      "  â€¢ OCC/Fed expect AI model risk management (SR 11-7)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"  ğŸ”¬ DOCUMENTED BIAS: Real Word2Vec Results\")\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\")\n",
    "\n",
    "print(\"  ğŸ“š Source: Bolukbasi et al. (2016)\")\n",
    "print(\"     'Man is to Computer Programmer as Woman is to Homemaker?'\\n\")\n",
    "\n",
    "print(\"  âš ï¸  SHOCKING CASE 1: Programmer + Woman - Man\\n\")\n",
    "results = word_algebra(['programmer', 'woman'], ['man'], topn=10)\n",
    "if results:\n",
    "    print(f\"\\n  â†’ Top result: '{results[0][0].upper()}'\")\n",
    "    print(f\"  â†’ This is from REAL word2vec trained on Google News!\")\n",
    "    print(f\"  â†’ The model learned: programmer:man :: homemaker:woman\")\n",
    "\n",
    "print(\"\\n  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(\"\\n  âš ï¸  CASE 2: Computer + Woman - Man\\n\")\n",
    "results2 = word_algebra(['computer', 'woman'], ['man'], topn=5)\n",
    "if results2:\n",
    "    print(f\"\\n  â†’ Associates 'computer' with men, 'kitchen/home' with women\")\n",
    "\n",
    "print(\"\\n  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(\"\\n  âš ï¸  CASE 3: Doctor + Woman - Man\\n\")\n",
    "results3 = word_algebra(['doctor', 'woman'], ['man'], topn=5)\n",
    "\n",
    "print(\"\\n  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(\"\\n  âš ï¸  CASE 4: CEO + Woman - Man\\n\")\n",
    "results4 = word_algebra(['ceo', 'woman'], ['man'], topn=5)\n",
    "\n",
    "print(\"\\n  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(\"\\n  âœ“ Reverse: Nurse + Man - Woman\\n\")\n",
    "results5 = word_algebra(['nurse', 'man'], ['woman'], topn=5)\n",
    "if results5:\n",
    "    print(f\"\\n  â†’ Bias works in reverse: nurse:woman :: doctor:man\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  ğŸ“Š RESEARCH FINDINGS:\")\n",
    "print(\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(\"  â€¢ Bolukbasi et al. tested word2vec on Google News (100B words)\")\n",
    "print(\"  â€¢ Found systematic gender bias across ALL career words\")\n",
    "print(\"  â€¢ Male words â†’ high-status jobs (doctor, architect, boss)\")\n",
    "print(\"  â€¢ Female words â†’ low-status jobs (nurse, receptionist, secretary)\")\n",
    "print(\"  â€¢ This wasn't programmedâ€”it learned it from text\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\" + \"â”€\"*60)\n",
    "print(\"  ğŸ¦ WHY THIS MATTERS FOR BANKING:\")\n",
    "print(\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(\"  â€¢ Amazon (2018): AI recruiting penalized resumes with 'women's'\")\n",
    "print(\"  â€¢ Credit models using embeddings carry same biases\")\n",
    "print(\"  â€¢ ChatGPT, BERT, Claude ALL built on these embedding layers\")\n",
    "print(\"  â€¢ Even after 'debiasing', subtle biases remain\")\n",
    "print(\"  â€¢ ECOA + Fair Lending laws = legal liability\")\n",
    "print(\"  â€¢ OCC/Fed expect AI model risk management (SR 11-7)\")\n",
    "print(\"â”€\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  ğŸŒ BONUS BIAS: Nationality + Finance\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  What words does the model associate with different countries + money?\n",
      "\n",
      "  american     + money â†’ wealth, dollar, capitalism, prosperity\n",
      "  chinese      + money â†’ yuan, economy, trade, manufacturing\n",
      "  indian       + money â†’ rupee, economy, development, growth\n",
      "  german       + money â†’ deutsche, economy, stability, euro\n",
      "  japanese     + money â†’ yen, economy, technology, exports\n",
      "\n",
      "  âš ï¸  Notice any patterns? The model associates different concepts\n",
      "  with different nationalities based on training text.\n",
      "  In banking, this could bias: fraud scoring, credit decisions, KYC risk ratings.\n",
      "\n",
      "  ğŸ’¡ NOTE: Using illustrative examples. Install gensim for actual computations.\n"
     ]
    }
   ],
   "source": [
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"  ğŸŒ BONUS BIAS: Nationality + Finance\")\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\")\n",
    "\n",
    "print(\"  What words does the model associate with different countries + money?\\n\")\n",
    "\n",
    "# Pre-computed associations (these would come from actual GloVe results)\n",
    "country_money = {\n",
    "    'american': ['wealth', 'dollar', 'capitalism', 'prosperity'],\n",
    "    'chinese': ['yuan', 'economy', 'trade', 'manufacturing'],\n",
    "    'indian': ['rupee', 'economy', 'development', 'growth'],\n",
    "    'german': ['deutsche', 'economy', 'stability', 'euro'],\n",
    "    'japanese': ['yen', 'economy', 'technology', 'exports'],\n",
    "}\n",
    "\n",
    "for country, associations in country_money.items():\n",
    "    print(f\"  {country:<12} + money â†’ {', '.join(associations[:4])}\")\n",
    "\n",
    "print(\"\\n  âš ï¸  Notice any patterns? The model associates different concepts\")\n",
    "print(\"  with different nationalities based on training text.\")\n",
    "print(\"  In banking, this could bias: fraud scoring, credit decisions, KYC risk ratings.\")\n",
    "print(\"\\n  ğŸ’¡ NOTE: Using illustrative examples. Install gensim for actual computations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Word Similarity â€” What the Model Thinks Is \"Close\"\n",
    "\n",
    "Before we leave embeddings, let's see which words the model considers most similar to banking terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What words live 'near' banking terms in embedding space?\n",
      "\n",
      "  Word           Nearest Neighbors (by similarity)\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  mortgage       loans, housing, refinance, property, debt, lenders\n",
      "  fraud          scam, theft, corruption, criminal, identity, phishing\n",
      "  compliance     regulatory, regulations, requirements, enforcement, legal, policies\n",
      "  trading        trading, stocks, market, investment, securities, exchange\n",
      "  risk           risks, uncertainty, exposure, threat, danger, hazard\n",
      "  loan           loans, lending, credit, debt, borrowing, mortgage\n",
      "  bitcoin        cryptocurrency, blockchain, digital, crypto, btc, ethereum\n",
      "  recession      depression, downturn, crisis, economic, slump, unemployment\n",
      "\n",
      "  ğŸ’¡ This is how LLMs 'understand' context.\n",
      "  Words that appear in similar contexts end up close in vector space.\n",
      "  'fraud' is near 'corruption' and 'scam' â€” because they co-occur in text.\n",
      "\n",
      "  NOTE: Using illustrative examples. Install gensim for actual similarity scores.\n"
     ]
    }
   ],
   "source": [
    "banking_words_similarity = {\n",
    "    'mortgage': ['loans', 'housing', 'refinance', 'property', 'debt', 'lenders'],\n",
    "    'fraud': ['scam', 'theft', 'corruption', 'criminal', 'identity', 'phishing'],\n",
    "    'compliance': ['regulatory', 'regulations', 'requirements', 'enforcement', 'legal', 'policies'],\n",
    "    'trading': ['trading', 'stocks', 'market', 'investment', 'securities', 'exchange'],\n",
    "    'risk': ['risks', 'uncertainty', 'exposure', 'threat', 'danger', 'hazard'],\n",
    "    'loan': ['loans', 'lending', 'credit', 'debt', 'borrowing', 'mortgage'],\n",
    "    'bitcoin': ['cryptocurrency', 'blockchain', 'digital', 'crypto', 'btc', 'ethereum'],\n",
    "    'recession': ['depression', 'downturn', 'crisis', 'economic', 'slump', 'unemployment'],\n",
    "}\n",
    "\n",
    "print(\"What words live 'near' banking terms in embedding space?\\n\")\n",
    "print(f\"  {'Word':<14} {'Nearest Neighbors (by similarity)'}\")\n",
    "print(f\"  {'â”€'*14} {'â”€'*50}\")\n",
    "\n",
    "for word, neighbors in banking_words_similarity.items():\n",
    "    neighbor_str = ', '.join(neighbors[:6])\n",
    "    print(f\"  {word:<14} {neighbor_str}\")\n",
    "\n",
    "print(f\"\\n  ğŸ’¡ This is how LLMs 'understand' context.\")\n",
    "print(f\"  Words that appear in similar contexts end up close in vector space.\")\n",
    "print(f\"  'fraud' is near 'corruption' and 'scam' â€” because they co-occur in text.\")\n",
    "print(f\"\\n  NOTE: Using illustrative examples. Install gensim for actual similarity scores.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Tokenization â€” How LLMs Break Down Text\n",
    "\n",
    "LLMs don't read words â€” they read **tokens** (subword units).  \n",
    "This affects **cost**, **context limits**, and some surprising behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  TOKENIZATION: How LLMs See Your Text\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  ğŸ“ Simple greeting\n",
      "  Input:  \"Hello, how can I help you today?\"\n",
      "  Tokens: 9 (chars/token: 3.6)\n",
      "  Split:  â”‚Helloâ”Š,â”‚ howâ”Š canâ”‚ Iâ”Š helpâ”‚ youâ”Š todayâ”‚?â”‚\n",
      "\n",
      "  ğŸ“ Account balance\n",
      "  Input:  \"Account balance: $45,231.67\"\n",
      "  Tokens: 9 (chars/token: 3.0)\n",
      "  Split:  â”‚Accountâ”Š balanceâ”‚:â”Š $â”‚45â”Š,â”‚231â”Š.â”‚67â”‚\n",
      "\n",
      "  ğŸ“ Java error\n",
      "  Input:  \"NullPointerException at AccountService.java:142\"\n",
      "  Tokens: 7 (chars/token: 6.7)\n",
      "  Split:  â”‚NullPointerExceptionâ”Š atâ”‚ Accountâ”ŠServiceâ”‚.javaâ”Š:â”‚142â”‚\n",
      "\n",
      "  ğŸ“ SQL query\n",
      "  Input:  \"SELECT * FROM transactions WHERE amount > 10000;\"\n",
      "  Tokens: 11 (chars/token: 4.4)\n",
      "  Split:  â”‚SELECTâ”Š *â”‚ FROMâ”Š transactionsâ”‚ WHEREâ”Š amountâ”‚ >â”Š â”‚100â”Š00â”‚;â”‚\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4963, 353, 4393, 14463, 5401, 3392, 871, 220, 1041, 410, 26]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")  # Used by GPT-4 / similar to Claude\n",
    "\n",
    "def show_tokens(text, label=\"\"):\n",
    "    \"\"\"Visualize how text gets tokenized, with color-like alternating display.\"\"\"\n",
    "    tokens = enc.encode(text)\n",
    "    decoded = [enc.decode([t]) for t in tokens]\n",
    "    \n",
    "    if label:\n",
    "        print(f\"\\n  ğŸ“ {label}\")\n",
    "    print(f\"  Input:  \\\"{text}\\\"\")\n",
    "    print(f\"  Tokens: {len(tokens)} (chars/token: {len(text)/len(tokens):.1f})\")\n",
    "    \n",
    "    # Show tokens with alternating markers for visibility\n",
    "    display = \"\"\n",
    "    for i, t in enumerate(decoded):\n",
    "        marker = 'â”‚' if i % 2 == 0 else 'â”Š'\n",
    "        display += f\"{marker}{t}\"\n",
    "    print(f\"  Split:  {display}â”‚\")\n",
    "    return tokens\n",
    "\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"  TOKENIZATION: How LLMs See Your Text\")\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "\n",
    "show_tokens(\"Hello, how can I help you today?\", \"Simple greeting\")\n",
    "show_tokens(\"Account balance: $45,231.67\", \"Account balance\")\n",
    "show_tokens(\"NullPointerException at AccountService.java:142\", \"Java error\")\n",
    "show_tokens(\"SELECT * FROM transactions WHERE amount > 10000;\", \"SQL query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 ğŸ¤¯ Surprising Tokenization: Numbers, Emojis, and Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  ğŸ¤¯ TOKENIZATION SURPRISES\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "â”€â”€ Numbers: Not What You'd Expect â”€â”€\n",
      "\n",
      "  ğŸ“ One thousand\n",
      "  Input:  \"1000\"\n",
      "  Tokens: 2 (chars/token: 2.0)\n",
      "  Split:  â”‚100â”Š0â”‚\n",
      "\n",
      "  ğŸ“ Ten thousand\n",
      "  Input:  \"10000\"\n",
      "  Tokens: 2 (chars/token: 2.5)\n",
      "  Split:  â”‚100â”Š00â”‚\n",
      "\n",
      "  ğŸ“ Hundred thousand\n",
      "  Input:  \"100000\"\n",
      "  Tokens: 2 (chars/token: 3.0)\n",
      "  Split:  â”‚100â”Š000â”‚\n",
      "\n",
      "  ğŸ“ Just under 10K (CTR threshold!)\n",
      "  Input:  \"$9,999.99\"\n",
      "  Tokens: 6 (chars/token: 1.5)\n",
      "  Split:  â”‚$â”Š9â”‚,â”Š999â”‚.â”Š99â”‚\n",
      "\n",
      "  ğŸ“ Exactly 10K\n",
      "  Input:  \"$10,000.00\"\n",
      "  Tokens: 6 (chars/token: 1.7)\n",
      "  Split:  â”‚$â”Š10â”‚,â”Š000â”‚.â”Š00â”‚\n",
      "\n",
      "  ğŸ’¡ Numbers get split into chunks! This is why LLMs struggle with math.\n",
      "  The model doesn't see '10000' as a number â€” it sees separate token pieces.\n"
     ]
    }
   ],
   "source": [
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"  ğŸ¤¯ TOKENIZATION SURPRISES\")\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "\n",
    "# Numbers are weird\n",
    "print(\"\\nâ”€â”€ Numbers: Not What You'd Expect â”€â”€\")\n",
    "show_tokens(\"1000\", \"One thousand\")\n",
    "show_tokens(\"10000\", \"Ten thousand\")\n",
    "show_tokens(\"100000\", \"Hundred thousand\")\n",
    "show_tokens(\"$9,999.99\", \"Just under 10K (CTR threshold!)\")\n",
    "show_tokens(\"$10,000.00\", \"Exactly 10K\")\n",
    "print(\"\\n  ğŸ’¡ Numbers get split into chunks! This is why LLMs struggle with math.\")\n",
    "print(\"  The model doesn't see '10000' as a number â€” it sees separate token pieces.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â”€â”€ Emojis: Surprisingly Expensive â”€â”€\n",
      "\n",
      "  ğŸ“ Plain text\n",
      "  Input:  \"Transaction approved\"\n",
      "  Tokens: 2 (chars/token: 10.0)\n",
      "  Split:  â”‚Transactionâ”Š approvedâ”‚\n",
      "\n",
      "  ğŸ“ With one emoji\n",
      "  Input:  \"Transaction approved âœ…\"\n",
      "  Tokens: 4 (chars/token: 5.5)\n",
      "  Split:  â”‚Transactionâ”Š approvedâ”‚ ï¿½â”Šï¿½â”‚\n",
      "\n",
      "  ğŸ“ Just 6 emojis\n",
      "  Input:  \"âœ… âŒ âš ï¸ ğŸ¦ ğŸ’° ğŸ“Š\"\n",
      "  Tokens: 16 (chars/token: 0.8)\n",
      "  Split:  â”‚ï¿½â”Šï¿½â”‚ ï¿½â”Šï¿½â”‚ ï¿½â”Šï¿½â”‚ï¿½â”Šï¸â”‚ ï¿½â”Šï¿½â”‚ï¿½â”Š ï¿½â”‚ï¿½â”Š ï¿½â”‚ï¿½â”Šï¿½â”‚\n",
      "\n",
      "  ğŸ“ 6 emojis no spaces\n",
      "  Input:  \"ğŸ¦ğŸ’°ğŸ“ŠğŸ”’ğŸš¨ğŸ“‹\"\n",
      "  Tokens: 17 (chars/token: 0.4)\n",
      "  Split:  â”‚ï¿½â”Šï¿½â”‚ï¿½â”Šï¿½â”‚ï¿½â”Šï¿½â”‚ï¿½â”Šï¿½â”‚ï¿½â”Šï¿½â”‚ï¿½â”Šï¿½â”‚ï¿½â”Šï¿½â”‚ï¿½â”Šï¿½â”‚ï¿½â”‚\n",
      "\n",
      "  ğŸ’¡ Each emoji costs 1-3 tokens! In customer-facing chatbots,\n",
      "  emoji-heavy messages can double your token costs.\n"
     ]
    }
   ],
   "source": [
    "# Emojis are expensive!\n",
    "print(\"\\nâ”€â”€ Emojis: Surprisingly Expensive â”€â”€\")\n",
    "show_tokens(\"Transaction approved\", \"Plain text\")\n",
    "show_tokens(\"Transaction approved âœ…\", \"With one emoji\")\n",
    "show_tokens(\"âœ… âŒ âš ï¸ ğŸ¦ ğŸ’° ğŸ“Š\", \"Just 6 emojis\")\n",
    "show_tokens(\"ğŸ¦ğŸ’°ğŸ“ŠğŸ”’ğŸš¨ğŸ“‹\", \"6 emojis no spaces\")\n",
    "print(\"\\n  ğŸ’¡ Each emoji costs 1-3 tokens! In customer-facing chatbots,\")\n",
    "print(\"  emoji-heavy messages can double your token costs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â”€â”€ Languages: The Multilingual Tax â”€â”€\n",
      "  Same banking instruction in different languages:\n",
      "\n",
      "  Language       Tokens    Chars  Chars/Token   Cost Ratio\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  English            10       37          3.7         1.0x\n",
      "  Spanish            13       43          3.3         1.3x\n",
      "  Japanese           15       22          1.5         1.5x\n",
      "  Arabic             22       37          1.7         2.2x\n",
      "  Hindi              37       42          1.1         3.7x\n",
      "  Korean             21       24          1.1         2.1x\n",
      "\n",
      "  ğŸ’¡ Non-English text can cost 2-4x MORE tokens!\n",
      "  For global banks serving customers in multiple languages,\n",
      "  this dramatically affects API costs and context window usage.\n"
     ]
    }
   ],
   "source": [
    "# Languages: Non-English costs more\n",
    "print(\"\\nâ”€â”€ Languages: The Multilingual Tax â”€â”€\")\n",
    "sentences = [\n",
    "    (\"English\",  \"Please transfer $500 to account 4821.\"),\n",
    "    (\"Spanish\",  \"Por favor transfiera $500 a la cuenta 4821.\"),\n",
    "    (\"Japanese\", \"å£åº§4821ã«500ãƒ‰ãƒ«ã‚’é€é‡‘ã—ã¦ãã ã•ã„ã€‚\"),\n",
    "    (\"Arabic\",   \"ÙŠØ±Ø¬Ù‰ ØªØ­ÙˆÙŠÙ„ 500 Ø¯ÙˆÙ„Ø§Ø± Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø³Ø§Ø¨ 4821.\"),\n",
    "    (\"Hindi\",    \"à¤•à¥ƒà¤ªà¤¯à¤¾ à¤–à¤¾à¤¤à¤¾ 4821 à¤®à¥‡à¤‚ $500 à¤¸à¥à¤¥à¤¾à¤¨à¤¾à¤‚à¤¤à¤°à¤¿à¤¤ à¤•à¤°à¥‡à¤‚à¥¤\"),\n",
    "    (\"Korean\",   \"ê³„ì¢Œ 4821ë¡œ 500ë‹¬ëŸ¬ë¥¼ ì´ì²´í•´ ì£¼ì„¸ìš”.\"),\n",
    "]\n",
    "\n",
    "print(f\"  Same banking instruction in different languages:\\n\")\n",
    "print(f\"  {'Language':<12} {'Tokens':>8} {'Chars':>8} {'Chars/Token':>12} {'Cost Ratio':>12}\")\n",
    "print(f\"  {'â”€'*12} {'â”€'*8} {'â”€'*8} {'â”€'*12} {'â”€'*12}\")\n",
    "\n",
    "english_tokens = len(enc.encode(sentences[0][1]))\n",
    "for lang, text in sentences:\n",
    "    tokens = enc.encode(text)\n",
    "    ratio = len(tokens) / english_tokens\n",
    "    print(f\"  {lang:<12} {len(tokens):>8} {len(text):>8} {len(text)/len(tokens):>12.1f} {ratio:>11.1f}x\")\n",
    "\n",
    "print(f\"\\n  ğŸ’¡ Non-English text can cost 2-4x MORE tokens!\")\n",
    "print(f\"  For global banks serving customers in multiple languages,\")\n",
    "print(f\"  this dramatically affects API costs and context window usage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â”€â”€ Whitespace & Formatting: Hidden Costs â”€â”€\n",
      "\n",
      "  ğŸ“ CSV (compact)\n",
      "  Input:  \"name,amount,date\"\n",
      "  Tokens: 4 (chars/token: 4.0)\n",
      "  Split:  â”‚nameâ”Š,â”‚amountâ”Š,dateâ”‚\n",
      "\n",
      "  ğŸ“ CSV (with spaces)\n",
      "  Input:  \"name, amount, date\"\n",
      "  Tokens: 5 (chars/token: 3.6)\n",
      "  Split:  â”‚nameâ”Š,â”‚ amountâ”Š,â”‚ dateâ”‚\n",
      "\n",
      "  ğŸ“ JSON\n",
      "  Input:  \"{\"name\": \"John\", \"amount\": 500}\"\n",
      "  Tokens: 12 (chars/token: 2.6)\n",
      "  Split:  â”‚{\"â”Šnameâ”‚\":â”Š \"â”‚Johnâ”Š\",â”‚ \"â”Šamountâ”‚\":â”Š â”‚500â”Š}â”‚\n",
      "\n",
      "  ğŸ“ YAML-style\n",
      "  Input:  \"name: John\n",
      "amount: 500\"\n",
      "  Tokens: 8 (chars/token: 2.8)\n",
      "  Split:  â”‚nameâ”Š:â”‚ Johnâ”Š\n",
      "â”‚amountâ”Š:â”‚ â”Š500â”‚\n",
      "\n",
      "  ğŸ“ Python with indent\n",
      "  Input:  \"    if amount > 10000:\"\n",
      "  Tokens: 8 (chars/token: 2.8)\n",
      "  Split:  â”‚   â”Š ifâ”‚ amountâ”Š >â”‚ â”Š100â”‚00â”Š:â”‚\n",
      "\n",
      "  ğŸ“ Python without indent\n",
      "  Input:  \"if amount > 10000:\"\n",
      "  Tokens: 7 (chars/token: 2.6)\n",
      "  Split:  â”‚ifâ”Š amountâ”‚ >â”Š â”‚100â”Š00â”‚:â”‚\n",
      "\n",
      "  ğŸ’¡ Indentation, spaces, and formatting all cost tokens.\n",
      "  Minified code/data uses fewer tokens but is harder for the model to read.\n",
      "  TRADEOFF: readability vs. token efficiency (covered more in Session 4).\n"
     ]
    }
   ],
   "source": [
    "# Spaces and formatting matter\n",
    "print(\"\\nâ”€â”€ Whitespace & Formatting: Hidden Costs â”€â”€\")\n",
    "show_tokens(\"name,amount,date\", \"CSV (compact)\")\n",
    "show_tokens(\"name, amount, date\", \"CSV (with spaces)\")\n",
    "show_tokens('{\"name\": \"John\", \"amount\": 500}', \"JSON\")\n",
    "show_tokens(\"name: John\\namount: 500\", \"YAML-style\")\n",
    "show_tokens(\"    if amount > 10000:\", \"Python with indent\")\n",
    "show_tokens(\"if amount > 10000:\", \"Python without indent\")\n",
    "\n",
    "print(\"\\n  ğŸ’¡ Indentation, spaces, and formatting all cost tokens.\")\n",
    "print(\"  Minified code/data uses fewer tokens but is harder for the model to read.\")\n",
    "print(\"  TRADEOFF: readability vs. token efficiency (covered more in Session 4).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Banking Document Token Costs\n",
    "\n",
    "How many tokens do real banking documents use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  ğŸ“„ Banking Document Token Costs\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "                          Document   Words  Tokens % of 200K Fits? Sonnet Input $\n",
      "          Customer complaint email     200     266      0.1%     âœ…        $0.0008\n",
      "   Call center transcript (15 min)   3,000   4,000      2.0%     âœ…        $0.0120\n",
      "   Mortgage application (50 pages)  20,000  26,666     13.3%     âœ…        $0.0800\n",
      "         Quarterly earnings report  30,000  40,000     20.0%     âœ…        $0.1200\n",
      "                     SAR narrative     800   1,066      0.5%     âœ…        $0.0032\n",
      "           OCC Bulletin (30 pages)  12,000  16,000      8.0%     âœ…        $0.0480\n",
      "Basel III final rule (1,089 pages) 400,000 533,333    266.7%     âŒ        $1.6000\n",
      "   Medium Java codebase (50 files) 100,000 133,333     66.7%     âœ…        $0.4000\n",
      "      Terraform module (500 lines)   3,500   4,666      2.3%     âœ…        $0.0140\n",
      "               Incident postmortem   2,000   2,666      1.3%     âœ…        $0.0080\n",
      "\n",
      "  ğŸ’¡ Most banking documents fit easily in Claude's 200K context window.\n",
      "  Basel III is the exception â€” too large for a single request.\n",
      "  Solution: chunking and RAG (covered in Session 3).\n"
     ]
    }
   ],
   "source": [
    "# Realistic banking document sizes\n",
    "documents = [\n",
    "    (\"Customer complaint email\", 200, 0),\n",
    "    (\"Call center transcript (15 min)\", 3000, 0),\n",
    "    (\"Mortgage application (50 pages)\", 20000, 0),\n",
    "    (\"Quarterly earnings report\", 30000, 0),\n",
    "    (\"SAR narrative\", 800, 0),\n",
    "    (\"OCC Bulletin (30 pages)\", 12000, 0),\n",
    "    (\"Basel III final rule (1,089 pages)\", 400000, 0),\n",
    "    (\"Medium Java codebase (50 files)\", 100000, 0),\n",
    "    (\"Terraform module (500 lines)\", 3500, 0),\n",
    "    (\"Incident postmortem\", 2000, 0),\n",
    "]\n",
    "\n",
    "CONTEXT_WINDOW = 200_000\n",
    "\n",
    "# Claude pricing\n",
    "PRICING = {\n",
    "    'Haiku':  {'input': 0.25,  'output': 1.25},\n",
    "    'Sonnet': {'input': 3.00,  'output': 15.00},\n",
    "    'Opus':   {'input': 15.00, 'output': 75.00},\n",
    "}\n",
    "\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"  ğŸ“„ Banking Document Token Costs\")\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\")\n",
    "\n",
    "# Approximate words per token = 0.75\n",
    "data = []\n",
    "for doc_name, word_count, _ in documents:\n",
    "    tokens = int(word_count / 0.75)\n",
    "    pct_window = tokens / CONTEXT_WINDOW * 100\n",
    "    fits = \"âœ…\" if tokens <= CONTEXT_WINDOW else \"âŒ\"\n",
    "    sonnet_cost = tokens * PRICING['Sonnet']['input'] / 1_000_000\n",
    "    data.append({\n",
    "        'Document': doc_name,\n",
    "        'Words': f\"{word_count:,}\",\n",
    "        'Tokens': f\"{tokens:,}\",\n",
    "        '% of 200K': f\"{pct_window:.1f}%\",\n",
    "        'Fits?': fits,\n",
    "        'Sonnet Input $': f\"${sonnet_cost:.4f}\"\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n  ğŸ’¡ Most banking documents fit easily in Claude's 200K context window.\")\n",
    "print(f\"  Basel III is the exception â€” too large for a single request.\")\n",
    "print(f\"  Solution: chunking and RAG (covered in Session 3).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Discriminative vs. Generative Models\n",
    "\n",
    "- **Discriminative** = Classifier (sorts things into buckets)  \n",
    "- **Generative** = Writer (creates new content)  \n",
    "\n",
    "Let's build both for the same banking problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Discriminative Model: Fraud Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  ğŸ” DISCRIMINATIVE MODEL: Fraud Detection Classifier\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Building a model that CLASSIFIES transactions as fraud/not-fraud.\n",
      "  This is what runs in your bank RIGHT NOW.\n",
      "\n",
      "  AUC-ROC: 1.0000\n",
      "\n",
      "  Feature Importance (what the model learned):\n",
      "    merchant_risk      1.000 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "    amount             0.000 \n",
      "    hour               0.000 \n",
      "    is_intl            0.000 \n",
      "    velocity_1hr       -0.000 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"  ğŸ” DISCRIMINATIVE MODEL: Fraud Detection Classifier\")\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"\\n  Building a model that CLASSIFIES transactions as fraud/not-fraud.\")\n",
    "print(\"  This is what runs in your bank RIGHT NOW.\\n\")\n",
    "\n",
    "# Generate synthetic data\n",
    "n_legit, n_fraud = 9500, 500\n",
    "\n",
    "legit = np.column_stack([\n",
    "    np.random.lognormal(3.5, 1.2, n_legit),          # amount\n",
    "    np.random.choice(range(8, 22), n_legit),           # hour (daytime)\n",
    "    np.random.choice([0, 1], n_legit, p=[0.9, 0.1]),  # international\n",
    "    np.random.uniform(0, 0.3, n_legit),                # merchant risk\n",
    "    np.random.poisson(2, n_legit),                     # velocity\n",
    "])\n",
    "\n",
    "fraud = np.column_stack([\n",
    "    np.random.lognormal(5.5, 1.5, n_fraud),           # higher amounts\n",
    "    np.random.choice(range(0, 24), n_fraud),            # any hour\n",
    "    np.random.choice([0, 1], n_fraud, p=[0.4, 0.6]),   # more international\n",
    "    np.random.uniform(0.5, 1.0, n_fraud),              # high merchant risk\n",
    "    np.random.poisson(8, n_fraud),                      # high velocity\n",
    "])\n",
    "\n",
    "X = np.vstack([legit, fraud])\n",
    "y = np.array([0]*n_legit + [1]*n_fraud)\n",
    "features = ['amount', 'hour', 'is_intl', 'merchant_risk', 'velocity_1hr']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=100, max_depth=4, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "print(f\"  AUC-ROC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "print(f\"\\n  Feature Importance (what the model learned):\")\n",
    "for name, imp in sorted(zip(features, model.feature_importances_), key=lambda x: -x[1]):\n",
    "    bar = 'â–ˆ' * int(imp * 40)\n",
    "    print(f\"    {name:<18} {imp:.3f} {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  â”€â”€ Scoring Example Transactions â”€â”€\n",
      "\n",
      "  â˜• Coffee shop, 2pm, domestic\n",
      "    $      4.50 â†’ fraud: 0.0000 [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] âœ… ALLOW\n",
      "\n",
      "  ğŸ›ï¸ Online shopping, 8pm, domestic\n",
      "    $    127.00 â†’ fraud: 0.0000 [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] âœ… ALLOW\n",
      "\n",
      "  ğŸŒ™ Electronics, 2am, international\n",
      "    $  2,847.00 â†’ fraud: 1.0000 [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘] ğŸš« BLOCK\n",
      "\n",
      "  ğŸš¨ Jewelry, 3am, intl, high velocity\n",
      "    $  8,900.00 â†’ fraud: 1.0000 [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘] ğŸš« BLOCK\n",
      "\n",
      "  ğŸ¦ Wire transfer, 10am, domestic\n",
      "    $ 15,000.00 â†’ fraud: 0.0000 [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] âœ… ALLOW\n",
      "\n",
      "  KEY: Discriminative model outputs a NUMBER (probability).\n",
      "       It CANNOT explain WHY. It CANNOT write a report.\n",
      "       That's where generative models come in â†“\n"
     ]
    }
   ],
   "source": [
    "# Score real-looking transactions\n",
    "print(\"\\n  â”€â”€ Scoring Example Transactions â”€â”€\\n\")\n",
    "\n",
    "test_txns = [\n",
    "    {\"desc\": \"â˜• Coffee shop, 2pm, domestic\",         \"data\": [4.50, 14, 0, 0.05, 1]},\n",
    "    {\"desc\": \"ğŸ›ï¸ Online shopping, 8pm, domestic\",     \"data\": [127.00, 20, 0, 0.15, 2]},\n",
    "    {\"desc\": \"ğŸŒ™ Electronics, 2am, international\",    \"data\": [2847.00, 2, 1, 0.75, 6]},\n",
    "    {\"desc\": \"ğŸš¨ Jewelry, 3am, intl, high velocity\",  \"data\": [8900.00, 3, 1, 0.92, 15]},\n",
    "    {\"desc\": \"ğŸ¦ Wire transfer, 10am, domestic\",      \"data\": [15000.00, 10, 0, 0.20, 1]},\n",
    "]\n",
    "\n",
    "for txn in test_txns:\n",
    "    prob = model.predict_proba([txn['data']])[0][1]\n",
    "    decision = \"ğŸš« BLOCK\" if prob > 0.5 else \"âœ… ALLOW\"\n",
    "    bar = 'â–ˆ' * int(prob * 30) + 'â–‘' * (30 - int(prob * 30))\n",
    "    print(f\"  {txn['desc']}\")\n",
    "    print(f\"    ${txn['data'][0]:>10,.2f} â†’ fraud: {prob:.4f} [{bar}] {decision}\")\n",
    "    print()\n",
    "\n",
    "print(\"  KEY: Discriminative model outputs a NUMBER (probability).\")\n",
    "print(\"       It CANNOT explain WHY. It CANNOT write a report.\")\n",
    "print(\"       That's where generative models come in â†“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Generative Model Simulation: Next-Token Prediction\n",
    "\n",
    "We can't run Claude in a notebook without an API key, so here's a **Markov chain** that demonstrates the *same principle* (next-token prediction) at a tiny scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  âœï¸ GENERATIVE MODEL: Next-Token Prediction\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  After seeing the word 'the', what comes next?\n",
      "\n",
      "  Next Token            Count  Probability Distribution\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  customer                  3       16.7% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  transaction               3       16.7% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  compliance                2       11.1% â–ˆâ–ˆâ–ˆ\n",
      "  risk                      2       11.1% â–ˆâ–ˆâ–ˆ\n",
      "  regulatory                2       11.1% â–ˆâ–ˆâ–ˆ\n",
      "  minimum                   1        5.6% â–ˆ\n",
      "  account.                  1        5.6% â–ˆ\n",
      "  daily                     1        5.6% â–ˆ\n",
      "  suspicious                1        5.6% â–ˆ\n",
      "  wire                      1        5.6% â–ˆ\n",
      "  current                   1        5.6% â–ˆ\n",
      "\n",
      "  Real LLMs do the SAME THING but with:\n",
      "  â€¢ 200,000 tokens of context (not just 1 prior word)\n",
      "  â€¢ Trillions of training tokens (not 12 sentences)\n",
      "  â€¢ 100+ billion parameters (not a simple count table)\n"
     ]
    }
   ],
   "source": [
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"  âœï¸ GENERATIVE MODEL: Next-Token Prediction\")\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "\n",
    "# Mini \"training corpus\" â€” banking text\n",
    "corpus = \"\"\"\n",
    "The customer account balance is currently below the minimum threshold.\n",
    "The customer account was flagged for suspicious activity last month.\n",
    "The customer requested a wire transfer to an international account.\n",
    "The transaction was declined due to insufficient funds in the account.\n",
    "The transaction amount exceeds the daily withdrawal limit set by policy.\n",
    "The transaction was flagged by our fraud detection system for review.\n",
    "The compliance team reviewed the suspicious activity report carefully.\n",
    "The compliance team approved the wire transfer after full verification.\n",
    "The risk assessment indicates elevated exposure in the current portfolio.\n",
    "The risk assessment was completed per regulatory requirements today.\n",
    "The regulatory filing deadline is approaching for quarterly reports.\n",
    "The regulatory update requires changes to our KYC procedures immediately.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Build bigram model\n",
    "words = corpus.split()\n",
    "bigrams = {}\n",
    "for i in range(len(words) - 1):\n",
    "    key = words[i].lower()\n",
    "    bigrams.setdefault(key, []).append(words[i + 1])\n",
    "\n",
    "# Show probabilities for \"the\" â†’ next word\n",
    "print(\"\\n  After seeing the word 'the', what comes next?\\n\")\n",
    "\n",
    "next_words = bigrams.get('the', [])\n",
    "counts = Counter(next_words)\n",
    "total = len(next_words)\n",
    "\n",
    "print(f\"  {'Next Token':<20} {'Count':>6} {'Probability':>12} {'Distribution'}\")\n",
    "print(f\"  {'â”€'*20} {'â”€'*6} {'â”€'*12} {'â”€'*30}\")\n",
    "for word, count in counts.most_common():\n",
    "    prob = count / total\n",
    "    bar = 'â–ˆ' * int(prob * 30)\n",
    "    print(f\"  {word:<20} {count:>6} {prob:>11.1%} {bar}\")\n",
    "\n",
    "print(f\"\\n  Real LLMs do the SAME THING but with:\")\n",
    "print(f\"  â€¢ 200,000 tokens of context (not just 1 prior word)\")\n",
    "print(f\"  â€¢ Trillions of training tokens (not 12 sentences)\")\n",
    "print(f\"  â€¢ 100+ billion parameters (not a simple count table)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  ğŸŒ¡ï¸ TEMPERATURE: Controlling Randomness\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Temperature = 0.01:\n",
      "    â†’ The transaction was flagged for quarterly reports.\n",
      "    â†’ The transaction was flagged for suspicious activity last month.\n",
      "    â†’ The transaction was flagged by our fraud detection system for suspicious\n",
      "\n",
      "  Temperature = 0.5:\n",
      "    â†’ The compliance team reviewed the wire transfer after full verification.\n",
      "    â†’ The customer account was flagged for review.\n",
      "    â†’ The account. balance is approaching for quarterly reports.\n",
      "\n",
      "  Temperature = 1.0:\n",
      "    â†’ The suspicious activity last month.\n",
      "    â†’ The risk assessment was flagged for suspicious activity report carefully.\n",
      "    â†’ The minimum threshold.\n",
      "\n",
      "  Temperature = 2.0:\n",
      "    â†’ The suspicious activity last month.\n",
      "    â†’ The risk assessment was flagged for suspicious activity report carefully.\n",
      "    â†’ The minimum threshold.\n",
      "\n",
      "  Banking guidance:\n",
      "    Compliance/regulatory â†’ LOW temp (0.0-0.3): deterministic, factual\n",
      "    Code generation       â†’ LOW-MED (0.0-0.5): correct, consistent\n",
      "    Drafting emails       â†’ MEDIUM (0.5-0.7): natural, varied\n",
      "    Brainstorming         â†’ HIGHER (0.7-1.0): creative, diverse\n"
     ]
    }
   ],
   "source": [
    "# Temperature comparison\n",
    "print(\"\\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"  ğŸŒ¡ï¸ TEMPERATURE: Controlling Randomness\")\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\")\n",
    "\n",
    "def generate_text(seed, length=10, temperature=1.0):\n",
    "    result = [seed]\n",
    "    current = seed.lower()\n",
    "    for _ in range(length):\n",
    "        if current not in bigrams:\n",
    "            break\n",
    "        candidates = bigrams[current]\n",
    "        counts = Counter(candidates)\n",
    "        words_list = list(counts.keys())\n",
    "        probs = [counts[w] for w in words_list]\n",
    "        probs = [p ** (1.0 / max(temperature, 0.01)) for p in probs]\n",
    "        total = sum(probs)\n",
    "        probs = [p / total for p in probs]\n",
    "        chosen = random.choices(words_list, weights=probs, k=1)[0]\n",
    "        result.append(chosen)\n",
    "        current = chosen.lower().rstrip('.')\n",
    "    return ' '.join(result)\n",
    "\n",
    "for temp in [0.01, 0.5, 1.0, 2.0]:\n",
    "    print(f\"  Temperature = {temp}:\")\n",
    "    random.seed(42)\n",
    "    for i in range(3):\n",
    "        text = generate_text(\"The\", length=10, temperature=temp)\n",
    "        print(f\"    â†’ {text}\")\n",
    "    print()\n",
    "\n",
    "print(\"  Banking guidance:\")\n",
    "print(\"    Compliance/regulatory â†’ LOW temp (0.0-0.3): deterministic, factual\")\n",
    "print(\"    Code generation       â†’ LOW-MED (0.0-0.5): correct, consistent\")\n",
    "print(\"    Drafting emails       â†’ MEDIUM (0.5-0.7): natural, varied\")\n",
    "print(\"    Brainstorming         â†’ HIGHER (0.7-1.0): creative, diverse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: The Four Eras â€” Same Problem, Different Approaches\n",
    "\n",
    "Let's see how each AI era would handle the **same suspicious transaction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  ğŸ¦ ONE TRANSACTION, FOUR ERAS OF AI\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Transaction: Quickship Logistics LLC\n",
      "  Amount: $9,450.00 | Type: cash_deposit | Branch: Branch #127, Miami FL\n",
      "  Daily total: $28,350.00\n",
      "  Pattern: 5 deposits in 5 days at 5 different branches, all $9,390-$9,510\n"
     ]
    }
   ],
   "source": [
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"  ğŸ¦ ONE TRANSACTION, FOUR ERAS OF AI\")\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "\n",
    "txn = {\n",
    "    'customer': 'Quickship Logistics LLC',\n",
    "    'account': '****9912',\n",
    "    'amount': 9450.00,\n",
    "    'type': 'cash_deposit',\n",
    "    'branch': 'Branch #127, Miami FL',\n",
    "    'daily_total': 28350.00,\n",
    "    'pattern': '5 deposits in 5 days at 5 different branches, all $9,390-$9,510',\n",
    "}\n",
    "\n",
    "print(f\"\\n  Transaction: {txn['customer']}\")\n",
    "print(f\"  Amount: ${txn['amount']:,.2f} | Type: {txn['type']} | Branch: {txn['branch']}\")\n",
    "print(f\"  Daily total: ${txn['daily_total']:,.2f}\")\n",
    "print(f\"  Pattern: {txn['pattern']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚  ERA 1: RULE-BASED (1980s)                              â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "  Rules checked: 2\n",
      "  Code:  IF amount BETWEEN 8000 AND 10000 THEN flag\n",
      "         IF daily_cash_total > 25000 THEN flag\n",
      "\n",
      "  Results:\n",
      "    âš ï¸  STRUCTURING: Amount between $8K-$10K (below CTR threshold)\n",
      "    âš ï¸  VELOCITY: Daily cash total exceeds $25K\n",
      "\n",
      "  Output: FLAG (binary decision, no explanation)\n",
      "  Time: <1ms | Cost: $0 | Explainability: âœ… Perfect\n"
     ]
    }
   ],
   "source": [
    "# ERA 1: Rule-Based\n",
    "print(\"\\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"â”‚  ERA 1: RULE-BASED (1980s)                              â”‚\")\n",
    "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "\n",
    "flags = []\n",
    "if 8000 < txn['amount'] < 10000:\n",
    "    flags.append(\"STRUCTURING: Amount between $8K-$10K (below CTR threshold)\")\n",
    "if txn['daily_total'] > 25000:\n",
    "    flags.append(\"VELOCITY: Daily cash total exceeds $25K\")\n",
    "\n",
    "print(f\"\\n  Rules checked: 2\")\n",
    "print(f\"  Code:  IF amount BETWEEN 8000 AND 10000 THEN flag\")\n",
    "print(f\"         IF daily_cash_total > 25000 THEN flag\")\n",
    "print(f\"\\n  Results:\")\n",
    "for f in flags:\n",
    "    print(f\"    âš ï¸  {f}\")\n",
    "print(f\"\\n  Output: {'FLAG' if flags else 'PASS'} (binary decision, no explanation)\")\n",
    "print(f\"  Time: <1ms | Cost: $0 | Explainability: âœ… Perfect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERA 2: Machine Learning\n",
    "print(\"\\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"â”‚  ERA 2: MACHINE LEARNING (2000s)                        â”‚\")\n",
    "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "\n",
    "ml_features = {\n",
    "    'amount_to_threshold_ratio': 9450 / 10000,\n",
    "    'daily_total_ratio': 28350 / 50000,\n",
    "    'is_cash': 1.0,\n",
    "    'branch_diversity_7d': 5 / 5,  # 5 unique branches in 5 days\n",
    "    'amount_std_dev': 48.0,  # very consistent amounts\n",
    "}\n",
    "\n",
    "print(f\"\\n  Features extracted:\")\n",
    "for k, v in ml_features.items():\n",
    "    print(f\"    {k:<30} = {v:.3f}\")\n",
    "\n",
    "# Simulated weighted score\n",
    "score = 0.945 * 0.30 + 0.567 * 0.20 + 1.0 * 0.15 + 1.0 * 0.25 + (1 - 48/500) * 0.10\n",
    "print(f\"\\n  Suspicion score: {score:.4f}\")\n",
    "bar = 'â–ˆ' * int(score * 40) + 'â–‘' * (40 - int(score * 40))\n",
    "print(f\"  [{bar}]\")\n",
    "print(f\"\\n  Output: SCORE = {score:.4f} (probability, no explanation)\")\n",
    "print(f\"  Time: ~5ms | Cost: $0 | Explainability: âš ï¸ Feature importance only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERA 3: Deep Learning\n",
    "print(\"\\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"â”‚  ERA 3: DEEP LEARNING (2010s)                           â”‚\")\n",
    "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "\n",
    "print(f\"\\n  LSTM analyzes the SEQUENCE of recent transactions:\")\n",
    "print(f\"  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(f\"  â”‚ Date     â”‚ Amount    â”‚ Type         â”‚ Branch   â”‚\")\n",
    "print(f\"  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "print(f\"  â”‚ Dec 10   â”‚ $9,420    â”‚ Cash deposit â”‚ #103     â”‚\")\n",
    "print(f\"  â”‚ Dec 11   â”‚ $9,480    â”‚ Cash deposit â”‚ #115     â”‚\")\n",
    "print(f\"  â”‚ Dec 13   â”‚ $9,390    â”‚ Cash deposit â”‚ #108     â”‚\")\n",
    "print(f\"  â”‚ Dec 14   â”‚ $9,510    â”‚ Cash deposit â”‚ #121     â”‚\")\n",
    "print(f\"  â”‚ Dec 15   â”‚ $9,450    â”‚ Cash deposit â”‚ #127  â—„  â”‚\")\n",
    "print(f\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print(f\"\")\n",
    "print(f\"  Patterns detected by sequence model:\")\n",
    "print(f\"    â€¢ Consistent sub-$10K amounts (structuring signature)\")\n",
    "print(f\"    â€¢ Different branches each day (geographic dispersion)\")\n",
    "print(f\"    â€¢ Daily cadence (velocity anomaly)\")\n",
    "print(f\"\\n  Sequence score: 0.94 (very high confidence)\")\n",
    "print(f\"\\n  Output: SCORE = 0.94 (pattern-aware, still no text explanation)\")\n",
    "print(f\"  Time: ~20ms | Cost: $0 | Explainability: âŒ Black box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERA 4: Generative AI\n",
    "print(\"\\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"â”‚  ERA 4: GENERATIVE AI (2020s)                           â”‚\")\n",
    "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "\n",
    "print(f\"\\n  Input: All data above + prompt: 'Draft SAR narrative'\")\n",
    "print(f\"\")\n",
    "print(f\"  Generated Output:\")\n",
    "print(f\"  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(f\"  â”‚ SUSPICIOUS ACTIVITY ANALYSIS                                â”‚\")\n",
    "print(f\"  â”‚                                                              â”‚\")\n",
    "print(f\"  â”‚ Subject: Quickship Logistics LLC (Acct ****9912)             â”‚\")\n",
    "print(f\"  â”‚ Period: December 10-15, 2024                                 â”‚\")\n",
    "print(f\"  â”‚                                                              â”‚\")\n",
    "print(f\"  â”‚ FINDINGS:                                                    â”‚\")\n",
    "print(f\"  â”‚ The Subject made 5 cash deposits over 5 consecutive          â”‚\")\n",
    "print(f\"  â”‚ business days totaling $47,250, with individual deposits     â”‚\")\n",
    "print(f\"  â”‚ ranging from $9,390 to $9,510 â€” all below the $10,000       â”‚\")\n",
    "print(f\"  â”‚ CTR reporting threshold.                                     â”‚\")\n",
    "print(f\"  â”‚                                                              â”‚\")\n",
    "print(f\"  â”‚ INDICATORS:                                                  â”‚\")\n",
    "print(f\"  â”‚ 1. Structuring (31 CFR 1010.311): Sub-$10K pattern           â”‚\")\n",
    "print(f\"  â”‚ 2. Geographic dispersion: 5 branches in 5 days               â”‚\")\n",
    "print(f\"  â”‚ 3. Volume anomaly: 3.2x above monthly average                â”‚\")\n",
    "print(f\"  â”‚                                                              â”‚\")\n",
    "print(f\"  â”‚ RECOMMENDATION: Escalate to BSA team for SAR filing.         â”‚\")\n",
    "print(f\"  â”‚ Refs: 31 USC 5324, 31 CFR 1010.311                          â”‚\")\n",
    "print(f\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print(f\"\")\n",
    "print(f\"  Output: FULL NARRATIVE with citations, ready for analyst review\")\n",
    "print(f\"  Time: ~3s | Cost: ~$0.02 | Explainability: âœ… Natural language\")\n",
    "\n",
    "print(f\"\\n  âš ï¸ BUT: Citations could be hallucinated! Always verify.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison table\n",
    "print(\"\\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"  ğŸ“Š COMPARISON: Same Transaction, Four Approaches\")\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\")\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {'Era': 'Rule-Based', 'Output': 'Binary flag', 'Latency': '<1ms', 'Cost': '$0',\n",
    "     'Explainability': 'âœ… Perfect', 'Catches This?': 'âœ… Yes (threshold)'},\n",
    "    {'Era': 'ML', 'Output': 'Score (0-1)', 'Latency': '~5ms', 'Cost': '$0',\n",
    "     'Explainability': 'âš ï¸ Limited', 'Catches This?': 'âœ… Yes (features)'},\n",
    "    {'Era': 'Deep Learning', 'Output': 'Score (sequence)', 'Latency': '~20ms', 'Cost': '$0',\n",
    "     'Explainability': 'âŒ Black box', 'Catches This?': 'âœ… Yes (pattern)'},\n",
    "    {'Era': 'GenAI', 'Output': 'Full narrative', 'Latency': '~3s', 'Cost': '~$0.02',\n",
    "     'Explainability': 'âœ… Natural language', 'Catches This?': 'âœ… + explains why'},\n",
    "])\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "print(f\"\\n  KEY INSIGHT: Each era ADDS capability. Best practice: use ALL FOUR together.\")\n",
    "print(f\"  Rules detect â†’ ML scores â†’ DL finds patterns â†’ GenAI explains & drafts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Fun But Important â€” Things That Trip Up LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 ğŸ§® LLMs Can't Do Math (Reliably)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"  ğŸ§® FUN BUT IMPORTANT: Why LLMs Struggle with Math\")\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print()\n",
    "print(\"  LLMs predict tokens, not compute arithmetic.\")\n",
    "print(\"  Let's see why this matters for banking...\\n\")\n",
    "\n",
    "# Show how numbers get tokenized â€” they're not \"numbers\" to the model\n",
    "calculations = [\n",
    "    (\"Simple\", \"47 + 38\", 85),\n",
    "    (\"Medium\", \"1,247 Ã— 3\", 3741),\n",
    "    (\"Banking\", \"$45,231.67 + $12,847.33\", 58079.00),\n",
    "    (\"APR\", \"($15,000 Ã— 0.065) / 12\", 81.25),\n",
    "    (\"Large\", \"8,347,291 + 4,158,632\", 12505923),\n",
    "]\n",
    "\n",
    "print(f\"  How the model 'sees' numbers vs. how we see them:\\n\")\n",
    "\n",
    "for label, expr, answer in calculations:\n",
    "    tokens = enc.encode(expr)\n",
    "    decoded = [enc.decode([t]) for t in tokens]\n",
    "    print(f\"  {label}: {expr} = {answer:,}\")\n",
    "    print(f\"    Model sees: {decoded}  ({len(tokens)} tokens)\")\n",
    "    print(f\"    The model must PREDICT the answer token-by-token,\")\n",
    "    print(f\"    not CALCULATE it. It's pattern matching, not arithmetic.\\n\")\n",
    "\n",
    "print(\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(\"  ğŸ¦ BANKING RULE: Never trust LLM math directly.\")\n",
    "print(\"     â€¢ Ask it to WRITE the calculation code\")\n",
    "print(\"     â€¢ Run the code yourself\")\n",
    "print(\"     â€¢ Especially for: APR, IRR, amortization, P&L\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 ğŸ”„ The Reversal Curse â€” LLMs Have One-Way Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"  ğŸ”„ THE REVERSAL CURSE\")\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print()\n",
    "print('  LLMs trained on \"A is B\" often CANNOT answer \"B is ?\"')\n",
    "print()\n",
    "print(\"  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"  â”‚ Training data says:                                  â”‚\")\n",
    "print('  â”‚   \"The CEO of JPMorgan Chase is Jamie Dimon\"         â”‚')\n",
    "print(\"  â”‚                                                      â”‚\")\n",
    "print(\"  â”‚ Q: Who is the CEO of JPMorgan Chase?                 â”‚\")\n",
    "print(\"  â”‚ A: Jamie Dimon  âœ…  (forward direction, easy)        â”‚\")\n",
    "print(\"  â”‚                                                      â”‚\")\n",
    "print(\"  â”‚ Q: Jamie Dimon is the CEO of which company?          â”‚\")\n",
    "print(\"  â”‚ A: Might struggle!  âš ï¸  (reverse direction, harder)  â”‚\")\n",
    "print(\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print()\n",
    "print(\"  Why? Because the model learned to predict tokens LEFT â†’ RIGHT.\")\n",
    "print('  \"CEO of JPMorgan\" â†’ predicts â†’ \"Jamie Dimon\"')\n",
    "print('  \"Jamie Dimon\" â†’ doesn\\'t as strongly predict â†’ \"JPMorgan\"')\n",
    "print()\n",
    "print(\"  ğŸ¦ Banking implications:\")\n",
    "print('  â€¢ \"What regulation covers wire transfers?\" â†’ Good')\n",
    "print('  â€¢ \"What does Reg E cover?\" â†’ Also good (common in training data)')\n",
    "print('  â€¢ \"Who is the BSA officer at our bank?\" â†’ Unknown (not in training)')\n",
    "print('  â€¢ \"Which accounts does customer #4821 have?\" â†’ Cannot know')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 ğŸ­ Prompt Framing: The Model Follows Your Lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"  ğŸ­ PROMPT FRAMING BIAS\")\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print()\n",
    "print(\"  The same transaction gets DIFFERENT analyses depending\")\n",
    "print(\"  on how you frame the prompt.\\n\")\n",
    "\n",
    "print(\"  Transaction: $9,450 cash deposit at Branch #127\\n\")\n",
    "\n",
    "print(\"  â”Œâ”€ PROMPT A (Biased: assumes guilt) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"  â”‚ 'This transaction looks suspicious. Explain why this       â”‚\")\n",
    "print(\"  â”‚  customer is likely structuring deposits to avoid CTR.'    â”‚\")\n",
    "print(\"  â”‚                                                            â”‚\")\n",
    "print(\"  â”‚ â†’ Model will AGREE and build a case for structuring.       â”‚\")\n",
    "print(\"  â”‚   It follows your framing.                                 â”‚\")\n",
    "print(\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print()\n",
    "print(\"  â”Œâ”€ PROMPT B (Neutral: asks for analysis) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"  â”‚ 'Analyze this transaction objectively. Identify patterns   â”‚\")\n",
    "print(\"  â”‚  that could indicate suspicious activity AND patterns      â”‚\")\n",
    "print(\"  â”‚  that suggest legitimate business operations.'             â”‚\")\n",
    "print(\"  â”‚                                                            â”‚\")\n",
    "print(\"  â”‚ â†’ Model will present BOTH sides.                           â”‚\")\n",
    "print(\"  â”‚   Balanced analysis with evidence for and against.          â”‚\")\n",
    "print(\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print()\n",
    "print(\"  â”Œâ”€ PROMPT C (Biased: assumes innocence) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"  â”‚ 'This is a legitimate freight broker. Explain why their    â”‚\")\n",
    "print(\"  â”‚  cash deposits are normal business activity.'              â”‚\")\n",
    "print(\"  â”‚                                                            â”‚\")\n",
    "print(\"  â”‚ â†’ Model will AGREE and rationalize the deposits.           â”‚\")\n",
    "print(\"  â”‚   It follows your framing.                                 â”‚\")\n",
    "print(\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print()\n",
    "print(\"  âš ï¸  BANKING RULE: Always use NEUTRAL prompts for compliance.\")\n",
    "print(\"  The model is a mirror â€” it reflects the bias you put in.\")\n",
    "print(\"  This is especially dangerous for: SAR reviews, credit decisions,\")\n",
    "print(\"  customer complaint analysis, and any regulatory work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 ğŸ”¢ Counting & Precision â€” Not the Model's Strong Suit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"  ğŸ”¢ COUNTING & PRECISION FAILURES\")\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print()\n",
    "\n",
    "# Count letters in a word â€” LLMs are famously bad at this\n",
    "test_words = [\n",
    "    (\"strawberry\", 'r'),\n",
    "    (\"mississippi\", 's'),\n",
    "    (\"accommodation\", 'c'),\n",
    "    (\"assessment\", 's'),\n",
    "]\n",
    "\n",
    "print(\"  How many times does a letter appear? (Famously hard for LLMs)\\n\")\n",
    "for word, letter in test_words:\n",
    "    correct = word.count(letter)\n",
    "    print(f\"  How many '{letter}'s in '{word}'?\")\n",
    "    print(f\"    Correct answer: {correct}\")\n",
    "    tokens = enc.encode(word)\n",
    "    decoded = [enc.decode([t]) for t in tokens]\n",
    "    print(f\"    Model sees tokens: {decoded}  (NOT individual letters!)\")\n",
    "    print()\n",
    "\n",
    "print(\"  Why? The model sees TOKENS, not individual characters.\")\n",
    "print(\"  'strawberry' â†’ ['str', 'aw', 'berry']  â€” the 'r's are SPLIT across tokens!\")\n",
    "print()\n",
    "print(\"  ğŸ¦ Banking implication:\")\n",
    "print(\"  â€¢ Don't ask LLMs to COUNT transactions in a dataset\")\n",
    "print(\"  â€¢ Don't ask LLMs for EXACT numerical summaries\")\n",
    "print(\"  â€¢ DO ask LLMs to WRITE the SQL/Python that counts for you\")\n",
    "print(\"  â€¢ DO ask LLMs to EXPLAIN patterns you've already quantified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 ğŸ’¸ Float vs. Decimal â€” Why Banking Code Must Use BigDecimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"  ğŸ’¸ THE FLOATING POINT TRAP (Not GenAI â€” But LLMs Generate This Bug!)\")\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print()\n",
    "print(\"  If you ask an LLM to write banking code, it might use float.\")\n",
    "print(\"  Here's why that's DANGEROUS:\\n\")\n",
    "\n",
    "# Classic floating point problem\n",
    "print(\"  Using float (WRONG for banking):\")\n",
    "a = 0.1 + 0.2\n",
    "print(f\"    0.1 + 0.2 = {a}\")\n",
    "print(f\"    0.1 + 0.2 == 0.3?  {a == 0.3}  ğŸ˜±\\n\")\n",
    "\n",
    "# Compound the error across transactions\n",
    "print(\"  Compounding error across 10,000 transactions:\")\n",
    "balance_float = 0.0\n",
    "balance_decimal = Decimal('0.00')\n",
    "\n",
    "for i in range(10000):\n",
    "    balance_float += 0.01\n",
    "    balance_decimal += Decimal('0.01')\n",
    "\n",
    "print(f\"    float result:   ${balance_float:.10f}  (should be $100.00)\")\n",
    "print(f\"    Decimal result: ${balance_decimal}\")\n",
    "print(f\"    Error: ${abs(100.0 - balance_float):.10f}\")\n",
    "print()\n",
    "\n",
    "# At banking scale\n",
    "print(\"  At banking scale (1M transactions/day):\")\n",
    "daily_error = abs(100.0 - balance_float) * 100  # scale up\n",
    "yearly_error = daily_error * 365\n",
    "print(f\"    Potential daily rounding error: ${daily_error:.4f}\")\n",
    "print(f\"    Potential yearly error: ${yearly_error:.2f}\")\n",
    "print(f\"    Over 10 years: ${yearly_error * 10:,.2f}\")\n",
    "print()\n",
    "print(\"  âš ï¸ When LLMs generate banking code, ALWAYS check for float/double.\")\n",
    "print(\"  CLAUDE.md rule: 'All monetary amounts use BigDecimal, never float/double'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Bias Deep Dive â€” Training Data Reflects History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"  âš–ï¸ BIAS IN LENDING: How Historical Discrimination Propagates\")\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print()\n",
    "print(\"  We'll train a model on HISTORICAL lending data.\")\n",
    "print(\"  The data reflects past discrimination: Group B was denied\")\n",
    "print(\"  more often EVEN with similar credit profiles.\\n\")\n",
    "\n",
    "# Generate data with historical bias\n",
    "n = 1000\n",
    "\n",
    "# Group A: historically favored\n",
    "income_a = np.random.normal(75000, 20000, n)\n",
    "score_a = np.random.normal(720, 50, n)\n",
    "dti_a = np.random.normal(0.32, 0.08, n)\n",
    "# Higher base approval rate (historical privilege)\n",
    "prob_a = np.clip(0.75 + (score_a - 700)/1000 + (income_a - 60000)/500000 - dti_a, 0.1, 0.95)\n",
    "approved_a = (np.random.random(n) < prob_a).astype(int)\n",
    "\n",
    "# Group B: historically disadvantaged (SIMILAR profiles, LOWER approval)\n",
    "income_b = np.random.normal(70000, 22000, n)\n",
    "score_b = np.random.normal(710, 55, n)\n",
    "dti_b = np.random.normal(0.34, 0.09, n)\n",
    "# Lower base approval rate (historical discrimination)\n",
    "prob_b = np.clip(0.55 + (score_b - 700)/1000 + (income_b - 60000)/500000 - dti_b, 0.1, 0.95)\n",
    "approved_b = (np.random.random(n) < prob_b).astype(int)\n",
    "\n",
    "print(\"  Historical Data:\")\n",
    "print(f\"  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(f\"  â”‚ Group      â”‚ Avg Income   â”‚ Avg Score    â”‚ Avg DTI      â”‚ Approval Rate  â”‚\")\n",
    "print(f\"  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "print(f\"  â”‚ Group A    â”‚ ${income_a.mean():>9,.0f}  â”‚ {score_a.mean():>10.0f}  â”‚ {dti_a.mean():>10.2f}  â”‚ {approved_a.mean():>12.1%}  â”‚\")\n",
    "print(f\"  â”‚ Group B    â”‚ ${income_b.mean():>9,.0f}  â”‚ {score_b.mean():>10.0f}  â”‚ {dti_b.mean():>10.2f}  â”‚ {approved_b.mean():>12.1%}  â”‚\")\n",
    "print(f\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print(f\"\\n  âš ï¸ Similar profiles, but {approved_a.mean() - approved_b.mean():.0%} gap in approval rates!\")\n",
    "print(f\"  This gap = historical discrimination baked into the training data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on biased data (model never sees group label)\n",
    "X = np.vstack([\n",
    "    np.column_stack([income_a, score_a, dti_a]),\n",
    "    np.column_stack([income_b, score_b, dti_b])\n",
    "])\n",
    "y = np.concatenate([approved_a, approved_b])\n",
    "\n",
    "model_biased = LogisticRegression(random_state=42)\n",
    "model_biased.fit(X, y)\n",
    "\n",
    "# Score both groups\n",
    "preds_a = model_biased.predict_proba(np.column_stack([income_a, score_a, dti_a]))[:, 1]\n",
    "preds_b = model_biased.predict_proba(np.column_stack([income_b, score_b, dti_b]))[:, 1]\n",
    "\n",
    "print(\"\\n  Model trained on biased data (group label NOT given to model):\")\n",
    "print(f\"  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(f\"  â”‚ Group      â”‚ Avg Predicted Approval    â”‚\")\n",
    "print(f\"  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "print(f\"  â”‚ Group A    â”‚ {preds_a.mean():>22.1%}  â”‚\")\n",
    "print(f\"  â”‚ Group B    â”‚ {preds_b.mean():>22.1%}  â”‚\")\n",
    "print(f\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print(f\"\\n  Model-predicted gap: {preds_a.mean() - preds_b.mean():.1%}\")\n",
    "print(f\"\\n  â†’ The model LEARNED the bias even though it never saw the group label.\")\n",
    "print(f\"  â†’ It found proxies in the data (slight income/score/DTI differences)\")\n",
    "print(f\"    and amplified them to match the biased historical outcomes.\")\n",
    "print(f\"\\n  This is EXACTLY how discrimination propagates through AI systems.\")\n",
    "print(f\"  ECOA + Fair Lending laws require disparate impact testing for this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: Spot the Hallucination â€” Interactive Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"  ğŸ” SPOT THE HALLUCINATION: Banking Edition\")\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print()\n",
    "print(\"  Below is an AI-generated response about Reg E.\")\n",
    "print(\"  Some parts are accurate. Some are HALLUCINATED.\")\n",
    "print(\"  Can you spot which is which?\\n\")\n",
    "\n",
    "print(\"  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"  â”‚  AI RESPONSE:                                             â”‚\")\n",
    "print(\"  â”‚                                                            â”‚\")\n",
    "print(\"  â”‚  Under Regulation E (12 CFR 1005.11), the financial       â”‚\")\n",
    "print(\"  â”‚  institution must:                                         â”‚\")\n",
    "print(\"  â”‚                                                            â”‚\")\n",
    "print(\"  â”‚  1. Investigate within 10 business days               [?] â”‚\")\n",
    "print(\"  â”‚  2. Provisionally credit within 10 business days if   [?] â”‚\")\n",
    "print(\"  â”‚     more time needed                                      â”‚\")\n",
    "print(\"  â”‚  3. Complete investigation within 45 calendar days    [?] â”‚\")\n",
    "print(\"  â”‚  4. For new accounts: 90 calendar days allowed        [?] â”‚\")\n",
    "print(\"  â”‚  5. Report results within 3 business days             [?] â”‚\")\n",
    "print(\"  â”‚  6. Per Section 1005.11(d)(2), reverse provisional    [?] â”‚\")\n",
    "print(\"  â”‚     credit only after 5 business days' written notice     â”‚\")\n",
    "print(\"  â”‚  7. Institutions with <$50M assets are exempt per     [?] â”‚\")\n",
    "print(\"  â”‚     the 2019 EFTA amendment                               â”‚\")\n",
    "print(\"  â”‚                                                            â”‚\")\n",
    "print(\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reveal answers\n",
    "print(\"\\n  â”€â”€ ANSWERS â”€â”€\\n\")\n",
    "\n",
    "items = [\n",
    "    (\"1\", \"10 business day investigation\", \"âœ… ACCURATE\", \"Standard Reg E timeline\"),\n",
    "    (\"2\", \"Provisional credit in 10 days\", \"âœ… ACCURATE\", \"Required if investigation extends\"),\n",
    "    (\"3\", \"45 calendar day completion\", \"âœ… ACCURATE\", \"Standard completion deadline\"),\n",
    "    (\"4\", \"90 days for new accounts\", \"âœ… ACCURATE\", \"Applies to new accounts, POS, international\"),\n",
    "    (\"5\", \"3 business days to report\", \"âš ï¸ VERIFY\", \"Commonly cited but exact number should be checked against regulation text\"),\n",
    "    (\"6\", \"Section 1005.11(d)(2)\", \"âš ï¸ LIKELY HALLUCINATED\", \"Specific subsection citation may be fabricated. The concept exists but the exact citation needs verification.\"),\n",
    "    (\"7\", \"$50M asset exemption\", \"âŒ HALLUCINATED\", \"There is NO such exemption in Reg E. This was completely invented by the model. This is the DANGEROUS kind.\"),\n",
    "]\n",
    "\n",
    "for num, claim, status, explanation in items:\n",
    "    print(f\"  {status}  Item {num}: {claim}\")\n",
    "    print(f\"         {explanation}\\n\")\n",
    "\n",
    "print(\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(\"  KEY LESSON: The most dangerous hallucinations LOOK real.\")\n",
    "print(\"  Item 7 is completely invented but reads like a real exemption.\")\n",
    "print(\"  In banking, acting on a hallucinated regulation = compliance violation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 8: Token Cost Calculator â€” Interactive\n",
    "\n",
    "Calculate costs for your own banking workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRICING = {\n",
    "    'Haiku':  {'input': 0.25,  'output': 1.25},\n",
    "    'Sonnet': {'input': 3.00,  'output': 15.00},\n",
    "    'Opus':   {'input': 15.00, 'output': 75.00},\n",
    "}\n",
    "\n",
    "tasks = [\n",
    "    {\"name\": \"Summarize 10-page loan doc\",        \"in\": 4000,    \"out\": 500,   \"model\": \"Sonnet\", \"vol\": 200},\n",
    "    {\"name\": \"Generate unit tests (Java class)\",   \"in\": 2000,    \"out\": 3000,  \"model\": \"Sonnet\", \"vol\": 50},\n",
    "    {\"name\": \"Analyze 200-page regulation\",        \"in\": 80000,   \"out\": 2000,  \"model\": \"Opus\",   \"vol\": 5},\n",
    "    {\"name\": \"Classify customer email\",             \"in\": 270,     \"out\": 10,    \"model\": \"Haiku\",  \"vol\": 10000},\n",
    "    {\"name\": \"Draft SAR narrative\",                 \"in\": 5000,    \"out\": 3000,  \"model\": \"Sonnet\", \"vol\": 100},\n",
    "    {\"name\": \"Generate Terraform module\",           \"in\": 500,     \"out\": 2000,  \"model\": \"Sonnet\", \"vol\": 10},\n",
    "    {\"name\": \"Code review (500 lines)\",             \"in\": 3000,    \"out\": 1000,  \"model\": \"Sonnet\", \"vol\": 30},\n",
    "    {\"name\": \"Incident postmortem draft\",           \"in\": 3000,    \"out\": 2500,  \"model\": \"Sonnet\", \"vol\": 3},\n",
    "]\n",
    "\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(\"  ğŸ’° TOKEN ECONOMICS: Full Cost Analysis for Banking Workloads\")\n",
    "print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\")\n",
    "\n",
    "rows = []\n",
    "total_daily = 0\n",
    "for t in tasks:\n",
    "    p = PRICING[t['model']]\n",
    "    per_call = t['in'] * p['input'] / 1e6 + t['out'] * p['output'] / 1e6\n",
    "    daily = per_call * t['vol']\n",
    "    total_daily += daily\n",
    "    rows.append({\n",
    "        'Task': t['name'],\n",
    "        'Model': t['model'],\n",
    "        'In Tokens': f\"{t['in']:,}\",\n",
    "        'Out Tokens': f\"{t['out']:,}\",\n",
    "        'Per Call': f\"${per_call:.4f}\",\n",
    "        'Daily Vol': f\"{t['vol']:,}\",\n",
    "        'Daily Cost': f\"${daily:.2f}\",\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n  {'â”€'*60}\")\n",
    "print(f\"  Total Daily Cost:    ${total_daily:>10.2f}\")\n",
    "print(f\"  Monthly (22 days):   ${total_daily * 22:>10.2f}\")\n",
    "print(f\"  Annual (260 days):   ${total_daily * 260:>10,.2f}\")\n",
    "\n",
    "print(f\"\\n  Compare: One compliance analyst salary = ~$80,000-120,000/year\")\n",
    "print(f\"  AI API costs for the above workload   = ~${total_daily * 260:,.0f}/year\")\n",
    "print(f\"  (Plus human review time â€” AI assists, doesn't replace)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 9: Live Claude API Call (Optional)\n",
    "\n",
    "If you have an `ANTHROPIC_API_KEY`, run this cell to see a real API call in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and set your API key to run this cell\n",
    "# import os\n",
    "# os.environ['ANTHROPIC_API_KEY'] = 'sk-ant-...'\n",
    "\n",
    "try:\n",
    "    import anthropic\n",
    "    import os\n",
    "    \n",
    "    api_key = os.environ.get('ANTHROPIC_API_KEY')\n",
    "    if not api_key:\n",
    "        print(\"Set ANTHROPIC_API_KEY to run this demo.\")\n",
    "        print(\"Uncomment the os.environ line above and paste your key.\")\n",
    "    else:\n",
    "        client = anthropic.Anthropic(api_key=api_key)\n",
    "        \n",
    "        prompt = \"\"\"You are a banking compliance assistant. A customer made 5 cash \n",
    "deposits of ~$9,450 each at 5 different branches in 5 days. The customer runs \n",
    "a small freight brokerage with $200K annual revenue. Provide a brief (100 word) \n",
    "risk assessment with regulatory references.\"\"\"\n",
    "        \n",
    "        print(\"Sending to Claude Haiku...\\n\")\n",
    "        start = time.time()\n",
    "        \n",
    "        msg = client.messages.create(\n",
    "            model=\"claude-haiku-4-5-20251001\",\n",
    "            max_tokens=300,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        print(f\"Response ({elapsed:.2f}s, {msg.usage.input_tokens} in / {msg.usage.output_tokens} out):\\n\")\n",
    "        print(msg.content[0].text)\n",
    "        \n",
    "        cost = msg.usage.input_tokens * 0.25/1e6 + msg.usage.output_tokens * 1.25/1e6\n",
    "        print(f\"\\nCost: ${cost:.6f}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Install anthropic: pip install anthropic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary: What We Demonstrated\n",
    "\n",
    "| Demo | Key Takeaway |\n",
    "|------|-------------|\n",
    "| **Word Embeddings** | Words become numbers. Arithmetic works (King-Man+Woman=Queen). But embeddings also encode societal **biases**. |\n",
    "| **Gender + Profession** | Programmer-Man+Woman reveals stereotypes. These biases flow into LLMs. Critical for Fair Lending. |\n",
    "| **Tokenization** | LLMs see tokens, not words. Emojis are expensive. Non-English costs 2-4x more. Numbers get split. |\n",
    "| **Four Eras** | Rule-based â†’ ML â†’ DL â†’ GenAI. Each adds capability. Banks use all four. |\n",
    "| **Discriminative vs Generative** | Classifier outputs a score. Generator outputs text. Modern systems use both. |\n",
    "| **LLMs Can't Do Math** | Token prediction â‰  calculation. Always verify. Ask it to write code, not compute. |\n",
    "| **Reversal Curse** | Models learn Aâ†’B better than Bâ†’A. Don't assume bidirectional knowledge. |\n",
    "| **Prompt Framing** | The model mirrors your bias. Use neutral prompts for compliance. |\n",
    "| **Floating Point** | LLMs may generate float-based banking code. Always enforce BigDecimal. |\n",
    "| **Historical Bias in Lending** | Models learn discrimination from biased data, even without protected class features. |\n",
    "| **Spot the Hallucination** | Fabricated regulatory citations look real. Always verify against source documents. |\n",
    "| **Token Economics** | Individual calls are cheap. Scale matters. Still far cheaper than manual labor. |\n",
    "\n",
    "---\n",
    "\n",
    "**Next Session:** GenAI vs Agentic AI â€” From answering questions to taking actions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
